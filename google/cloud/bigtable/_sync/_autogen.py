# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# This file is automatically generated by sync_surface_generator.py. Do not edit.


from __future__ import annotations
from abc import ABC
from functools import partial
from grpc import Channel
from grpc.aio import RpcContext
from typing import Any
from typing import Callable
from typing import Coroutine
from typing import Generator
from typing import Iterable
from typing import Iterator
from typing import List
from typing import Optional
from typing import Set
from typing import cast
import asyncio
import functools
import time
import warnings

from google.api_core import client_options as client_options_lib
from google.api_core import exceptions as core_exceptions
from google.api_core import retry as retries
from google.api_core.exceptions import GoogleAPICallError
from google.cloud.bigtable._helpers import _attempt_timeout_generator
from google.cloud.bigtable._helpers import _convert_retry_deadline
from google.cloud.bigtable._helpers import _make_metadata
from google.cloud.bigtable._mutate_rows import _MutateRowsIncomplete
from google.cloud.bigtable._read_rows import _StateMachine
from google.cloud.bigtable.client import Table
from google.cloud.bigtable.exceptions import InvalidChunk
from google.cloud.bigtable.exceptions import _RowSetComplete
from google.cloud.bigtable.mutations import Mutation
from google.cloud.bigtable.mutations import RowMutationEntry
from google.cloud.bigtable.mutations_batcher import MutationsBatcher
from google.cloud.bigtable.read_modify_write_rules import ReadModifyWriteRule
from google.cloud.bigtable.read_rows_query import ReadRowsQuery
from google.cloud.bigtable.row import Row
from google.cloud.bigtable.row import _LastScannedRow
from google.cloud.bigtable.row_filters import CellsRowLimitFilter
from google.cloud.bigtable.row_filters import RowFilter
from google.cloud.bigtable.row_filters import RowFilterChain
from google.cloud.bigtable.row_filters import StripValueTransformerFilter
from google.cloud.bigtable_v2.services.bigtable.async_client import DEFAULT_CLIENT_INFO
from google.cloud.bigtable_v2.services.bigtable.client import BigtableClient
from google.cloud.bigtable_v2.services.bigtable.transports.grpc import (
    BigtableGrpcTransport,
)
from google.cloud.bigtable_v2.types.bigtable import ReadRowsResponse
from google.cloud.client import ClientWithProject
import google.auth._default
import google.auth.credentials
import google.cloud.bigtable.exceptions
import google.cloud.bigtable.exceptions as bt_exceptions


class _ReadRowsOperation_Sync(Iterable[Row], ABC):
    """
    ReadRowsOperation handles the logic of merging chunks from a ReadRowsResponse stream
    into a stream of Row objects.

    ReadRowsOperation.merge_row_response_stream takes in a stream of ReadRowsResponse
    and turns them into a stream of Row objects using an internal
    StateMachine.

    ReadRowsOperation(request, client) handles row merging logic end-to-end, including
    performing retries on stream errors.
    """

    def __init__(
        self,
        request: dict[str, Any],
        client: BigtableClient,
        *,
        operation_timeout: float = 600.0,
        per_request_timeout: float | None = None,
    ):
        """
        Args:
          - request: the request dict to send to the Bigtable API
          - client: the Bigtable client to use to make the request
          - operation_timeout: the timeout to use for the entire operation, in seconds
          - per_request_timeout: the timeout to use when waiting for each individual grpc request, in seconds
                If not specified, defaults to operation_timeout
        """
        self._last_emitted_row_key: bytes | None = None
        self._emit_count = 0
        self._request = request
        self.operation_timeout = operation_timeout
        attempt_timeout_gen = _attempt_timeout_generator(
            per_request_timeout, operation_timeout
        )
        row_limit = request.get("rows_limit", 0)
        self._partial_retryable = partial(
            self._read_rows_retryable_attempt,
            client.read_rows,
            attempt_timeout_gen,
            row_limit,
        )
        predicate = retries.if_exception_type(
            core_exceptions.DeadlineExceeded,
            core_exceptions.ServiceUnavailable,
            core_exceptions.Aborted,
        )

        def on_error_fn(exc):
            if predicate(exc):
                self.transient_errors.append(exc)

        retry = retries.Retry(
            predicate=predicate,
            timeout=self.operation_timeout,
            initial=0.01,
            multiplier=2,
            maximum=60,
            on_error=on_error_fn,
            is_stream=True,
        )
        self._stream: Generator[Row, None, "Any"] | None = retry(
            self._partial_retryable
        )()
        self.transient_errors: List[Exception] = []

    def __iter__(self) -> Iterator[Row]:
        """Implements the Iterable interface"""
        return self

    def __next__(self) -> Row:
        """Implements the Iterator interface"""
        if self._stream is not None:
            return self._stream.__next__()
        else:
            raise GeneratorExit

    def close(self):
        """Close the stream and release resources"""
        if self._stream is not None:
            self._stream.close()
        self._stream = None
        self._emitted_seen_row_key = None

    def _read_rows_retryable_attempt(
        self,
        gapic_fn: Callable[..., Iterable[ReadRowsResponse]],
        timeout_generator: Iterator[float],
        total_row_limit: int,
    ) -> Generator[Row, None, "Any"]:
        """
        Retryable wrapper for merge_rows. This function is called each time
        a retry is attempted.

        Some fresh state is created on each retry:
          - grpc network stream
          - state machine to hold merge chunks received from stream
        Some state is shared between retries:
          - _last_emitted_row_key is used to ensure that
            duplicate rows are not emitted
          - request is stored and (potentially) modified on each retry
        """
        if self._last_emitted_row_key is not None:
            try:
                self._request["rows"] = _ReadRowsOperation_Sync._revise_request_rowset(
                    row_set=self._request.get("rows", None),
                    last_seen_row_key=self._last_emitted_row_key,
                )
            except _RowSetComplete:
                return
            if total_row_limit:
                new_limit = total_row_limit - self._emit_count
                if new_limit == 0:
                    return
                elif new_limit < 0:
                    raise RuntimeError("unexpected state: emit count exceeds row limit")
                else:
                    self._request["rows_limit"] = new_limit
        metadata = _make_metadata(
            self._request.get("table_name", None),
            self._request.get("app_profile_id", None),
        )
        new_gapic_stream: RpcContext = gapic_fn(
            self._request, timeout=next(timeout_generator), metadata=metadata
        )
        try:
            state_machine = _StateMachine()
            stream = _ReadRowsOperation_Sync.merge_row_response_stream(
                new_gapic_stream, state_machine
            )
            for new_item in stream:
                if (
                    self._last_emitted_row_key is not None
                    and new_item.row_key <= self._last_emitted_row_key
                ):
                    raise InvalidChunk("Last emitted row key out of order")
                if not isinstance(new_item, _LastScannedRow):
                    yield new_item
                    self._emit_count += 1
                self._last_emitted_row_key = new_item.row_key
                if total_row_limit and self._emit_count >= total_row_limit:
                    return
        except (Exception, GeneratorExit) as exc:
            new_gapic_stream.cancel()
            raise exc

    @staticmethod
    def _revise_request_rowset(
        row_set: dict[str, Any] | None, last_seen_row_key: bytes
    ) -> dict[str, Any]:
        """
        Revise the rows in the request to avoid ones we've already processed.

        Args:
          - row_set: the row set from the request
          - last_seen_row_key: the last row key encountered
        Raises:
          - _RowSetComplete: if there are no rows left to process after the revision
        """
        if row_set is None or (
            len(row_set.get("row_ranges", [])) == 0
            and len(row_set.get("row_keys", [])) == 0
        ):
            last_seen = last_seen_row_key
            return {"row_keys": [], "row_ranges": [{"start_key_open": last_seen}]}
        row_keys: list[bytes] = row_set.get("row_keys", [])
        adjusted_keys = [k for k in row_keys if k > last_seen_row_key]
        row_ranges: list[dict[str, Any]] = row_set.get("row_ranges", [])
        adjusted_ranges = []
        for row_range in row_ranges:
            end_key = row_range.get("end_key_closed", None) or row_range.get(
                "end_key_open", None
            )
            if end_key is None or end_key > last_seen_row_key:
                new_range = row_range.copy()
                start_key = row_range.get("start_key_closed", None) or row_range.get(
                    "start_key_open", None
                )
                if start_key is None or start_key <= last_seen_row_key:
                    new_range["start_key_open"] = last_seen_row_key
                    new_range.pop("start_key_closed", None)
                adjusted_ranges.append(new_range)
        if len(adjusted_keys) == 0 and len(adjusted_ranges) == 0:
            raise _RowSetComplete()
        return {"row_keys": adjusted_keys, "row_ranges": adjusted_ranges}

    @staticmethod
    def merge_row_response_stream(
        response_generator: Iterable[ReadRowsResponse], state_machine: _StateMachine
    ) -> Generator[Row, None, "Any"]:
        """
        Consume chunks from a ReadRowsResponse stream into a set of Rows

        Args:
          - response_generator: Iterable of ReadRowsResponse objects. Typically
                this is a stream of chunks from the Bigtable API
        Returns:
            - Generator of Rows
        Raises:
            - InvalidChunk: if the chunk stream is invalid
        """
        for row_response in response_generator:
            response_pb = row_response._pb
            last_scanned = response_pb.last_scanned_row_key
            if last_scanned:
                yield state_machine.handle_last_scanned_row(last_scanned)
            for chunk in response_pb.chunks:
                complete_row = state_machine.handle_chunk(chunk)
                if complete_row is not None:
                    yield complete_row
        if not state_machine.is_terminal_state():
            raise InvalidChunk("read_rows completed with partial state remaining")


class Table_Sync(ABC):
    """
    Main Data API surface

    Table object maintains table_id, and app_profile_id context, and passes them with
    each call
    """

    def __init__(
        self,
        client: BigtableDataClient_Sync,
        instance_id: str,
        table_id: str,
        app_profile_id: str | None = None,
        *,
        default_operation_timeout: float = 600,
        default_per_request_timeout: float | None = None,
    ):
        """
        Initialize a Table_Sync instance

        Must be created within an async context (running event loop)

        Args:
            instance_id: The Bigtable instance ID to associate with this client.
                instance_id is combined with the client's project to fully
                specify the instance
            table_id: The ID of the table. table_id is combined with the
                instance_id and the client's project to fully specify the table
            app_profile_id: (Optional) The app profile to associate with requests.
                https://cloud.google.com/bigtable/docs/app-profiles
            default_operation_timeout: (Optional) The default timeout, in seconds
            default_per_request_timeout: (Optional) The default timeout for individual
                rpc requests, in seconds
        Raises:
          - RuntimeError if called outside of an async context (no running event loop)
        """
        if default_operation_timeout <= 0:
            raise ValueError("default_operation_timeout must be greater than 0")
        if default_per_request_timeout is not None and default_per_request_timeout <= 0:
            raise ValueError("default_per_request_timeout must be greater than 0")
        if (
            default_per_request_timeout is not None
            and default_per_request_timeout > default_operation_timeout
        ):
            raise ValueError(
                "default_per_request_timeout must be less than default_operation_timeout"
            )
        self.client = client
        self.instance_id = instance_id
        self.instance_name = self.client._gapic_client.instance_path(
            self.client.project, instance_id
        )
        self.table_id = table_id
        self.table_name = self.client._gapic_client.table_path(
            self.client.project, instance_id, table_id
        )
        self.app_profile_id = app_profile_id
        self.default_operation_timeout = default_operation_timeout
        self.default_per_request_timeout = default_per_request_timeout
        self.__init__async__()

    def __init__async__(self):
        """Implementation purposely removed in sync mode"""

    def read_rows_stream(
        self,
        query: ReadRowsQuery | dict[str, Any],
        *,
        operation_timeout: float | None = None,
        per_request_timeout: float | None = None,
    ) -> ReadRowsIterator_Sync:
        """
        Returns an iterator to asynchronously stream back row data.

        Failed requests within operation_timeout and operation_deadline policies will be retried.

        Args:
            - query: contains details about which rows to return
            - operation_timeout: the time budget for the entire operation, in seconds.
                 Failed requests will be retried within the budget.
                 time is only counted while actively waiting on the network.
                 If None, defaults to the Table's default_operation_timeout
            - per_request_timeout: the time budget for an individual network request, in seconds.
                If it takes longer than this time to complete, the request will be cancelled with
                a DeadlineExceeded exception, and a retry will be attempted.
                If None, defaults to the Table's default_per_request_timeout

        Returns:
            - an asynchronous iterator that yields rows returned by the query
        Raises:
            - DeadlineExceeded: raised after operation timeout
                will be chained with a RetryExceptionGroup containing GoogleAPIError exceptions
                from any retries that failed
            - GoogleAPIError: raised if the request encounters an unrecoverable error
            - IdleTimeout: if iterator was abandoned
        """
        operation_timeout = operation_timeout or self.default_operation_timeout
        per_request_timeout = per_request_timeout or self.default_per_request_timeout
        if operation_timeout <= 0:
            raise ValueError("operation_timeout must be greater than 0")
        if per_request_timeout is not None and per_request_timeout <= 0:
            raise ValueError("per_request_timeout must be greater than 0")
        if per_request_timeout is not None and per_request_timeout > operation_timeout:
            raise ValueError(
                "per_request_timeout must not be greater than operation_timeout"
            )
        if per_request_timeout is None:
            per_request_timeout = operation_timeout
        request = query._to_dict() if isinstance(query, ReadRowsQuery) else query
        request["table_name"] = self.table_name
        if self.app_profile_id:
            request["app_profile_id"] = self.app_profile_id
        row_merger = (
            google.cloud.bigtable._sync._concrete._ReadRowsOperation_Sync_Concrete(
                request,
                self.client._gapic_client,
                operation_timeout=operation_timeout,
                per_request_timeout=per_request_timeout,
            )
        )
        output_generator = (
            google.cloud.bigtable._sync._concrete.ReadRowsIterator_Sync_Concrete(
                row_merger
            )
        )
        idle_timeout_seconds = 300
        output_generator._start_idle_timer(idle_timeout_seconds)
        return output_generator

    def read_rows(
        self,
        query: ReadRowsQuery | dict[str, Any],
        *,
        operation_timeout: float | None = None,
        per_request_timeout: float | None = None,
    ) -> list[Row]:
        """
        Helper function that returns a full list instead of a generator

        See read_rows_stream

        Returns:
            - a list of the rows returned by the query
        """
        row_generator = self.read_rows_stream(
            query,
            operation_timeout=operation_timeout,
            per_request_timeout=per_request_timeout,
        )
        results = [row for row in row_generator]
        return results

    def read_row(
        self,
        row_key: str | bytes,
        *,
        row_filter: RowFilter | None = None,
        operation_timeout: int | float | None = 60,
        per_request_timeout: int | float | None = None,
    ) -> Row | None:
        """
        Helper function to return a single row

        See read_rows_stream

        Raises:
            - google.cloud.bigtable.exceptions.RowNotFound: if the row does not exist
        Returns:
            - the individual row requested, or None if it does not exist
        """
        if row_key is None:
            raise ValueError("row_key must be string or bytes")
        query = ReadRowsQuery(row_keys=row_key, row_filter=row_filter, limit=1)
        results = self.read_rows(
            query,
            operation_timeout=operation_timeout,
            per_request_timeout=per_request_timeout,
        )
        if len(results) == 0:
            return None
        return results[0]

    def read_rows_sharded(
        self,
        query_list: list[ReadRowsQuery] | list[dict[str, Any]],
        *,
        limit: int | None,
        operation_timeout: int | float | None = 60,
        per_request_timeout: int | float | None = None,
    ) -> ReadRowsIterator_Sync:
        """
        Runs a sharded query in parallel

        Each query in query list will be run concurrently, with results yielded as they are ready
        yielded results may be out of order

        Args:
            - query_list: a list of queries to run in parallel
        """
        raise NotImplementedError

    def row_exists(
        self,
        row_key: str | bytes,
        *,
        operation_timeout: int | float | None = 60,
        per_request_timeout: int | float | None = None,
    ) -> bool:
        """
        Helper function to determine if a row exists

        uses the filters: chain(limit cells per row = 1, strip value)

        Returns:
            - a bool indicating whether the row exists
        """
        if row_key is None:
            raise ValueError("row_key must be string or bytes")
        strip_filter = StripValueTransformerFilter(flag=True)
        limit_filter = CellsRowLimitFilter(1)
        chain_filter = RowFilterChain(filters=[limit_filter, strip_filter])
        query = ReadRowsQuery(row_keys=row_key, limit=1, row_filter=chain_filter)
        results = self.read_rows(
            query,
            operation_timeout=operation_timeout,
            per_request_timeout=per_request_timeout,
        )
        return len(results) > 0

    def sample_keys(
        self,
        *,
        operation_timeout: int | float | None = 60,
        per_sample_timeout: int | float | None = 10,
        per_request_timeout: int | float | None = None,
    ) -> google.cloud.bigtable.RowKeySamples:
        """
        Return a set of RowKeySamples that delimit contiguous sections of the table of
        approximately equal size

        RowKeySamples output can be used with ReadRowsQuery.shard() to create a sharded query that
        can be parallelized across multiple backend nodes read_rows and read_rows_stream
        requests will call sample_keys internally for this purpose when sharding is enabled

        RowKeySamples is simply a type alias for list[tuple[bytes, int]]; a list of
            row_keys, along with offset positions in the table

        Returns:
            - a set of RowKeySamples the delimit contiguous sections of the table
        Raises:
            - DeadlineExceeded: raised after operation timeout
                will be chained with a RetryExceptionGroup containing all GoogleAPIError
                exceptions from any retries that failed
        """
        raise NotImplementedError

    def mutations_batcher(self, **kwargs) -> MutationsBatcher:
        raise NotImplementedError("Function marked as unsupported in sync mode")

    def mutate_row(
        self,
        row_key: str | bytes,
        mutations: list[Mutation] | Mutation,
        *,
        operation_timeout: float | None = 60,
        per_request_timeout: float | None = None,
    ):
        """
         Mutates a row atomically.

         Cells already present in the row are left unchanged unless explicitly changed
         by ``mutation``.

         Idempotent operations (i.e, all mutations have an explicit timestamp) will be
         retried on server failure. Non-idempotent operations will not.

         Args:
             - row_key: the row to apply mutations to
             - mutations: the set of mutations to apply to the row
             - operation_timeout: the time budget for the entire operation, in seconds.
                 Failed requests will be retried within the budget.
                 time is only counted while actively waiting on the network.
                 DeadlineExceeded exception raised after timeout
             - per_request_timeout: the time budget for an individual network request,
               in seconds. If it takes longer than this time to complete, the request
               will be cancelled with a DeadlineExceeded exception, and a retry will be
               attempted if within operation_timeout budget

        Raises:
             - DeadlineExceeded: raised after operation timeout
                 will be chained with a RetryExceptionGroup containing all
                 GoogleAPIError exceptions from any retries that failed
             - GoogleAPIError: raised on non-idempotent operations that cannot be
                 safely retried.
        """
        operation_timeout = operation_timeout or self.default_operation_timeout
        per_request_timeout = per_request_timeout or self.default_per_request_timeout
        if operation_timeout <= 0:
            raise ValueError("operation_timeout must be greater than 0")
        if per_request_timeout is not None and per_request_timeout <= 0:
            raise ValueError("per_request_timeout must be greater than 0")
        if per_request_timeout is not None and per_request_timeout > operation_timeout:
            raise ValueError("per_request_timeout must be less than operation_timeout")
        if isinstance(row_key, str):
            row_key = row_key.encode("utf-8")
        request = {"table_name": self.table_name, "row_key": row_key}
        if self.app_profile_id:
            request["app_profile_id"] = self.app_profile_id
        if isinstance(mutations, Mutation):
            mutations = [mutations]
        request["mutations"] = [mutation._to_dict() for mutation in mutations]
        if all((mutation.is_idempotent() for mutation in mutations)):
            predicate = retries.if_exception_type(
                core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable
            )
        else:
            predicate = retries.if_exception_type()
        transient_errors = []

        def on_error_fn(exc):
            if predicate(exc):
                transient_errors.append(exc)

        retry = retries.Retry(
            predicate=predicate,
            on_error=on_error_fn,
            timeout=operation_timeout,
            initial=0.01,
            multiplier=2,
            maximum=60,
        )
        retry_wrapped = retry(self.client._gapic_client.mutate_row)
        deadline_wrapped = _convert_retry_deadline(
            retry_wrapped, operation_timeout, transient_errors
        )
        metadata = _make_metadata(self.table_name, self.app_profile_id)
        deadline_wrapped(request, timeout=per_request_timeout, metadata=metadata)

    def bulk_mutate_rows(
        self,
        mutation_entries: list[RowMutationEntry],
        *,
        operation_timeout: float | None = 60,
        per_request_timeout: float | None = None,
        on_success: Callable[
            [int, RowMutationEntry], None | Coroutine[None, None, None]
        ]
        | None = None,
    ):
        """
        Applies mutations for multiple rows in a single batched request.

        Each individual RowMutationEntry is applied atomically, but separate entries
        may be applied in arbitrary order (even for entries targetting the same row)
        In total, the row_mutations can contain at most 100000 individual mutations
        across all entries

        Idempotent entries (i.e., entries with mutations with explicit timestamps)
        will be retried on failure. Non-idempotent will not, and will reported in a
        raised exception group

        Args:
            - mutation_entries: the batches of mutations to apply
                Each entry will be applied atomically, but entries will be applied
                in arbitrary order
            - operation_timeout: the time budget for the entire operation, in seconds.
                Failed requests will be retried within the budget.
                time is only counted while actively waiting on the network.
                DeadlineExceeded exception raised after timeout
            - per_request_timeout: the time budget for an individual network request,
                in seconds. If it takes longer than this time to complete, the request
                will be cancelled with a DeadlineExceeded exception, and a retry will
                be attempted if within operation_timeout budget
            - on_success: a callback function that will be called when each mutation
                entry is confirmed to be applied successfully. Will be passed the
                index and the entry itself.
        Raises:
            - MutationsExceptionGroup if one or more mutations fails
                Contains details about any failed entries in .exceptions
        """
        operation_timeout = operation_timeout or self.default_operation_timeout
        per_request_timeout = per_request_timeout or self.default_per_request_timeout
        if operation_timeout <= 0:
            raise ValueError("operation_timeout must be greater than 0")
        if per_request_timeout is not None and per_request_timeout <= 0:
            raise ValueError("per_request_timeout must be greater than 0")
        if per_request_timeout is not None and per_request_timeout > operation_timeout:
            raise ValueError("per_request_timeout must be less than operation_timeout")
        operation = _MutateRowsOperation_Sync(
            self.client._gapic_client,
            self,
            mutation_entries,
            operation_timeout,
            per_request_timeout,
        )
        operation.start()

    def check_and_mutate_row(
        self,
        row_key: str | bytes,
        predicate: RowFilter | dict[str, Any] | None,
        *,
        true_case_mutations: Mutation | list[Mutation] | None = None,
        false_case_mutations: Mutation | list[Mutation] | None = None,
        operation_timeout: int | float | None = 20,
    ) -> bool:
        """
        Mutates a row atomically based on the output of a predicate filter

        Non-idempotent operation: will not be retried

        Args:
            - row_key: the key of the row to mutate
            - predicate: the filter to be applied to the contents of the specified row.
                Depending on whether or not any results  are yielded,
                either true_case_mutations or false_case_mutations will be executed.
                If None, checks that the row contains any values at all.
            - true_case_mutations:
                Changes to be atomically applied to the specified row if
                predicate yields at least one cell when
                applied to row_key. Entries are applied in order,
                meaning that earlier mutations can be masked by later
                ones. Must contain at least one entry if
                false_case_mutations is empty, and at most 100000.
            - false_case_mutations:
                Changes to be atomically applied to the specified row if
                predicate_filter does not yield any cells when
                applied to row_key. Entries are applied in order,
                meaning that earlier mutations can be masked by later
                ones. Must contain at least one entry if
                `true_case_mutations is empty, and at most 100000.
            - operation_timeout: the time budget for the entire operation, in seconds.
                Failed requests will not be retried.
        Returns:
            - bool indicating whether the predicate was true or false
        Raises:
            - GoogleAPIError exceptions from grpc call
        """
        operation_timeout = operation_timeout or self.default_operation_timeout
        if operation_timeout <= 0:
            raise ValueError("operation_timeout must be greater than 0")
        row_key = row_key.encode("utf-8") if isinstance(row_key, str) else row_key
        if true_case_mutations is not None and (
            not isinstance(true_case_mutations, list)
        ):
            true_case_mutations = [true_case_mutations]
        true_case_dict = [m._to_dict() for m in true_case_mutations or []]
        if false_case_mutations is not None and (
            not isinstance(false_case_mutations, list)
        ):
            false_case_mutations = [false_case_mutations]
        false_case_dict = [m._to_dict() for m in false_case_mutations or []]
        if predicate is not None and (not isinstance(predicate, dict)):
            predicate = predicate.to_dict()
        metadata = _make_metadata(self.table_name, self.app_profile_id)
        result = self.client._gapic_client.check_and_mutate_row(
            request={
                "predicate_filter": predicate,
                "true_mutations": true_case_dict,
                "false_mutations": false_case_dict,
                "table_name": self.table_name,
                "row_key": row_key,
                "app_profile_id": self.app_profile_id,
            },
            metadata=metadata,
            timeout=operation_timeout,
        )
        return result.predicate_matched

    def read_modify_write_row(
        self,
        row_key: str | bytes,
        rules: ReadModifyWriteRule | list[ReadModifyWriteRule],
        *,
        operation_timeout: int | float | None = 20,
    ) -> Row:
        """
        Reads and modifies a row atomically according to input ReadModifyWriteRules,
        and returns the contents of all modified cells

        The new value for the timestamp is the greater of the existing timestamp or
        the current server time.

        Non-idempotent operation: will not be retried

        Args:
            - row_key: the key of the row to apply read/modify/write rules to
            - rules: A rule or set of rules to apply to the row.
                Rules are applied in order, meaning that earlier rules will affect the
                results of later ones.
           - operation_timeout: the time budget for the entire operation, in seconds.
                Failed requests will not be retried.
        Returns:
            - Row: containing cell data that was modified as part of the
                operation
        Raises:
            - GoogleAPIError exceptions from grpc call
        """
        operation_timeout = operation_timeout or self.default_operation_timeout
        row_key = row_key.encode("utf-8") if isinstance(row_key, str) else row_key
        if operation_timeout <= 0:
            raise ValueError("operation_timeout must be greater than 0")
        if rules is not None and (not isinstance(rules, list)):
            rules = [rules]
        if not rules:
            raise ValueError("rules must contain at least one item")
        rules_dict = [rule._to_dict() for rule in rules]
        metadata = _make_metadata(self.table_name, self.app_profile_id)
        result = self.client._gapic_client.read_modify_write_row(
            request={
                "rules": rules_dict,
                "table_name": self.table_name,
                "row_key": row_key,
                "app_profile_id": self.app_profile_id,
            },
            metadata=metadata,
            timeout=operation_timeout,
        )
        return Row._from_pb(result.row)

    def close(self):
        """Called to close the Table_Sync instance and release any resources held by it."""
        self.client._remove_instance_registration(self.instance_id, self)

    def __enter__(self):
        """
        Implement async context manager protocol

        Register this instance with the client, so that
        grpc channels will be warmed for the specified instance
        """
        self.client._register_instance(self.instance_id, self)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Implement async context manager protocol

        Unregister this instance with the client, so that
        grpc channels will no longer be warmed
        """
        self.close()


class BigtableDataClient_Sync(ClientWithProject, ABC):
    def __init__(
        self,
        *,
        project: str | None = None,
        pool_size: int = 3,
        credentials: google.auth.credentials.Credentials | None = None,
        client_options: dict[str, Any]
        | "google.api_core.client_options.ClientOptions"
        | None = None,
    ):
        """
        Create a client instance for the Bigtable Data API

        Client should be created within an async context (running event loop)

        Args:
            project: the project which the client acts on behalf of.
                If not passed, falls back to the default inferred
                from the environment.
            pool_size: The number of grpc channels to maintain
                in the internal channel pool.
            credentials:
                Thehe OAuth2 Credentials to use for this
                client. If not passed (and if no ``_http`` object is
                passed), falls back to the default inferred from the
                environment.
            client_options (Optional[Union[dict, google.api_core.client_options.ClientOptions]]):
                Client options used to set user options
                on the client. API Endpoint should be set through client_options.
        Raises:
          - RuntimeError if called outside of an async context (no running event loop)
          - ValueError if pool_size is less than 1
        """
        client_info = DEFAULT_CLIENT_INFO
        client_info.client_library_version = client_info.gapic_version
        if type(client_options) is dict:
            client_options = client_options_lib.from_dict(client_options)
        client_options = cast(
            Optional[client_options_lib.ClientOptions], client_options
        )
        ClientWithProject.__init__(
            self,
            credentials=credentials,
            project=project,
            client_options=client_options,
        )
        transport_str = self.__init__transport__(pool_size)
        self._gapic_client = BigtableClient(
            transport=transport_str,
            credentials=credentials,
            client_options=client_options,
            client_info=client_info,
        )
        self.transport = cast(BigtableGrpcTransport, self._gapic_client.transport)
        self._active_instances: Set[str] = set()
        self._instance_owners: dict[str, Set[int]] = {}
        self._channel_init_time = time.time()
        self._channel_refresh_tasks: list[asyncio.Task[None]] = []
        try:
            self.start_background_channel_refresh()
        except RuntimeError:
            warnings.warn(
                f"{self.__class__.__name__} should be started in an asyncio event loop. Channel refresh will not be started",
                RuntimeWarning,
                stacklevel=2,
            )

    def __init__transport__(self, pool_size: int):
        """Implementation purposely removed in sync mode"""

    def start_background_channel_refresh(self) -> None:
        """Implementation purposely removed in sync mode"""

    def close(self, timeout: float = 2.0):
        raise NotImplementedError(
            "Corresponding Async Function contains unhandled asyncio calls"
        )

    def _ping_and_warm_instances(
        self, channel: Channel
    ) -> list[GoogleAPICallError | None]:
        raise NotImplementedError(
            "Corresponding Async Function contains unhandled asyncio calls"
        )

    def _manage_channel(
        self,
        channel_idx: int,
        refresh_interval_min: float = 60 * 35,
        refresh_interval_max: float = 60 * 45,
        grace_period: float = 60 * 10,
    ) -> None:
        """Implementation purposely removed in sync mode"""

    def _register_instance(self, instance_id: str, owner: Table_Sync) -> None:
        """Implementation purposely removed in sync mode"""

    def _remove_instance_registration(
        self, instance_id: str, owner: Table_Sync
    ) -> bool:
        """
        Removes an instance from the client's registered instances, to prevent
        warming new channels for the instance

        If instance_id is not registered, or is still in use by other tables, returns False

        Args:
            - instance_id: id of the instance to remove
            - owner: table that owns the instance. Owners will be tracked in
              _instance_owners, and instances will only be unregistered when all
              owners call _remove_instance_registration
        Returns:
            - True if instance was removed
        """
        instance_name = self._gapic_client.instance_path(self.project, instance_id)
        owner_list = self._instance_owners.get(instance_name, set())
        try:
            owner_list.remove(id(owner))
            if len(owner_list) == 0:
                self._active_instances.remove(instance_name)
            return True
        except KeyError:
            return False

    def get_table(
        self,
        instance_id: str,
        table_id: str,
        app_profile_id: str | None = None,
        default_operation_timeout: float = 600,
        default_per_request_timeout: float | None = None,
    ) -> Table_Sync:
        """
        Returns a table instance for making data API requests

        Args:
            instance_id: The Bigtable instance ID to associate with this client.
                instance_id is combined with the client's project to fully
                specify the instance
            table_id: The ID of the table.
            app_profile_id: (Optional) The app profile to associate with requests.
                https://cloud.google.com/bigtable/docs/app-profiles
        """
        return google.cloud.bigtable._sync._concrete.Table_Sync_Concrete(
            self,
            instance_id,
            table_id,
            app_profile_id,
            default_operation_timeout=default_operation_timeout,
            default_per_request_timeout=default_per_request_timeout,
        )

    def __enter__(self):
        self.start_background_channel_refresh()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
        self._gapic_client.__exit__(exc_type, exc_val, exc_tb)


class ReadRowsIterator_Sync(Iterable[Row], ABC):
    """
    Async iterator for ReadRows responses.
    """

    def __init__(self, merger: _ReadRowsOperation_Sync):
        self._merger: _ReadRowsOperation_Sync = merger
        self._error: Exception | None = None
        self.last_interaction_time = time.time()
        self._idle_timeout_task: asyncio.Task[None] | None = None
        self._next_fn = _convert_retry_deadline(
            self._merger.__next__,
            self._merger.operation_timeout,
            self._merger.transient_errors,
        )

    def _start_idle_timer(self, idle_timeout: float):
        """Implementation purposely removed in sync mode"""

    @property
    def active(self):
        """Returns True if the iterator is still active and has not been closed"""
        return self._error is None

    def __iter__(self):
        """Implement the Iterator protocol."""
        return self

    def __next__(self) -> Row:
        """
        Implement the Iterator potocol.

        Return the next item in the stream if active, or
        raise an exception if the stream has been closed.
        """
        if self._error is not None:
            raise self._error
        try:
            self.last_interaction_time = time.time()
            return self._next_fn()
        except Exception as e:
            self._finish_with_error(e)
            raise e

    def _finish_with_error(self, e: Exception):
        """
        Helper function to close the stream and clean up resources
        after an error has occurred.
        """
        if self.active:
            self._merger.close()
            self._error = e
        if self._idle_timeout_task is not None:
            self._idle_timeout_task.cancel()
            self._idle_timeout_task = None

    def close(self):
        """Support closing the stream with an explicit call to aclose()"""
        self._finish_with_error(StopIteration(f"{self.__class__.__name__} closed"))


class _MutateRowsOperation_Sync(ABC):
    """
    MutateRowsOperation manages the logic of sending a set of row mutations,
    and retrying on failed entries. It manages this using the _run_attempt
    function, which attempts to mutate all outstanding entries, and raises
    _MutateRowsIncomplete if any retryable errors are encountered.

    Errors are exposed as a MutationsExceptionGroup, which contains a list of
    exceptions organized by the related failed mutation entries.
    """

    def __init__(
        self,
        gapic_client: "BigtableAsyncClient",
        table: "Table",
        mutation_entries: list["RowMutationEntry"],
        operation_timeout: float,
        per_request_timeout: float | None,
    ):
        """
        Args:
          - gapic_client: the client to use for the mutate_rows call
          - table: the table associated with the request
          - mutation_entries: a list of RowMutationEntry objects to send to the server
          - operation_timeout: the timeout t o use for the entire operation, in seconds.
          - per_request_timeout: the timeoutto use for each mutate_rows attempt, in seconds.
              If not specified, the request will run until operation_timeout is reached.
        """
        metadata = _make_metadata(table.table_name, table.app_profile_id)
        self._gapic_fn = functools.partial(
            gapic_client.mutate_rows,
            table_name=table.table_name,
            app_profile_id=table.app_profile_id,
            metadata=metadata,
        )
        self.is_retryable = retries.if_exception_type(
            core_exceptions.DeadlineExceeded,
            core_exceptions.ServiceUnavailable,
            _MutateRowsIncomplete,
        )
        retry = retries.Retry(
            predicate=self.is_retryable,
            timeout=operation_timeout,
            initial=0.01,
            multiplier=2,
            maximum=60,
        )
        retry_wrapped = retry(self._run_attempt)
        self._operation = _convert_retry_deadline(retry_wrapped, operation_timeout)
        self.timeout_generator = _attempt_timeout_generator(
            per_request_timeout, operation_timeout
        )
        self.mutations = mutation_entries
        self.remaining_indices = list(range(len(self.mutations)))
        self.errors: dict[int, list[Exception]] = {}

    def start(self):
        """
        Start the operation, and run until completion

        Raises:
          - MutationsExceptionGroup: if any mutations failed
        """
        try:
            self._operation()
        except Exception as exc:
            incomplete_indices = self.remaining_indices.copy()
            for idx in incomplete_indices:
                self._handle_entry_error(idx, exc)
        finally:
            all_errors = []
            for idx, exc_list in self.errors.items():
                if len(exc_list) == 0:
                    raise core_exceptions.ClientError(
                        f"Mutation {idx} failed with no associated errors"
                    )
                elif len(exc_list) == 1:
                    cause_exc = exc_list[0]
                else:
                    cause_exc = bt_exceptions.RetryExceptionGroup(exc_list)
                entry = self.mutations[idx]
                all_errors.append(
                    bt_exceptions.FailedMutationEntryError(idx, entry, cause_exc)
                )
            if all_errors:
                raise bt_exceptions.MutationsExceptionGroup(
                    all_errors, len(self.mutations)
                )

    def _run_attempt(self):
        """
        Run a single attempt of the mutate_rows rpc.

        Raises:
          - _MutateRowsIncomplete: if there are failed mutations eligible for
              retry after the attempt is complete
          - GoogleAPICallError: if the gapic rpc fails
        """
        request_entries = [
            self.mutations[idx]._to_dict() for idx in self.remaining_indices
        ]
        active_request_indices = {
            req_idx: orig_idx
            for (req_idx, orig_idx) in enumerate(self.remaining_indices)
        }
        self.remaining_indices = []
        if not request_entries:
            return
        try:
            result_generator = self._gapic_fn(
                timeout=next(self.timeout_generator), entries=request_entries
            )
            for result_list in result_generator:
                for result in result_list.entries:
                    orig_idx = active_request_indices[result.index]
                    entry_error = core_exceptions.from_grpc_status(
                        result.status.code,
                        result.status.message,
                        details=result.status.details,
                    )
                    if result.status.code != 0:
                        self._handle_entry_error(orig_idx, entry_error)
                    del active_request_indices[result.index]
        except Exception as exc:
            for idx in active_request_indices.values():
                self._handle_entry_error(idx, exc)
                raise
        if self.remaining_indices:
            raise _MutateRowsIncomplete

    def _handle_entry_error(self, idx: int, exc: Exception):
        """
        Add an exception to the list of exceptions for a given mutation index,
        and add the index to the list of remaining indices if the exception is
        retryable.

        Args:
          - idx: the index of the mutation that failed
          - exc: the exception to add to the list
        """
        entry = self.mutations[idx]
        self.errors.setdefault(idx, []).append(exc)
        if (
            entry.is_idempotent()
            and self.is_retryable(exc)
            and (idx not in self.remaining_indices)
        ):
            self.remaining_indices.append(idx)
