# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# This file is automatically generated by sync_surface_generator.py. Do not edit.


from __future__ import annotations
from abc import ABC
from itertools import zip_longest
from tests.unit.data._async.test__mutate_rows import TestMutateRowsOperation
from tests.unit.data._async.test__read_rows import TestReadRowsOperation
from tests.unit.data._async.test_mutations_batcher import Test_FlowControl
from tests.unit.v2_client.test_row_merger import ReadRowsTest
from tests.unit.v2_client.test_row_merger import TestFile
from unittest import mock
import asyncio
import concurrent.futures
import grpc
import mock
import os
import pytest
import re
import threading
import time
import warnings

from google.api_core import exceptions as core_exceptions
from google.api_core import grpc_helpers
from google.auth.credentials import AnonymousCredentials
from google.cloud.bigtable.data import ReadRowsQuery
from google.cloud.bigtable.data import TABLE_DEFAULT
from google.cloud.bigtable.data import Table
from google.cloud.bigtable.data import mutations
from google.cloud.bigtable.data.exceptions import InvalidChunk
from google.cloud.bigtable.data.exceptions import _MutateRowsIncomplete
from google.cloud.bigtable.data.read_modify_write_rules import AppendValueRule
from google.cloud.bigtable.data.read_modify_write_rules import IncrementRule
from google.cloud.bigtable.data.read_rows_query import ReadRowsQuery
from google.cloud.bigtable.data.row import Row
from google.cloud.bigtable_v2 import ReadRowsResponse
from google.cloud.bigtable_v2.services.bigtable.client import BigtableClient
from google.cloud.bigtable_v2.services.bigtable.transports.pooled_grpc import (
    PooledBigtableGrpcTransport,
)
from google.cloud.bigtable_v2.services.bigtable.transports.pooled_grpc import (
    PooledChannel,
)
from google.cloud.bigtable_v2.types import MutateRowsResponse
from google.cloud.bigtable_v2.types import ReadRowsResponse
from google.rpc import status_pb2
import google.api_core.exceptions
import google.api_core.exceptions as core_exceptions
import google.api_core.retry


class TestMutateRowsOperation(ABC):
    def _target_class(self):
        from google.cloud.bigtable.data._sync._mutate_rows import _MutateRowsOperation

        return _MutateRowsOperation

    def _make_one(self, *args, **kwargs):
        if not args:
            kwargs["gapic_client"] = kwargs.pop("gapic_client", mock.Mock())
            kwargs["table"] = kwargs.pop("table", mock.Mock())
            kwargs["operation_timeout"] = kwargs.pop("operation_timeout", 5)
            kwargs["attempt_timeout"] = kwargs.pop("attempt_timeout", 0.1)
            kwargs["retryable_exceptions"] = kwargs.pop("retryable_exceptions", ())
            kwargs["mutation_entries"] = kwargs.pop("mutation_entries", [])
        return self._target_class()(*args, **kwargs)

    def _make_mutation(self, count=1, size=1):
        mutation = mock.Mock()
        mutation.size.return_value = size
        mutation.mutations = [mock.Mock()] * count
        return mutation

    def _mock_stream(self, mutation_list, error_dict):
        for idx, entry in enumerate(mutation_list):
            code = error_dict.get(idx, 0)
            yield MutateRowsResponse(
                entries=[
                    MutateRowsResponse.Entry(
                        index=idx, status=status_pb2.Status(code=code)
                    )
                ]
            )

    def _make_mock_gapic(self, mutation_list, error_dict=None):
        mock_fn = mock.Mock()
        if error_dict is None:
            error_dict = {}
        mock_fn.side_effect = lambda *args, **kwargs: self._mock_stream(
            mutation_list, error_dict
        )
        return mock_fn

    def test_ctor(self):
        """test that constructor sets all the attributes correctly"""
        from google.cloud.bigtable.data._async._mutate_rows import _EntryWithProto
        from google.cloud.bigtable.data.exceptions import _MutateRowsIncomplete
        from google.api_core.exceptions import DeadlineExceeded
        from google.api_core.exceptions import Aborted

        client = mock.Mock()
        table = mock.Mock()
        entries = [self._make_mutation(), self._make_mutation()]
        operation_timeout = 0.05
        attempt_timeout = 0.01
        retryable_exceptions = ()
        instance = self._make_one(
            client,
            table,
            entries,
            operation_timeout,
            attempt_timeout,
            retryable_exceptions,
        )
        assert client.mutate_rows.call_count == 0
        instance._gapic_fn()
        assert client.mutate_rows.call_count == 1
        inner_kwargs = client.mutate_rows.call_args[1]
        assert len(inner_kwargs) == 4
        assert inner_kwargs["table_name"] == table.table_name
        assert inner_kwargs["app_profile_id"] == table.app_profile_id
        assert inner_kwargs["retry"] is None
        metadata = inner_kwargs["metadata"]
        assert len(metadata) == 1
        assert metadata[0][0] == "x-goog-request-params"
        assert str(table.table_name) in metadata[0][1]
        assert str(table.app_profile_id) in metadata[0][1]
        entries_w_pb = [_EntryWithProto(e, e._to_pb()) for e in entries]
        assert instance.mutations == entries_w_pb
        assert next(instance.timeout_generator) == attempt_timeout
        assert instance.is_retryable is not None
        assert instance.is_retryable(DeadlineExceeded("")) is False
        assert instance.is_retryable(Aborted("")) is False
        assert instance.is_retryable(_MutateRowsIncomplete("")) is True
        assert instance.is_retryable(RuntimeError("")) is False
        assert instance.remaining_indices == list(range(len(entries)))
        assert instance.errors == {}

    def test_ctor_too_many_entries(self):
        """should raise an error if an operation is created with more than 100,000 entries"""
        from google.cloud.bigtable.data._async._mutate_rows import (
            _MUTATE_ROWS_REQUEST_MUTATION_LIMIT,
        )

        assert _MUTATE_ROWS_REQUEST_MUTATION_LIMIT == 100000
        client = mock.Mock()
        table = mock.Mock()
        entries = [self._make_mutation()] * (_MUTATE_ROWS_REQUEST_MUTATION_LIMIT + 1)
        operation_timeout = 0.05
        attempt_timeout = 0.01
        with pytest.raises(ValueError) as e:
            self._make_one(client, table, entries, operation_timeout, attempt_timeout)
        assert "mutate_rows requests can contain at most 100000 mutations" in str(
            e.value
        )
        assert "Found 100001" in str(e.value)

    def test_mutate_rows_operation(self):
        """Test successful case of mutate_rows_operation"""
        client = mock.Mock()
        table = mock.Mock()
        entries = [self._make_mutation(), self._make_mutation()]
        operation_timeout = 0.05
        cls = self._target_class()
        with mock.patch(
            f"{cls.__module__}.{cls.__name__}._run_attempt", mock.Mock()
        ) as attempt_mock:
            instance = self._make_one(
                client, table, entries, operation_timeout, operation_timeout
            )
            instance.start()
            assert attempt_mock.call_count == 1

    @pytest.mark.parametrize(
        "exc_type", [RuntimeError, ZeroDivisionError, core_exceptions.Forbidden]
    )
    def test_mutate_rows_attempt_exception(self, exc_type):
        """exceptions raised from attempt should be raised in MutationsExceptionGroup"""
        client = mock.Mock()
        table = mock.Mock()
        entries = [self._make_mutation(), self._make_mutation()]
        operation_timeout = 0.05
        expected_exception = exc_type("test")
        client.mutate_rows.side_effect = expected_exception
        found_exc = None
        try:
            instance = self._make_one(
                client, table, entries, operation_timeout, operation_timeout
            )
            instance._run_attempt()
        except Exception as e:
            found_exc = e
        assert client.mutate_rows.call_count == 1
        assert type(found_exc) is exc_type
        assert found_exc == expected_exception
        assert len(instance.errors) == 2
        assert len(instance.remaining_indices) == 0

    @pytest.mark.parametrize(
        "exc_type", [RuntimeError, ZeroDivisionError, core_exceptions.Forbidden]
    )
    def test_mutate_rows_exception(self, exc_type):
        """exceptions raised from retryable should be raised in MutationsExceptionGroup"""
        from google.cloud.bigtable.data.exceptions import MutationsExceptionGroup
        from google.cloud.bigtable.data.exceptions import FailedMutationEntryError

        client = mock.Mock()
        table = mock.Mock()
        entries = [self._make_mutation(), self._make_mutation()]
        operation_timeout = 0.05
        expected_cause = exc_type("abort")
        with mock.patch.object(
            self._target_class(), "_run_attempt", mock.Mock()
        ) as attempt_mock:
            attempt_mock.side_effect = expected_cause
            found_exc = None
            try:
                instance = self._make_one(
                    client, table, entries, operation_timeout, operation_timeout
                )
                instance.start()
            except MutationsExceptionGroup as e:
                found_exc = e
            assert attempt_mock.call_count == 1
            assert len(found_exc.exceptions) == 2
            assert isinstance(found_exc.exceptions[0], FailedMutationEntryError)
            assert isinstance(found_exc.exceptions[1], FailedMutationEntryError)
            assert found_exc.exceptions[0].__cause__ == expected_cause
            assert found_exc.exceptions[1].__cause__ == expected_cause

    @pytest.mark.parametrize(
        "exc_type", [core_exceptions.DeadlineExceeded, RuntimeError]
    )
    def test_mutate_rows_exception_retryable_eventually_pass(self, exc_type):
        """If an exception fails but eventually passes, it should not raise an exception"""
        client = mock.Mock()
        table = mock.Mock()
        entries = [self._make_mutation()]
        operation_timeout = 1
        expected_cause = exc_type("retry")
        num_retries = 2
        with mock.patch.object(
            self._target_class(), "_run_attempt", mock.Mock()
        ) as attempt_mock:
            attempt_mock.side_effect = [expected_cause] * num_retries + [None]
            instance = self._make_one(
                client,
                table,
                entries,
                operation_timeout,
                operation_timeout,
                retryable_exceptions=(exc_type,),
            )
            instance.start()
            assert attempt_mock.call_count == num_retries + 1

    def test_mutate_rows_incomplete_ignored(self):
        """MutateRowsIncomplete exceptions should not be added to error list"""
        from google.cloud.bigtable.data.exceptions import _MutateRowsIncomplete
        from google.cloud.bigtable.data.exceptions import MutationsExceptionGroup
        from google.api_core.exceptions import DeadlineExceeded

        client = mock.Mock()
        table = mock.Mock()
        entries = [self._make_mutation()]
        operation_timeout = 0.05
        with mock.patch.object(
            self._target_class(), "_run_attempt", mock.Mock()
        ) as attempt_mock:
            attempt_mock.side_effect = _MutateRowsIncomplete("ignored")
            found_exc = None
            try:
                instance = self._make_one(
                    client, table, entries, operation_timeout, operation_timeout
                )
                instance.start()
            except MutationsExceptionGroup as e:
                found_exc = e
            assert attempt_mock.call_count > 0
            assert len(found_exc.exceptions) == 1
            assert isinstance(found_exc.exceptions[0].__cause__, DeadlineExceeded)

    def test_run_attempt_single_entry_success(self):
        """Test mutating a single entry"""
        mutation = self._make_mutation()
        expected_timeout = 1.3
        mock_gapic_fn = self._make_mock_gapic({0: mutation})
        instance = self._make_one(
            mutation_entries=[mutation], attempt_timeout=expected_timeout
        )
        with mock.patch.object(instance, "_gapic_fn", mock_gapic_fn):
            instance._run_attempt()
        assert len(instance.remaining_indices) == 0
        assert mock_gapic_fn.call_count == 1
        (_, kwargs) = mock_gapic_fn.call_args
        assert kwargs["timeout"] == expected_timeout
        assert kwargs["entries"] == [mutation._to_pb()]

    def test_run_attempt_empty_request(self):
        """Calling with no mutations should result in no API calls"""
        mock_gapic_fn = self._make_mock_gapic([])
        instance = self._make_one(mutation_entries=[])
        instance._run_attempt()
        assert mock_gapic_fn.call_count == 0

    def test_run_attempt_partial_success_retryable(self):
        """Some entries succeed, but one fails. Should report the proper index, and raise incomplete exception"""
        from google.cloud.bigtable.data.exceptions import _MutateRowsIncomplete

        success_mutation = self._make_mutation()
        success_mutation_2 = self._make_mutation()
        failure_mutation = self._make_mutation()
        mutations = [success_mutation, failure_mutation, success_mutation_2]
        mock_gapic_fn = self._make_mock_gapic(mutations, error_dict={1: 300})
        instance = self._make_one(mutation_entries=mutations)
        instance.is_retryable = lambda x: True
        with mock.patch.object(instance, "_gapic_fn", mock_gapic_fn):
            with pytest.raises(_MutateRowsIncomplete):
                instance._run_attempt()
        assert instance.remaining_indices == [1]
        assert 0 not in instance.errors
        assert len(instance.errors[1]) == 1
        assert instance.errors[1][0].grpc_status_code == 300
        assert 2 not in instance.errors

    def test_run_attempt_partial_success_non_retryable(self):
        """Some entries succeed, but one fails. Exception marked as non-retryable. Do not raise incomplete error"""
        success_mutation = self._make_mutation()
        success_mutation_2 = self._make_mutation()
        failure_mutation = self._make_mutation()
        mutations = [success_mutation, failure_mutation, success_mutation_2]
        mock_gapic_fn = self._make_mock_gapic(mutations, error_dict={1: 300})
        instance = self._make_one(mutation_entries=mutations)
        instance.is_retryable = lambda x: False
        with mock.patch.object(instance, "_gapic_fn", mock_gapic_fn):
            instance._run_attempt()
        assert instance.remaining_indices == []
        assert 0 not in instance.errors
        assert len(instance.errors[1]) == 1
        assert instance.errors[1][0].grpc_status_code == 300
        assert 2 not in instance.errors


class TestReadRowsOperation(ABC):
    """
    Tests helper functions in the ReadRowsOperation class
    in-depth merging logic in merge_row_response_stream and _read_rows_retryable_attempt
    is tested in test_read_rows_acceptance test_client_read_rows, and conformance tests
    """

    @staticmethod
    def _get_target_class():
        from google.cloud.bigtable.data._sync._read_rows import _ReadRowsOperation

        return _ReadRowsOperation

    def _make_one(self, *args, **kwargs):
        return self._get_target_class()(*args, **kwargs)

    def test_ctor(self):
        from google.cloud.bigtable.data import ReadRowsQuery

        row_limit = 91
        query = ReadRowsQuery(limit=row_limit)
        client = mock.Mock()
        client.read_rows = mock.Mock()
        client.read_rows.return_value = None
        table = mock.Mock()
        table._client = client
        table.table_name = "test_table"
        table.app_profile_id = "test_profile"
        expected_operation_timeout = 42
        expected_request_timeout = 44
        time_gen_mock = mock.Mock()
        with mock.patch(
            "google.cloud.bigtable.data._helpers._attempt_timeout_generator",
            time_gen_mock,
        ):
            instance = self._make_one(
                query,
                table,
                operation_timeout=expected_operation_timeout,
                attempt_timeout=expected_request_timeout,
            )
        assert time_gen_mock.call_count == 1
        time_gen_mock.assert_called_once_with(
            expected_request_timeout, expected_operation_timeout
        )
        assert instance._last_yielded_row_key is None
        assert instance._remaining_count == row_limit
        assert instance.operation_timeout == expected_operation_timeout
        assert client.read_rows.call_count == 0
        assert instance._metadata == [
            (
                "x-goog-request-params",
                "table_name=test_table&app_profile_id=test_profile",
            )
        ]
        assert instance.request.table_name == table.table_name
        assert instance.request.app_profile_id == table.app_profile_id
        assert instance.request.rows_limit == row_limit

    @pytest.mark.parametrize(
        "in_keys,last_key,expected",
        [
            (["b", "c", "d"], "a", ["b", "c", "d"]),
            (["a", "b", "c"], "b", ["c"]),
            (["a", "b", "c"], "c", []),
            (["a", "b", "c"], "d", []),
            (["d", "c", "b", "a"], "b", ["d", "c"]),
        ],
    )
    def test_revise_request_rowset_keys(self, in_keys, last_key, expected):
        from google.cloud.bigtable_v2.types import RowSet as RowSetPB
        from google.cloud.bigtable_v2.types import RowRange as RowRangePB

        in_keys = [key.encode("utf-8") for key in in_keys]
        expected = [key.encode("utf-8") for key in expected]
        last_key = last_key.encode("utf-8")
        sample_range = RowRangePB(start_key_open=last_key)
        row_set = RowSetPB(row_keys=in_keys, row_ranges=[sample_range])
        revised = self._get_target_class()._revise_request_rowset(row_set, last_key)
        assert revised.row_keys == expected
        assert revised.row_ranges == [sample_range]

    @pytest.mark.parametrize(
        "in_ranges,last_key,expected",
        [
            (
                [{"start_key_open": "b", "end_key_closed": "d"}],
                "a",
                [{"start_key_open": "b", "end_key_closed": "d"}],
            ),
            (
                [{"start_key_closed": "b", "end_key_closed": "d"}],
                "a",
                [{"start_key_closed": "b", "end_key_closed": "d"}],
            ),
            (
                [{"start_key_open": "a", "end_key_closed": "d"}],
                "b",
                [{"start_key_open": "b", "end_key_closed": "d"}],
            ),
            (
                [{"start_key_closed": "a", "end_key_open": "d"}],
                "b",
                [{"start_key_open": "b", "end_key_open": "d"}],
            ),
            (
                [{"start_key_closed": "b", "end_key_closed": "d"}],
                "b",
                [{"start_key_open": "b", "end_key_closed": "d"}],
            ),
            ([{"start_key_closed": "b", "end_key_closed": "d"}], "d", []),
            ([{"start_key_closed": "b", "end_key_open": "d"}], "d", []),
            ([{"start_key_closed": "b", "end_key_closed": "d"}], "e", []),
            ([{"start_key_closed": "b"}], "z", [{"start_key_open": "z"}]),
            ([{"start_key_closed": "b"}], "a", [{"start_key_closed": "b"}]),
            (
                [{"end_key_closed": "z"}],
                "a",
                [{"start_key_open": "a", "end_key_closed": "z"}],
            ),
            (
                [{"end_key_open": "z"}],
                "a",
                [{"start_key_open": "a", "end_key_open": "z"}],
            ),
        ],
    )
    def test_revise_request_rowset_ranges(self, in_ranges, last_key, expected):
        from google.cloud.bigtable_v2.types import RowSet as RowSetPB
        from google.cloud.bigtable_v2.types import RowRange as RowRangePB

        next_key = (last_key + "a").encode("utf-8")
        last_key = last_key.encode("utf-8")
        in_ranges = [
            RowRangePB(**{k: v.encode("utf-8") for (k, v) in r.items()})
            for r in in_ranges
        ]
        expected = [
            RowRangePB(**{k: v.encode("utf-8") for (k, v) in r.items()})
            for r in expected
        ]
        row_set = RowSetPB(row_ranges=in_ranges, row_keys=[next_key])
        revised = self._get_target_class()._revise_request_rowset(row_set, last_key)
        assert revised.row_keys == [next_key]
        assert revised.row_ranges == expected

    @pytest.mark.parametrize("last_key", ["a", "b", "c"])
    def test_revise_request_full_table(self, last_key):
        from google.cloud.bigtable_v2.types import RowSet as RowSetPB
        from google.cloud.bigtable_v2.types import RowRange as RowRangePB

        last_key = last_key.encode("utf-8")
        row_set = RowSetPB()
        for selected_set in [row_set, None]:
            revised = self._get_target_class()._revise_request_rowset(
                selected_set, last_key
            )
            assert revised.row_keys == []
            assert len(revised.row_ranges) == 1
            assert revised.row_ranges[0] == RowRangePB(start_key_open=last_key)

    def test_revise_to_empty_rowset(self):
        """revising to an empty rowset should raise error"""
        from google.cloud.bigtable.data.exceptions import _RowSetComplete
        from google.cloud.bigtable_v2.types import RowSet as RowSetPB
        from google.cloud.bigtable_v2.types import RowRange as RowRangePB

        row_keys = [b"a", b"b", b"c"]
        row_range = RowRangePB(end_key_open=b"c")
        row_set = RowSetPB(row_keys=row_keys, row_ranges=[row_range])
        with pytest.raises(_RowSetComplete):
            self._get_target_class()._revise_request_rowset(row_set, b"d")

    @pytest.mark.parametrize(
        "start_limit,emit_num,expected_limit",
        [
            (10, 0, 10),
            (10, 1, 9),
            (10, 10, 0),
            (None, 10, None),
            (None, 0, None),
            (4, 2, 2),
        ],
    )
    def test_revise_limit(self, start_limit, emit_num, expected_limit):
        """
        revise_limit should revise the request's limit field
        - if limit is 0 (unlimited), it should never be revised
        - if start_limit-emit_num == 0, the request should end early
        - if the number emitted exceeds the new limit, an exception should
          should be raised (tested in test_revise_limit_over_limit)
        """
        from google.cloud.bigtable.data import ReadRowsQuery
        from google.cloud.bigtable_v2.types import ReadRowsResponse

        def awaitable_stream():
            def mock_stream():
                for i in range(emit_num):
                    yield ReadRowsResponse(
                        chunks=[
                            ReadRowsResponse.CellChunk(
                                row_key=str(i).encode(),
                                family_name="b",
                                qualifier=b"c",
                                value=b"d",
                                commit_row=True,
                            )
                        ]
                    )

            return mock_stream()

        query = ReadRowsQuery(limit=start_limit)
        table = mock.Mock()
        table.table_name = "table_name"
        table.app_profile_id = "app_profile_id"
        instance = self._make_one(query, table, 10, 10)
        assert instance._remaining_count == start_limit
        for val in instance.chunk_stream(awaitable_stream()):
            pass
        assert instance._remaining_count == expected_limit

    @pytest.mark.parametrize("start_limit,emit_num", [(5, 10), (3, 9), (1, 10)])
    def test_revise_limit_over_limit(self, start_limit, emit_num):
        """
        Should raise runtime error if we get in state where emit_num > start_num
        (unless start_num == 0, which represents unlimited)
        """
        from google.cloud.bigtable.data import ReadRowsQuery
        from google.cloud.bigtable_v2.types import ReadRowsResponse
        from google.cloud.bigtable.data.exceptions import InvalidChunk

        def awaitable_stream():
            def mock_stream():
                for i in range(emit_num):
                    yield ReadRowsResponse(
                        chunks=[
                            ReadRowsResponse.CellChunk(
                                row_key=str(i).encode(),
                                family_name="b",
                                qualifier=b"c",
                                value=b"d",
                                commit_row=True,
                            )
                        ]
                    )

            return mock_stream()

        query = ReadRowsQuery(limit=start_limit)
        table = mock.Mock()
        table.table_name = "table_name"
        table.app_profile_id = "app_profile_id"
        instance = self._make_one(query, table, 10, 10)
        assert instance._remaining_count == start_limit
        with pytest.raises(InvalidChunk) as e:
            for val in instance.chunk_stream(awaitable_stream()):
                pass
        assert "emit count exceeds row limit" in str(e.value)

    def test_close(self):
        """
        should be able to close a stream safely with aclose.
        Closed generators should raise StopIteration on next yield
        """

        def mock_stream():
            while True:
                yield 1

        with mock.patch.object(
            self._get_target_class(), "_read_rows_attempt"
        ) as mock_attempt:
            instance = self._make_one(mock.Mock(), mock.Mock(), 1, 1)
            wrapped_gen = mock_stream()
            mock_attempt.return_value = wrapped_gen
            gen = instance.start_operation()
            gen.__next__()
            gen.close()
            with pytest.raises(StopIteration):
                gen.__next__()
            gen.close()
            with pytest.raises(StopIteration):
                wrapped_gen.__next__()

    def test_retryable_ignore_repeated_rows(self):
        """Duplicate rows should cause an invalid chunk error"""
        from google.cloud.bigtable.data.exceptions import InvalidChunk
        from google.cloud.bigtable_v2.types import ReadRowsResponse

        row_key = b"duplicate"

        def mock_awaitable_stream():
            def mock_stream():
                while True:
                    yield ReadRowsResponse(
                        chunks=[
                            ReadRowsResponse.CellChunk(row_key=row_key, commit_row=True)
                        ]
                    )
                    yield ReadRowsResponse(
                        chunks=[
                            ReadRowsResponse.CellChunk(row_key=row_key, commit_row=True)
                        ]
                    )

            return mock_stream()

        instance = mock.Mock()
        instance._last_yielded_row_key = None
        instance._remaining_count = None
        stream = self._get_target_class().chunk_stream(
            instance, mock_awaitable_stream()
        )
        stream.__next__()
        with pytest.raises(InvalidChunk) as exc:
            stream.__next__()
        assert "row keys should be strictly increasing" in str(exc.value)


class Test_FlowControl(ABC):
    @staticmethod
    def _target_class():
        from google.cloud.bigtable.data._sync.mutations_batcher import _FlowControl

        return _FlowControl

    def _make_one(self, max_mutation_count=10, max_mutation_bytes=100):
        return self._target_class()(max_mutation_count, max_mutation_bytes)

    @staticmethod
    def _make_mutation(count=1, size=1):
        mutation = mock.Mock()
        mutation.size.return_value = size
        mutation.mutations = [mock.Mock()] * count
        return mutation

    def test_ctor(self):
        max_mutation_count = 9
        max_mutation_bytes = 19
        instance = self._make_one(max_mutation_count, max_mutation_bytes)
        assert instance._max_mutation_count == max_mutation_count
        assert instance._max_mutation_bytes == max_mutation_bytes
        assert instance._in_flight_mutation_count == 0
        assert instance._in_flight_mutation_bytes == 0
        assert isinstance(instance._capacity_condition, threading.Condition)

    def test_ctor_invalid_values(self):
        """Test that values are positive, and fit within expected limits"""
        with pytest.raises(ValueError) as e:
            self._make_one(0, 1)
            assert "max_mutation_count must be greater than 0" in str(e.value)
        with pytest.raises(ValueError) as e:
            self._make_one(1, 0)
            assert "max_mutation_bytes must be greater than 0" in str(e.value)

    @pytest.mark.parametrize(
        "max_count,max_size,existing_count,existing_size,new_count,new_size,expected",
        [
            (1, 1, 0, 0, 0, 0, True),
            (1, 1, 1, 1, 1, 1, False),
            (10, 10, 0, 0, 0, 0, True),
            (10, 10, 0, 0, 9, 9, True),
            (10, 10, 0, 0, 11, 9, True),
            (10, 10, 0, 1, 11, 9, True),
            (10, 10, 1, 0, 11, 9, False),
            (10, 10, 0, 0, 9, 11, True),
            (10, 10, 1, 0, 9, 11, True),
            (10, 10, 0, 1, 9, 11, False),
            (10, 1, 0, 0, 1, 0, True),
            (1, 10, 0, 0, 0, 8, True),
            (float("inf"), float("inf"), 0, 0, 10000000000.0, 10000000000.0, True),
            (8, 8, 0, 0, 10000000000.0, 10000000000.0, True),
            (12, 12, 6, 6, 5, 5, True),
            (12, 12, 5, 5, 6, 6, True),
            (12, 12, 6, 6, 6, 6, True),
            (12, 12, 6, 6, 7, 7, False),
            (12, 12, 0, 0, 13, 13, True),
            (12, 12, 12, 0, 0, 13, True),
            (12, 12, 0, 12, 13, 0, True),
            (12, 12, 1, 1, 13, 13, False),
            (12, 12, 1, 1, 0, 13, False),
            (12, 12, 1, 1, 13, 0, False),
        ],
    )
    def test__has_capacity(
        self,
        max_count,
        max_size,
        existing_count,
        existing_size,
        new_count,
        new_size,
        expected,
    ):
        """_has_capacity should return True if the new mutation will will not exceed the max count or size"""
        instance = self._make_one(max_count, max_size)
        instance._in_flight_mutation_count = existing_count
        instance._in_flight_mutation_bytes = existing_size
        assert instance._has_capacity(new_count, new_size) == expected

    @pytest.mark.parametrize(
        "existing_count,existing_size,added_count,added_size,new_count,new_size",
        [
            (0, 0, 0, 0, 0, 0),
            (2, 2, 1, 1, 1, 1),
            (2, 0, 1, 0, 1, 0),
            (0, 2, 0, 1, 0, 1),
            (10, 10, 0, 0, 10, 10),
            (10, 10, 5, 5, 5, 5),
            (0, 0, 1, 1, -1, -1),
        ],
    )
    def test_remove_from_flow_value_update(
        self,
        existing_count,
        existing_size,
        added_count,
        added_size,
        new_count,
        new_size,
    ):
        """completed mutations should lower the inflight values"""
        instance = self._make_one()
        instance._in_flight_mutation_count = existing_count
        instance._in_flight_mutation_bytes = existing_size
        mutation = self._make_mutation(added_count, added_size)
        instance.remove_from_flow(mutation)
        assert instance._in_flight_mutation_count == new_count
        assert instance._in_flight_mutation_bytes == new_size

    def test__remove_from_flow_unlock(self):
        """capacity condition should notify after mutation is complete"""
        import inspect

        instance = self._make_one(10, 10)
        instance._in_flight_mutation_count = 10
        instance._in_flight_mutation_bytes = 10

        def task_routine():
            with instance._capacity_condition:
                instance._capacity_condition.wait_for(
                    lambda: instance._has_capacity(1, 1)
                )

        if inspect.iscoroutinefunction(task_routine):
            task = threading.Thread(task_routine())
            task_alive = lambda: not task.done()
        else:
            import threading

            thread = threading.Thread(target=task_routine)
            thread.start()
            task_alive = thread.is_alive
        time.sleep(0.05)
        assert task_alive() is True
        mutation = self._make_mutation(count=0, size=5)
        instance.remove_from_flow([mutation])
        time.sleep(0.05)
        assert instance._in_flight_mutation_count == 10
        assert instance._in_flight_mutation_bytes == 5
        assert task_alive() is True
        instance._in_flight_mutation_bytes = 10
        mutation = self._make_mutation(count=5, size=0)
        instance.remove_from_flow([mutation])
        time.sleep(0.05)
        assert instance._in_flight_mutation_count == 5
        assert instance._in_flight_mutation_bytes == 10
        assert task_alive() is True
        instance._in_flight_mutation_count = 10
        mutation = self._make_mutation(count=5, size=5)
        instance.remove_from_flow([mutation])
        time.sleep(0.05)
        assert instance._in_flight_mutation_count == 5
        assert instance._in_flight_mutation_bytes == 5
        assert task_alive() is False

    @pytest.mark.parametrize(
        "mutations,count_cap,size_cap,expected_results",
        [
            ([(5, 5), (1, 1), (1, 1)], 10, 10, [[(5, 5), (1, 1), (1, 1)]]),
            ([(1, 1), (1, 1), (1, 1)], 1, 1, [[(1, 1)], [(1, 1)], [(1, 1)]]),
            ([(1, 1), (1, 1), (1, 1)], 2, 10, [[(1, 1), (1, 1)], [(1, 1)]]),
            ([(1, 1), (1, 1), (1, 1)], 10, 2, [[(1, 1), (1, 1)], [(1, 1)]]),
            (
                [(1, 1), (5, 5), (4, 1), (1, 4), (1, 1)],
                5,
                5,
                [[(1, 1)], [(5, 5)], [(4, 1), (1, 4)], [(1, 1)]],
            ),
        ],
    )
    def test_add_to_flow(self, mutations, count_cap, size_cap, expected_results):
        """Test batching with various flow control settings"""
        mutation_objs = [self._make_mutation(count=m[0], size=m[1]) for m in mutations]
        instance = self._make_one(count_cap, size_cap)
        i = 0
        for batch in instance.add_to_flow(mutation_objs):
            expected_batch = expected_results[i]
            assert len(batch) == len(expected_batch)
            for j in range(len(expected_batch)):
                assert len(batch[j].mutations) == expected_batch[j][0]
                assert batch[j].size() == expected_batch[j][1]
            instance.remove_from_flow(batch)
            i += 1
        assert i == len(expected_results)

    @pytest.mark.parametrize(
        "mutations,max_limit,expected_results",
        [
            ([(1, 1)] * 11, 10, [[(1, 1)] * 10, [(1, 1)]]),
            ([(1, 1)] * 10, 1, [[(1, 1)] for _ in range(10)]),
            ([(1, 1)] * 10, 2, [[(1, 1), (1, 1)] for _ in range(5)]),
        ],
    )
    def test_add_to_flow_max_mutation_limits(
        self, mutations, max_limit, expected_results
    ):
        """
        Test flow control running up against the max API limit
        Should submit request early, even if the flow control has room for more
        """
        async_patch = mock.patch(
            "google.cloud.bigtable.data._async.mutations_batcher._MUTATE_ROWS_REQUEST_MUTATION_LIMIT",
            max_limit,
        )
        sync_patch = mock.patch(
            "google.cloud.bigtable.data._sync._autogen._MUTATE_ROWS_REQUEST_MUTATION_LIMIT",
            max_limit,
        )
        with async_patch, sync_patch:
            mutation_objs = [
                self._make_mutation(count=m[0], size=m[1]) for m in mutations
            ]
            instance = self._make_one(float("inf"), float("inf"))
            i = 0
            for batch in instance.add_to_flow(mutation_objs):
                expected_batch = expected_results[i]
                assert len(batch) == len(expected_batch)
                for j in range(len(expected_batch)):
                    assert len(batch[j].mutations) == expected_batch[j][0]
                    assert batch[j].size() == expected_batch[j][1]
                instance.remove_from_flow(batch)
                i += 1
            assert i == len(expected_results)

    def test_add_to_flow_oversize(self):
        """mutations over the flow control limits should still be accepted"""
        instance = self._make_one(2, 3)
        large_size_mutation = self._make_mutation(count=1, size=10)
        large_count_mutation = self._make_mutation(count=10, size=1)
        results = [out for out in instance.add_to_flow([large_size_mutation])]
        assert len(results) == 1
        instance.remove_from_flow(results[0])
        count_results = [out for out in instance.add_to_flow(large_count_mutation)]
        assert len(count_results) == 1


class TestMutationsBatcher(ABC):
    def _get_target_class(self):
        from google.cloud.bigtable.data._sync.mutations_batcher import MutationsBatcher

        return MutationsBatcher

    @staticmethod
    def is_async():
        return False

    def _make_one(self, table=None, **kwargs):
        from google.api_core.exceptions import DeadlineExceeded
        from google.api_core.exceptions import ServiceUnavailable

        if table is None:
            table = mock.Mock()
            table.default_mutate_rows_operation_timeout = 10
            table.default_mutate_rows_attempt_timeout = 10
            table.default_mutate_rows_retryable_errors = (
                DeadlineExceeded,
                ServiceUnavailable,
            )
        return self._get_target_class()(table, **kwargs)

    @staticmethod
    def _make_mutation(count=1, size=1):
        mutation = mock.Mock()
        mutation.size.return_value = size
        mutation.mutations = [mock.Mock()] * count
        return mutation

    def test_ctor_defaults(self):
        with mock.patch.object(
            self._get_target_class(),
            "_timer_routine",
            return_value=concurrent.futures.Future(),
        ) as flush_timer_mock:
            table = mock.Mock()
            table.default_mutate_rows_operation_timeout = 10
            table.default_mutate_rows_attempt_timeout = 8
            table.default_mutate_rows_retryable_errors = [Exception]
            with self._make_one(table) as instance:
                assert instance._table == table
                assert instance.closed is False
                assert instance._flush_jobs == set()
                assert len(instance._staged_entries) == 0
                assert len(instance._oldest_exceptions) == 0
                assert len(instance._newest_exceptions) == 0
                assert instance._exception_list_limit == 10
                assert instance._exceptions_since_last_raise == 0
                assert instance._flow_control._max_mutation_count == 100000
                assert instance._flow_control._max_mutation_bytes == 104857600
                assert instance._flow_control._in_flight_mutation_count == 0
                assert instance._flow_control._in_flight_mutation_bytes == 0
                assert instance._entries_processed_since_last_raise == 0
                assert (
                    instance._operation_timeout
                    == table.default_mutate_rows_operation_timeout
                )
                assert (
                    instance._attempt_timeout
                    == table.default_mutate_rows_attempt_timeout
                )
                assert (
                    instance._retryable_errors
                    == table.default_mutate_rows_retryable_errors
                )
                time.sleep(0)
                assert flush_timer_mock.call_count == 1
                assert flush_timer_mock.call_args[0][0] == 5
                assert isinstance(instance._flush_timer, concurrent.futures.Future)

    def test_ctor_explicit(self):
        """Test with explicit parameters"""
        with mock.patch.object(
            self._get_target_class(),
            "_timer_routine",
            return_value=concurrent.futures.Future(),
        ) as flush_timer_mock:
            table = mock.Mock()
            flush_interval = 20
            flush_limit_count = 17
            flush_limit_bytes = 19
            flow_control_max_mutation_count = 1001
            flow_control_max_bytes = 12
            operation_timeout = 11
            attempt_timeout = 2
            retryable_errors = [Exception]
            with self._make_one(
                table,
                flush_interval=flush_interval,
                flush_limit_mutation_count=flush_limit_count,
                flush_limit_bytes=flush_limit_bytes,
                flow_control_max_mutation_count=flow_control_max_mutation_count,
                flow_control_max_bytes=flow_control_max_bytes,
                batch_operation_timeout=operation_timeout,
                batch_attempt_timeout=attempt_timeout,
                batch_retryable_errors=retryable_errors,
            ) as instance:
                assert instance._table == table
                assert instance.closed is False
                assert instance._flush_jobs == set()
                assert len(instance._staged_entries) == 0
                assert len(instance._oldest_exceptions) == 0
                assert len(instance._newest_exceptions) == 0
                assert instance._exception_list_limit == 10
                assert instance._exceptions_since_last_raise == 0
                assert (
                    instance._flow_control._max_mutation_count
                    == flow_control_max_mutation_count
                )
                assert (
                    instance._flow_control._max_mutation_bytes == flow_control_max_bytes
                )
                assert instance._flow_control._in_flight_mutation_count == 0
                assert instance._flow_control._in_flight_mutation_bytes == 0
                assert instance._entries_processed_since_last_raise == 0
                assert instance._operation_timeout == operation_timeout
                assert instance._attempt_timeout == attempt_timeout
                assert instance._retryable_errors == retryable_errors
                time.sleep(0)
                assert flush_timer_mock.call_count == 1
                assert flush_timer_mock.call_args[0][0] == flush_interval
                assert isinstance(instance._flush_timer, concurrent.futures.Future)

    def test_ctor_no_flush_limits(self):
        """Test with None for flush limits"""
        with mock.patch.object(
            self._get_target_class(),
            "_timer_routine",
            return_value=concurrent.futures.Future(),
        ) as flush_timer_mock:
            table = mock.Mock()
            table.default_mutate_rows_operation_timeout = 10
            table.default_mutate_rows_attempt_timeout = 8
            table.default_mutate_rows_retryable_errors = ()
            flush_interval = None
            flush_limit_count = None
            flush_limit_bytes = None
            with self._make_one(
                table,
                flush_interval=flush_interval,
                flush_limit_mutation_count=flush_limit_count,
                flush_limit_bytes=flush_limit_bytes,
            ) as instance:
                assert instance._table == table
                assert instance.closed is False
                assert instance._staged_entries == []
                assert len(instance._oldest_exceptions) == 0
                assert len(instance._newest_exceptions) == 0
                assert instance._exception_list_limit == 10
                assert instance._exceptions_since_last_raise == 0
                assert instance._flow_control._in_flight_mutation_count == 0
                assert instance._flow_control._in_flight_mutation_bytes == 0
                assert instance._entries_processed_since_last_raise == 0
                time.sleep(0)
                assert flush_timer_mock.call_count == 1
                assert flush_timer_mock.call_args[0][0] is None
                assert isinstance(instance._flush_timer, concurrent.futures.Future)

    def test_ctor_invalid_values(self):
        """Test that timeout values are positive, and fit within expected limits"""
        with pytest.raises(ValueError) as e:
            self._make_one(batch_operation_timeout=-1)
        assert "operation_timeout must be greater than 0" in str(e.value)
        with pytest.raises(ValueError) as e:
            self._make_one(batch_attempt_timeout=-1)
        assert "attempt_timeout must be greater than 0" in str(e.value)

    def test_default_argument_consistency(self):
        """
        We supply default arguments in MutationsBatcherAsync.__init__, and in
        table.mutations_batcher. Make sure any changes to defaults are applied to
        both places
        """
        import inspect

        get_batcher_signature = dict(
            inspect.signature(Table.mutations_batcher).parameters
        )
        get_batcher_signature.pop("self")
        batcher_init_signature = dict(
            inspect.signature(self._get_target_class()).parameters
        )
        batcher_init_signature.pop("table")
        assert len(get_batcher_signature.keys()) == len(batcher_init_signature.keys())
        assert len(get_batcher_signature) == 8
        assert set(get_batcher_signature.keys()) == set(batcher_init_signature.keys())
        for arg_name in get_batcher_signature.keys():
            assert (
                get_batcher_signature[arg_name].default
                == batcher_init_signature[arg_name].default
            )

    @pytest.mark.parametrize("input_val", [None, 0, -1])
    def test__start_flush_timer_w_empty_input(self, input_val):
        """Empty/invalid timer should return immediately"""
        with mock.patch.object(
            self._get_target_class(), "_schedule_flush"
        ) as flush_mock:
            with self._make_one() as instance:
                if self.is_async():
                    (sleep_obj, sleep_method) = (asyncio, "wait_for")
                else:
                    (sleep_obj, sleep_method) = (instance._closed, "wait")
                with mock.patch.object(sleep_obj, sleep_method) as sleep_mock:
                    result = instance._timer_routine(input_val)
                    assert sleep_mock.call_count == 0
                    assert flush_mock.call_count == 0
                    assert result is None

    @pytest.mark.filterwarnings("ignore::RuntimeWarning")
    def test__start_flush_timer_call_when_closed(self):
        """closed batcher's timer should return immediately"""
        with mock.patch.object(
            self._get_target_class(), "_schedule_flush"
        ) as flush_mock:
            with self._make_one() as instance:
                instance.close()
                flush_mock.reset_mock()
                if self.is_async():
                    (sleep_obj, sleep_method) = (asyncio, "wait_for")
                else:
                    (sleep_obj, sleep_method) = (instance._closed, "wait")
                with mock.patch.object(sleep_obj, sleep_method) as sleep_mock:
                    instance._timer_routine(10)
                    assert sleep_mock.call_count == 0
                    assert flush_mock.call_count == 0

    @pytest.mark.parametrize("num_staged", [0, 1, 10])
    @pytest.mark.filterwarnings("ignore::RuntimeWarning")
    def test__flush_timer(self, num_staged):
        """Timer should continue to call _schedule_flush in a loop"""
        with mock.patch.object(
            self._get_target_class(), "_schedule_flush"
        ) as flush_mock:
            expected_sleep = 12
            with self._make_one(flush_interval=expected_sleep) as instance:
                loop_num = 3
                instance._staged_entries = [mock.Mock()] * num_staged
                if self.is_async():
                    (sleep_obj, sleep_method) = (asyncio, "wait_for")
                else:
                    (sleep_obj, sleep_method) = (instance._closed, "wait")
                with mock.patch.object(sleep_obj, sleep_method) as sleep_mock:
                    sleep_mock.side_effect = [None] * loop_num + [TabError("expected")]
                    with pytest.raises(TabError):
                        self._get_target_class()._timer_routine(
                            instance, expected_sleep
                        )
                    instance._flush_timer = concurrent.futures.Future()
                    assert sleep_mock.call_count == loop_num + 1
                    sleep_kwargs = sleep_mock.call_args[1]
                    assert sleep_kwargs["timeout"] == expected_sleep
                    assert flush_mock.call_count == (0 if num_staged == 0 else loop_num)

    def test__flush_timer_close(self):
        """Timer should continue terminate after close"""
        with mock.patch.object(self._get_target_class(), "_schedule_flush"):
            with self._make_one() as instance:
                with mock.patch("asyncio.sleep"):
                    time.sleep(0.5)
                    assert instance._flush_timer.done() is False
                    instance.close()
                    time.sleep(0.1)
                    assert instance._flush_timer.done() is True

    def test_append_closed(self):
        """Should raise exception"""
        instance = self._make_one()
        instance.close()
        with pytest.raises(RuntimeError):
            instance.append(mock.Mock())

    def test_append_wrong_mutation(self):
        """
        Mutation objects should raise an exception.
        Only support RowMutationEntry
        """
        from google.cloud.bigtable.data.mutations import DeleteAllFromRow

        with self._make_one() as instance:
            expected_error = "invalid mutation type: DeleteAllFromRow. Only RowMutationEntry objects are supported by batcher"
            with pytest.raises(ValueError) as e:
                instance.append(DeleteAllFromRow())
            assert str(e.value) == expected_error

    def test_append_outside_flow_limits(self):
        """entries larger than mutation limits are still processed"""
        with self._make_one(
            flow_control_max_mutation_count=1, flow_control_max_bytes=1
        ) as instance:
            oversized_entry = self._make_mutation(count=0, size=2)
            instance.append(oversized_entry)
            assert instance._staged_entries == [oversized_entry]
            assert instance._staged_count == 0
            assert instance._staged_bytes == 2
            instance._staged_entries = []
        with self._make_one(
            flow_control_max_mutation_count=1, flow_control_max_bytes=1
        ) as instance:
            overcount_entry = self._make_mutation(count=2, size=0)
            instance.append(overcount_entry)
            assert instance._staged_entries == [overcount_entry]
            assert instance._staged_count == 2
            assert instance._staged_bytes == 0
            instance._staged_entries = []

    def test_append_flush_runs_after_limit_hit(self):
        """
        If the user appends a bunch of entries above the flush limits back-to-back,
        it should still flush in a single task
        """
        with mock.patch.object(
            self._get_target_class(), "_execute_mutate_rows"
        ) as op_mock:
            with self._make_one(flush_limit_bytes=100) as instance:

                def mock_call(*args, **kwargs):
                    return []

                op_mock.side_effect = mock_call
                instance.append(self._make_mutation(size=99))
                num_entries = 10
                for _ in range(num_entries):
                    instance.append(self._make_mutation(size=1))
                instance._wait_for_batch_results(*instance._flush_jobs)
                assert op_mock.call_count == 1
                sent_batch = op_mock.call_args[0][0]
                assert len(sent_batch) == 2
                assert len(instance._staged_entries) == num_entries - 1

    @pytest.mark.parametrize(
        "flush_count,flush_bytes,mutation_count,mutation_bytes,expect_flush",
        [
            (10, 10, 1, 1, False),
            (10, 10, 9, 9, False),
            (10, 10, 10, 1, True),
            (10, 10, 1, 10, True),
            (10, 10, 10, 10, True),
            (1, 1, 10, 10, True),
            (1, 1, 0, 0, False),
        ],
    )
    @pytest.mark.filterwarnings("ignore::RuntimeWarning")
    def test_append(
        self, flush_count, flush_bytes, mutation_count, mutation_bytes, expect_flush
    ):
        """test appending different mutations, and checking if it causes a flush"""
        with self._make_one(
            flush_limit_mutation_count=flush_count, flush_limit_bytes=flush_bytes
        ) as instance:
            assert instance._staged_count == 0
            assert instance._staged_bytes == 0
            assert instance._staged_entries == []
            mutation = self._make_mutation(count=mutation_count, size=mutation_bytes)
            with mock.patch.object(instance, "_schedule_flush") as flush_mock:
                instance.append(mutation)
            assert flush_mock.call_count == bool(expect_flush)
            assert instance._staged_count == mutation_count
            assert instance._staged_bytes == mutation_bytes
            assert instance._staged_entries == [mutation]
            instance._staged_entries = []

    def test_append_multiple_sequentially(self):
        """Append multiple mutations"""
        with self._make_one(
            flush_limit_mutation_count=8, flush_limit_bytes=8
        ) as instance:
            assert instance._staged_count == 0
            assert instance._staged_bytes == 0
            assert instance._staged_entries == []
            mutation = self._make_mutation(count=2, size=3)
            with mock.patch.object(instance, "_schedule_flush") as flush_mock:
                instance.append(mutation)
                assert flush_mock.call_count == 0
                assert instance._staged_count == 2
                assert instance._staged_bytes == 3
                assert len(instance._staged_entries) == 1
                instance.append(mutation)
                assert flush_mock.call_count == 0
                assert instance._staged_count == 4
                assert instance._staged_bytes == 6
                assert len(instance._staged_entries) == 2
                instance.append(mutation)
                assert flush_mock.call_count == 1
                assert instance._staged_count == 6
                assert instance._staged_bytes == 9
                assert len(instance._staged_entries) == 3
            instance._staged_entries = []

    def test_flush_flow_control_concurrent_requests(self):
        """requests should happen in parallel if flow control breaks up single flush into batches"""
        import time

        num_calls = 10
        fake_mutations = [self._make_mutation(count=1) for _ in range(num_calls)]
        with self._make_one(flow_control_max_mutation_count=1) as instance:
            with mock.patch.object(
                instance, "_execute_mutate_rows", mock.Mock()
            ) as op_mock:

                def mock_call(*args, **kwargs):
                    time.sleep(0.1)
                    return []

                op_mock.side_effect = mock_call
                start_time = time.monotonic()
                instance._staged_entries = fake_mutations
                instance._schedule_flush()
                time.sleep(0.01)
                for i in range(num_calls):
                    instance._flow_control.remove_from_flow(
                        [self._make_mutation(count=1)]
                    )
                    time.sleep(0.01)
                instance._wait_for_batch_results(*instance._flush_jobs)
                duration = time.monotonic() - start_time
                assert len(instance._oldest_exceptions) == 0
                assert len(instance._newest_exceptions) == 0
                assert duration < 0.5
                assert op_mock.call_count == num_calls

    def test_schedule_flush_no_mutations(self):
        """schedule flush should return None if no staged mutations"""
        with self._make_one() as instance:
            with mock.patch.object(instance, "_flush_internal") as flush_mock:
                for i in range(3):
                    assert instance._schedule_flush() is None
                    assert flush_mock.call_count == 0

    @pytest.mark.filterwarnings("ignore::RuntimeWarning")
    def test_schedule_flush_with_mutations(self):
        """if new mutations exist, should add a new flush task to _flush_jobs"""
        with self._make_one() as instance:
            with mock.patch.object(instance, "_flush_internal") as flush_mock:
                if not self.is_async():
                    flush_mock.side_effect = lambda x: time.sleep(0.1)
                for i in range(1, 4):
                    mutation = mock.Mock()
                    instance._staged_entries = [mutation]
                    instance._schedule_flush()
                    assert instance._staged_entries == []
                    time.sleep(0)
                    assert instance._staged_entries == []
                    assert instance._staged_count == 0
                    assert instance._staged_bytes == 0
                    assert flush_mock.call_count == 1
                    flush_mock.reset_mock()

    def test__flush_internal(self):
        """
        _flush_internal should:
          - await previous flush call
          - delegate batching to _flow_control
          - call _execute_mutate_rows on each batch
          - update self.exceptions and self._entries_processed_since_last_raise
        """
        num_entries = 10
        with self._make_one() as instance:
            with mock.patch.object(instance, "_execute_mutate_rows") as execute_mock:
                with mock.patch.object(
                    instance._flow_control, "add_to_flow"
                ) as flow_mock:

                    def gen(x):
                        yield x

                    flow_mock.side_effect = lambda x: gen(x)
                    mutations = [self._make_mutation(count=1, size=1)] * num_entries
                    instance._flush_internal(mutations)
                    assert instance._entries_processed_since_last_raise == num_entries
                    assert execute_mock.call_count == 1
                    assert flow_mock.call_count == 1
                    instance._oldest_exceptions.clear()
                    instance._newest_exceptions.clear()

    def test_flush_clears_job_list(self):
        """
        a job should be added to _flush_jobs when _schedule_flush is called,
        and removed when it completes
        """
        with self._make_one() as instance:
            with mock.patch.object(
                instance, "_flush_internal", mock.Mock()
            ) as flush_mock:
                if not self.is_async():
                    flush_mock.side_effect = lambda x: time.sleep(0.1)
                mutations = [self._make_mutation(count=1, size=1)]
                instance._staged_entries = mutations
                assert instance._flush_jobs == set()
                new_job = instance._schedule_flush()
                assert instance._flush_jobs == {new_job}
                if self.is_async():
                    new_job
                else:
                    new_job.result()
                assert instance._flush_jobs == set()

    @pytest.mark.parametrize(
        "num_starting,num_new_errors,expected_total_errors",
        [
            (0, 0, 0),
            (0, 1, 1),
            (0, 2, 2),
            (1, 0, 1),
            (1, 1, 2),
            (10, 2, 12),
            (10, 20, 20),
        ],
    )
    def test__flush_internal_with_errors(
        self, num_starting, num_new_errors, expected_total_errors
    ):
        """errors returned from _execute_mutate_rows should be added to internal exceptions"""
        from google.cloud.bigtable.data import exceptions

        num_entries = 10
        expected_errors = [
            exceptions.FailedMutationEntryError(mock.Mock(), mock.Mock(), ValueError())
        ] * num_new_errors
        with self._make_one() as instance:
            instance._oldest_exceptions = [mock.Mock()] * num_starting
            with mock.patch.object(instance, "_execute_mutate_rows") as execute_mock:
                execute_mock.return_value = expected_errors
                with mock.patch.object(
                    instance._flow_control, "add_to_flow"
                ) as flow_mock:

                    def gen(x):
                        yield x

                    flow_mock.side_effect = lambda x: gen(x)
                    mutations = [self._make_mutation(count=1, size=1)] * num_entries
                    instance._flush_internal(mutations)
                    assert instance._entries_processed_since_last_raise == num_entries
                    assert execute_mock.call_count == 1
                    assert flow_mock.call_count == 1
                    found_exceptions = instance._oldest_exceptions + list(
                        instance._newest_exceptions
                    )
                    assert len(found_exceptions) == expected_total_errors
                    for i in range(num_starting, expected_total_errors):
                        assert found_exceptions[i] == expected_errors[i - num_starting]
                        assert found_exceptions[i].index is None
            instance._oldest_exceptions.clear()
            instance._newest_exceptions.clear()

    def _mock_gapic_return(self, num=5):
        from google.cloud.bigtable_v2.types import MutateRowsResponse
        from google.rpc import status_pb2

        def gen(num):
            for i in range(num):
                entry = MutateRowsResponse.Entry(
                    index=i, status=status_pb2.Status(code=0)
                )
                yield MutateRowsResponse(entries=[entry])

        return gen(num)

    def test_timer_flush_end_to_end(self):
        """Flush should automatically trigger after flush_interval"""
        num_nutations = 10
        mutations = [self._make_mutation(count=2, size=2)] * num_nutations
        with self._make_one(flush_interval=0.05) as instance:
            instance._table.default_operation_timeout = 10
            instance._table.default_attempt_timeout = 9
            with mock.patch.object(
                instance._table.client._gapic_client, "mutate_rows"
            ) as gapic_mock:
                gapic_mock.side_effect = (
                    lambda *args, **kwargs: self._mock_gapic_return(num_nutations)
                )
                for m in mutations:
                    instance.append(m)
                assert instance._entries_processed_since_last_raise == 0
                time.sleep(0.1)
                assert instance._entries_processed_since_last_raise == num_nutations

    def test__execute_mutate_rows(self):
        if self.is_async():
            mutate_path = "_async.mutations_batcher._MutateRowsOperationAsync"
        else:
            mutate_path = "_sync._mutate_rows._MutateRowsOperation"
        with mock.patch(f"google.cloud.bigtable.data.{mutate_path}") as mutate_rows:
            mutate_rows.return_value = mock.Mock()
            start_operation = mutate_rows().start
            table = mock.Mock()
            table.table_name = "test-table"
            table.app_profile_id = "test-app-profile"
            table.default_mutate_rows_operation_timeout = 17
            table.default_mutate_rows_attempt_timeout = 13
            table.default_mutate_rows_retryable_errors = ()
            with self._make_one(table) as instance:
                batch = [self._make_mutation()]
                result = instance._execute_mutate_rows(batch)
                assert start_operation.call_count == 1
                (args, kwargs) = mutate_rows.call_args
                assert args[0] == table.client._gapic_client
                assert args[1] == table
                assert args[2] == batch
                kwargs["operation_timeout"] == 17
                kwargs["attempt_timeout"] == 13
                assert result == []

    def test__execute_mutate_rows_returns_errors(self):
        """Errors from operation should be retruned as list"""
        from google.cloud.bigtable.data.exceptions import (
            MutationsExceptionGroup,
            FailedMutationEntryError,
        )

        if self.is_async():
            mutate_path = "_async.mutations_batcher._MutateRowsOperationAsync"
        else:
            mutate_path = "_sync._mutate_rows._MutateRowsOperation"
        with mock.patch(
            f"google.cloud.bigtable.data.{mutate_path}.start"
        ) as mutate_rows:
            err1 = FailedMutationEntryError(0, mock.Mock(), RuntimeError("test error"))
            err2 = FailedMutationEntryError(1, mock.Mock(), RuntimeError("test error"))
            mutate_rows.side_effect = MutationsExceptionGroup([err1, err2], 10)
            table = mock.Mock()
            table.default_mutate_rows_operation_timeout = 17
            table.default_mutate_rows_attempt_timeout = 13
            table.default_mutate_rows_retryable_errors = ()
            with self._make_one(table) as instance:
                batch = [self._make_mutation()]
                result = instance._execute_mutate_rows(batch)
                assert len(result) == 2
                assert result[0] == err1
                assert result[1] == err2
                assert result[0].index is None
                assert result[1].index is None

    def test__raise_exceptions(self):
        """Raise exceptions and reset error state"""
        from google.cloud.bigtable.data import exceptions

        expected_total = 1201
        expected_exceptions = [RuntimeError("mock")] * 3
        with self._make_one() as instance:
            instance._oldest_exceptions = expected_exceptions
            instance._entries_processed_since_last_raise = expected_total
            try:
                instance._raise_exceptions()
            except exceptions.MutationsExceptionGroup as exc:
                assert list(exc.exceptions) == expected_exceptions
                assert str(expected_total) in str(exc)
            assert instance._entries_processed_since_last_raise == 0
            (instance._oldest_exceptions, instance._newest_exceptions) = ([], [])
            instance._raise_exceptions()

    def test___aenter__(self):
        """Should return self"""
        with self._make_one() as instance:
            assert instance.__enter__() == instance

    def test___aexit__(self):
        """aexit should call close"""
        with self._make_one() as instance:
            with mock.patch.object(instance, "close") as close_mock:
                instance.__exit__(None, None, None)
                assert close_mock.call_count == 1

    def test_close(self):
        """Should clean up all resources"""
        with self._make_one() as instance:
            with mock.patch.object(instance, "_schedule_flush") as flush_mock:
                with mock.patch.object(instance, "_raise_exceptions") as raise_mock:
                    instance.close()
                    assert instance.closed is True
                    assert instance._flush_timer.done() is True
                    assert instance._flush_jobs == set()
                    assert flush_mock.call_count == 1
                    assert raise_mock.call_count == 1

    def test_close_w_exceptions(self):
        """Raise exceptions on close"""
        from google.cloud.bigtable.data import exceptions

        expected_total = 10
        expected_exceptions = [RuntimeError("mock")]
        with self._make_one() as instance:
            instance._oldest_exceptions = expected_exceptions
            instance._entries_processed_since_last_raise = expected_total
            try:
                instance.close()
            except exceptions.MutationsExceptionGroup as exc:
                assert list(exc.exceptions) == expected_exceptions
                assert str(expected_total) in str(exc)
            assert instance._entries_processed_since_last_raise == 0
            (instance._oldest_exceptions, instance._newest_exceptions) = ([], [])

    def test__on_exit(self, recwarn):
        """Should raise warnings if unflushed mutations exist"""
        with self._make_one() as instance:
            instance._on_exit()
            assert len(recwarn) == 0
            num_left = 4
            instance._staged_entries = [mock.Mock()] * num_left
            with pytest.warns(UserWarning) as w:
                instance._on_exit()
                assert len(w) == 1
                assert "unflushed mutations" in str(w[0].message).lower()
                assert str(num_left) in str(w[0].message)
            instance._closed.set()
            instance._on_exit()
            assert len(recwarn) == 0
            instance._staged_entries = []

    def test_atexit_registration(self):
        """Should run _on_exit on program termination"""
        import atexit

        with mock.patch.object(atexit, "register") as register_mock:
            assert register_mock.call_count == 0
            with self._make_one():
                assert register_mock.call_count == 1

    def test_timeout_args_passed(self):
        """
        batch_operation_timeout and batch_attempt_timeout should be used
        in api calls
        """
        if self.is_async():
            mutate_path = "_async.mutations_batcher._MutateRowsOperationAsync"
        else:
            mutate_path = "_sync._mutate_rows._MutateRowsOperation"
        with mock.patch(
            f"google.cloud.bigtable.data.{mutate_path}", return_value=mock.Mock()
        ) as mutate_rows:
            expected_operation_timeout = 17
            expected_attempt_timeout = 13
            with self._make_one(
                batch_operation_timeout=expected_operation_timeout,
                batch_attempt_timeout=expected_attempt_timeout,
            ) as instance:
                assert instance._operation_timeout == expected_operation_timeout
                assert instance._attempt_timeout == expected_attempt_timeout
                instance._execute_mutate_rows([self._make_mutation()])
                assert mutate_rows.call_count == 1
                kwargs = mutate_rows.call_args[1]
                assert kwargs["operation_timeout"] == expected_operation_timeout
                assert kwargs["attempt_timeout"] == expected_attempt_timeout

    @pytest.mark.parametrize(
        "limit,in_e,start_e,end_e",
        [
            (10, 0, (10, 0), (10, 0)),
            (1, 10, (0, 0), (1, 1)),
            (10, 1, (0, 0), (1, 0)),
            (10, 10, (0, 0), (10, 0)),
            (10, 11, (0, 0), (10, 1)),
            (3, 20, (0, 0), (3, 3)),
            (10, 20, (0, 0), (10, 10)),
            (10, 21, (0, 0), (10, 10)),
            (2, 1, (2, 0), (2, 1)),
            (2, 1, (1, 0), (2, 0)),
            (2, 2, (1, 0), (2, 1)),
            (3, 1, (3, 1), (3, 2)),
            (3, 3, (3, 1), (3, 3)),
            (1000, 5, (999, 0), (1000, 4)),
            (1000, 5, (0, 0), (5, 0)),
            (1000, 5, (1000, 0), (1000, 5)),
        ],
    )
    def test__add_exceptions(self, limit, in_e, start_e, end_e):
        """
        Test that the _add_exceptions function properly updates the
        _oldest_exceptions and _newest_exceptions lists
        Args:
          - limit: the _exception_list_limit representing the max size of either list
          - in_e: size of list of exceptions to send to _add_exceptions
          - start_e: a tuple of ints representing the initial sizes of _oldest_exceptions and _newest_exceptions
          - end_e: a tuple of ints representing the expected sizes of _oldest_exceptions and _newest_exceptions
        """
        from collections import deque

        input_list = [RuntimeError(f"mock {i}") for i in range(in_e)]
        mock_batcher = mock.Mock()
        mock_batcher._oldest_exceptions = [
            RuntimeError(f"starting mock {i}") for i in range(start_e[0])
        ]
        mock_batcher._newest_exceptions = deque(
            [RuntimeError(f"starting mock {i}") for i in range(start_e[1])],
            maxlen=limit,
        )
        mock_batcher._exception_list_limit = limit
        mock_batcher._exceptions_since_last_raise = 0
        self._get_target_class()._add_exceptions(mock_batcher, input_list)
        assert len(mock_batcher._oldest_exceptions) == end_e[0]
        assert len(mock_batcher._newest_exceptions) == end_e[1]
        assert mock_batcher._exceptions_since_last_raise == in_e
        oldest_list_diff = end_e[0] - start_e[0]
        newest_list_diff = min(max(in_e - oldest_list_diff, 0), limit)
        for i in range(oldest_list_diff):
            assert mock_batcher._oldest_exceptions[i + start_e[0]] == input_list[i]
        for i in range(1, newest_list_diff + 1):
            assert mock_batcher._newest_exceptions[-i] == input_list[-i]

    @pytest.mark.parametrize(
        "input_retryables,expected_retryables",
        [
            (
                TABLE_DEFAULT.READ_ROWS,
                [
                    core_exceptions.DeadlineExceeded,
                    core_exceptions.ServiceUnavailable,
                    core_exceptions.Aborted,
                ],
            ),
            (
                TABLE_DEFAULT.DEFAULT,
                [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
            ),
            (
                TABLE_DEFAULT.MUTATE_ROWS,
                [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
            ),
            ([], []),
            ([4], [core_exceptions.DeadlineExceeded]),
        ],
    )
    def test_customizable_retryable_errors(self, input_retryables, expected_retryables):
        """
        Test that retryable functions support user-configurable arguments, and that the configured retryables are passed
        down to the gapic layer.
        """
        retryn_fn = (
            "retry_target_async"
            if "Async" in self._get_target_class().__name__
            else "retry_target"
        )
        with mock.patch.object(
            google.api_core.retry, "if_exception_type"
        ) as predicate_builder_mock:
            with mock.patch.object(google.api_core.retry, retryn_fn) as retry_fn_mock:
                table = None
                with mock.patch("asyncio.create_task"):
                    table = Table(mock.Mock(), "instance", "table")
                with self._make_one(
                    table, batch_retryable_errors=input_retryables
                ) as instance:
                    assert instance._retryable_errors == expected_retryables
                    expected_predicate = lambda a: a in expected_retryables
                    predicate_builder_mock.return_value = expected_predicate
                    retry_fn_mock.side_effect = RuntimeError("stop early")
                    mutation = self._make_mutation(count=1, size=1)
                    instance._execute_mutate_rows([mutation])
                    predicate_builder_mock.assert_called_once_with(
                        *expected_retryables, _MutateRowsIncomplete
                    )
                    retry_call_args = retry_fn_mock.call_args_list[0].args
                    assert retry_call_args[1] is expected_predicate


class TestBigtableDataClient(ABC):
    @staticmethod
    def _get_target_class():
        from google.cloud.bigtable.data._sync.client import BigtableDataClient

        return BigtableDataClient

    @classmethod
    def _make_client(cls, *args, use_emulator=True, **kwargs):
        import os

        env_mask = {}
        if use_emulator:
            env_mask["BIGTABLE_EMULATOR_HOST"] = "localhost"
            import warnings

            warnings.filterwarnings("ignore", category=RuntimeWarning)
        else:
            kwargs["credentials"] = kwargs.get("credentials", AnonymousCredentials())
            kwargs["project"] = kwargs.get("project", "project-id")
        with mock.patch.dict(os.environ, env_mask):
            return cls._get_target_class()(*args, **kwargs)

    @property
    def is_async(self):
        return False

    def test_ctor(self):
        expected_project = "project-id"
        expected_pool_size = 11
        expected_credentials = AnonymousCredentials()
        client = self._make_client(
            project="project-id",
            pool_size=expected_pool_size,
            credentials=expected_credentials,
            use_emulator=False,
        )
        time.sleep(0)
        assert client.project == expected_project
        assert len(client.transport._grpc_channel._pool) == expected_pool_size
        assert not client._active_instances
        assert len(client._channel_refresh_tasks) == expected_pool_size
        assert client.transport._credentials == expected_credentials
        client.close()

    def test_ctor_super_inits(self):
        from google.cloud.client import ClientWithProject
        from google.api_core import client_options as client_options_lib
        from google.cloud.bigtable import __version__ as bigtable_version

        project = "project-id"
        pool_size = 11
        credentials = AnonymousCredentials()
        client_options = {"api_endpoint": "foo.bar:1234"}
        options_parsed = client_options_lib.from_dict(client_options)
        asyncio_portion = "-async" if self.is_async else ""
        transport_str = f"bt-{bigtable_version}-data{asyncio_portion}-{pool_size}"
        with mock.patch.object(BigtableClient, "__init__") as bigtable_client_init:
            bigtable_client_init.return_value = None
            with mock.patch.object(
                ClientWithProject, "__init__"
            ) as client_project_init:
                client_project_init.return_value = None
                try:
                    self._make_client(
                        project=project,
                        pool_size=pool_size,
                        credentials=credentials,
                        client_options=options_parsed,
                        use_emulator=False,
                    )
                except AttributeError:
                    pass
                assert bigtable_client_init.call_count == 1
                kwargs = bigtable_client_init.call_args[1]
                assert kwargs["transport"] == transport_str
                assert kwargs["credentials"] == credentials
                assert kwargs["client_options"] == options_parsed
                assert client_project_init.call_count == 1
                kwargs = client_project_init.call_args[1]
                assert kwargs["project"] == project
                assert kwargs["credentials"] == credentials
                assert kwargs["client_options"] == options_parsed

    def test_ctor_dict_options(self):
        from google.api_core.client_options import ClientOptions

        client_options = {"api_endpoint": "foo.bar:1234"}
        with mock.patch.object(BigtableClient, "__init__") as bigtable_client_init:
            try:
                self._make_client(client_options=client_options)
            except TypeError:
                pass
            bigtable_client_init.assert_called_once()
            kwargs = bigtable_client_init.call_args[1]
            called_options = kwargs["client_options"]
            assert called_options.api_endpoint == "foo.bar:1234"
            assert isinstance(called_options, ClientOptions)
        with mock.patch.object(
            self._get_target_class(), "_start_background_channel_refresh"
        ) as start_background_refresh:
            client = self._make_client(
                client_options=client_options, use_emulator=False
            )
            start_background_refresh.assert_called_once()
            client.close()

    def test_veneer_grpc_headers(self):
        client_component = "data-async" if self.is_async else "data"
        VENEER_HEADER_REGEX = re.compile(
            "gapic\\/[0-9]+\\.[\\w.-]+ gax\\/[0-9]+\\.[\\w.-]+ gccl\\/[0-9]+\\.[\\w.-]+-"
            + client_component
            + " gl-python\\/[0-9]+\\.[\\w.-]+ grpc\\/[0-9]+\\.[\\w.-]+"
        )
        if self.is_async:
            patch = mock.patch("google.api_core.gapic_v1.method_async.wrap_method")
        else:
            patch = mock.patch("google.api_core.gapic_v1.method.wrap_method")
        with patch as gapic_mock:
            client = self._make_client(project="project-id")
            wrapped_call_list = gapic_mock.call_args_list
            assert len(wrapped_call_list) > 0
            for call in wrapped_call_list:
                client_info = call.kwargs["client_info"]
                assert client_info is not None, f"{call} has no client_info"
                wrapped_user_agent_sorted = " ".join(
                    sorted(client_info.to_user_agent().split(" "))
                )
                assert VENEER_HEADER_REGEX.match(
                    wrapped_user_agent_sorted
                ), f"'{wrapped_user_agent_sorted}' does not match {VENEER_HEADER_REGEX}"
            client.close()

    def test_channel_pool_creation(self):
        pool_size = 14
        with mock.patch.object(
            grpc_helpers, "create_channel", mock.Mock()
        ) as create_channel:
            client = self._make_client(project="project-id", pool_size=pool_size)
            assert create_channel.call_count == pool_size
            client.close()
        client = self._make_client(project="project-id", pool_size=pool_size)
        pool_list = list(client.transport._grpc_channel._pool)
        pool_set = set(client.transport._grpc_channel._pool)
        assert len(pool_list) == len(pool_set)
        client.close()

    def test_channel_pool_rotation(self):
        pool_size = 7
        with mock.patch.object(PooledChannel, "next_channel") as next_channel:
            client = self._make_client(project="project-id", pool_size=pool_size)
            assert len(client.transport._grpc_channel._pool) == pool_size
            next_channel.reset_mock()
            with mock.patch.object(
                type(client.transport._grpc_channel._pool[0]), "unary_unary"
            ) as unary_unary:
                channel_next = None
                for i in range(pool_size):
                    channel_last = channel_next
                    channel_next = client.transport.grpc_channel._pool[i]
                    assert channel_last != channel_next
                    next_channel.return_value = channel_next
                    client.transport.ping_and_warm()
                    assert next_channel.call_count == i + 1
                    unary_unary.assert_called_once()
                    unary_unary.reset_mock()
        client.close()

    def test_channel_pool_replace(self):
        import time

        sleep_module = asyncio if self.is_async else time
        with mock.patch.object(sleep_module, "sleep"):
            pool_size = 7
            client = self._make_client(project="project-id", pool_size=pool_size)
            for replace_idx in range(pool_size):
                start_pool = [
                    channel for channel in client.transport._grpc_channel._pool
                ]
                grace_period = 9
                with mock.patch.object(
                    type(client.transport._grpc_channel._pool[-1]), "close"
                ) as close:
                    new_channel = client.transport.create_channel()
                    client.transport.replace_channel(
                        replace_idx, grace=grace_period, new_channel=new_channel
                    )
                    close.assert_called_once()
                    if self.is_async:
                        close.assert_called_once_with(grace=grace_period)
                        close.assert_called_once()
                assert client.transport._grpc_channel._pool[replace_idx] == new_channel
                for i in range(pool_size):
                    if i != replace_idx:
                        assert client.transport._grpc_channel._pool[i] == start_pool[i]
                    else:
                        assert client.transport._grpc_channel._pool[i] != start_pool[i]
            client.close()

    def test__start_background_channel_refresh_tasks_exist(self):
        client = self._make_client(project="project-id", use_emulator=False)
        assert len(client._channel_refresh_tasks) > 0
        with mock.patch.object(asyncio, "create_task") as create_task:
            client._start_background_channel_refresh()
            create_task.assert_not_called()
        client.close()

    @pytest.mark.parametrize("pool_size", [1, 3, 7])
    def test__start_background_channel_refresh(self, pool_size):
        import concurrent.futures

        with mock.patch.object(
            self._get_target_class(), "_ping_and_warm_instances", mock.Mock()
        ) as ping_and_warm:
            client = self._make_client(
                project="project-id", pool_size=pool_size, use_emulator=False
            )
            client._start_background_channel_refresh()
            assert len(client._channel_refresh_tasks) == pool_size
            for task in client._channel_refresh_tasks:
                if self.is_async:
                    assert isinstance(task, asyncio.Task)
                else:
                    assert isinstance(task, concurrent.futures.Future)
            time.sleep(0.1)
            assert ping_and_warm.call_count == pool_size
            for channel in client.transport._grpc_channel._pool:
                ping_and_warm.assert_any_call(channel)
        client.close()

    def test__ping_and_warm_instances(self):
        """test ping and warm with mocked asyncio.gather"""
        client_mock = mock.Mock()
        client_mock._execute_ping_and_warms = (
            lambda *args: self._get_target_class()._execute_ping_and_warms(
                client_mock, *args
            )
        )
        gather_tuple = (
            (asyncio, "gather") if self.is_async else (client_mock._executor, "submit")
        )
        with mock.patch.object(*gather_tuple, mock.Mock()) as gather:
            if self.is_async:
                gather.side_effect = lambda *args, **kwargs: [None for _ in args]
            else:
                gather.side_effect = lambda fn, **kwargs: [fn(**kwargs)]
            channel = mock.Mock()
            client_mock._active_instances = []
            result = self._get_target_class()._ping_and_warm_instances(
                client_mock, channel
            )
            assert len(result) == 0
            if self.is_async:
                assert gather.call_args.kwargs == {"return_exceptions": True}
            client_mock._active_instances = [
                (mock.Mock(), mock.Mock(), mock.Mock())
            ] * 4
            gather.reset_mock()
            channel.reset_mock()
            result = self._get_target_class()._ping_and_warm_instances(
                client_mock, channel
            )
            assert len(result) == 4
            if self.is_async:
                gather.assert_called_once()
                gather.assert_called_once()
                assert len(gather.call_args.args) == 4
            else:
                assert gather.call_count == 4
            grpc_call_args = channel.unary_unary().call_args_list
            for idx, (_, kwargs) in enumerate(grpc_call_args):
                (
                    expected_instance,
                    expected_table,
                    expected_app_profile,
                ) = client_mock._active_instances[idx]
                request = kwargs["request"]
                assert request["name"] == expected_instance
                assert request["app_profile_id"] == expected_app_profile
                metadata = kwargs["metadata"]
                assert len(metadata) == 1
                assert metadata[0][0] == "x-goog-request-params"
                assert (
                    metadata[0][1]
                    == f"name={expected_instance}&app_profile_id={expected_app_profile}"
                )

    def test__ping_and_warm_single_instance(self):
        """should be able to call ping and warm with single instance"""
        client_mock = mock.Mock()
        client_mock._execute_ping_and_warms = (
            lambda *args: self._get_target_class()._execute_ping_and_warms(
                client_mock, *args
            )
        )
        gather_tuple = (
            (asyncio, "gather") if self.is_async else (client_mock._executor, "submit")
        )
        with mock.patch.object(*gather_tuple, mock.Mock()) as gather:
            gather.side_effect = lambda *args, **kwargs: [mock.Mock() for _ in args]
            if self.is_async:
                gather.side_effect = lambda *args, **kwargs: [None for _ in args]
            else:
                gather.side_effect = lambda fn, **kwargs: [fn(**kwargs)]
            channel = mock.Mock()
            client_mock._active_instances = [mock.Mock()] * 100
            test_key = ("test-instance", "test-table", "test-app-profile")
            result = self._get_target_class()._ping_and_warm_instances(
                client_mock, channel, test_key
            )
            assert len(result) == 1
            grpc_call_args = channel.unary_unary().call_args_list
            assert len(grpc_call_args) == 1
            kwargs = grpc_call_args[0][1]
            request = kwargs["request"]
            assert request["name"] == "test-instance"
            assert request["app_profile_id"] == "test-app-profile"
            metadata = kwargs["metadata"]
            assert len(metadata) == 1
            assert metadata[0][0] == "x-goog-request-params"
            assert (
                metadata[0][1] == "name=test-instance&app_profile_id=test-app-profile"
            )

    @pytest.mark.parametrize(
        "refresh_interval, wait_time, expected_sleep",
        [(0, 0, 0), (0, 1, 0), (10, 0, 10), (10, 5, 5), (10, 10, 0), (10, 15, 0)],
    )
    def test__manage_channel_first_sleep(
        self, refresh_interval, wait_time, expected_sleep
    ):
        import threading
        import time

        with mock.patch.object(time, "monotonic") as monotonic:
            monotonic.return_value = 0
            sleep_tuple = (
                (asyncio, "sleep") if self.is_async else (threading.Event, "wait")
            )
            with mock.patch.object(*sleep_tuple) as sleep:
                sleep.side_effect = asyncio.CancelledError
                try:
                    client = self._make_client(project="project-id")
                    client._channel_init_time = -wait_time
                    client._manage_channel(0, refresh_interval, refresh_interval)
                except asyncio.CancelledError:
                    pass
                sleep.assert_called_once()
                call_time = sleep.call_args[0][0]
                assert (
                    abs(call_time - expected_sleep) < 0.1
                ), f"refresh_interval: {refresh_interval}, wait_time: {wait_time}, expected_sleep: {expected_sleep}"
                client.close()

    def test__manage_channel_ping_and_warm(self):
        """_manage channel should call ping and warm internally"""
        import time
        import threading

        client_mock = mock.Mock()
        client_mock._is_closed.is_set.return_value = False
        client_mock._channel_init_time = time.monotonic()
        channel_list = [mock.Mock(), mock.Mock()]
        client_mock.transport.channels = channel_list
        new_channel = mock.Mock()
        client_mock.transport.grpc_channel._create_channel.return_value = new_channel
        sleep_tuple = (asyncio, "sleep") if self.is_async else (threading.Event, "wait")
        with mock.patch.object(*sleep_tuple):
            client_mock.transport.replace_channel.side_effect = asyncio.CancelledError
            ping_and_warm = client_mock._ping_and_warm_instances = mock.Mock()
            try:
                channel_idx = 1
                self._get_target_class()._manage_channel(client_mock, channel_idx, 10)
            except asyncio.CancelledError:
                pass
            assert ping_and_warm.call_count == 2
            assert client_mock.transport.replace_channel.call_count == 1
            old_channel = channel_list[channel_idx]
            assert old_channel != new_channel
            called_with = [call[0][0] for call in ping_and_warm.call_args_list]
            assert old_channel in called_with
            assert new_channel in called_with
            ping_and_warm.reset_mock()
            try:
                self._get_target_class()._manage_channel(client_mock, 0, 0, 0)
            except asyncio.CancelledError:
                pass
            ping_and_warm.assert_called_once_with(new_channel)

    @pytest.mark.parametrize(
        "refresh_interval, num_cycles, expected_sleep",
        [(None, 1, 60 * 35), (10, 10, 100), (10, 1, 10)],
    )
    def test__manage_channel_sleeps(self, refresh_interval, num_cycles, expected_sleep):
        import time
        import random
        import threading

        channel_idx = 1
        with mock.patch.object(random, "uniform") as uniform:
            uniform.side_effect = lambda min_, max_: min_
            with mock.patch.object(time, "time") as time_mock:
                time_mock.return_value = 0
                sleep_tuple = (
                    (asyncio, "sleep") if self.is_async else (threading.Event, "wait")
                )
                with mock.patch.object(*sleep_tuple) as sleep:
                    sleep.side_effect = [None for i in range(num_cycles - 1)] + [
                        asyncio.CancelledError
                    ]
                    client = self._make_client(project="project-id")
                    with mock.patch.object(client.transport, "replace_channel"):
                        try:
                            if refresh_interval is not None:
                                client._manage_channel(
                                    channel_idx, refresh_interval, refresh_interval
                                )
                            else:
                                client._manage_channel(channel_idx)
                        except asyncio.CancelledError:
                            pass
                    assert sleep.call_count == num_cycles
                    total_sleep = sum([call[0][0] for call in sleep.call_args_list])
                    assert (
                        abs(total_sleep - expected_sleep) < 0.1
                    ), f"refresh_interval={refresh_interval}, num_cycles={num_cycles}, expected_sleep={expected_sleep}"
        client.close()

    def test__manage_channel_random(self):
        import random
        import threading

        sleep_tuple = (asyncio, "sleep") if self.is_async else (threading.Event, "wait")
        with mock.patch.object(*sleep_tuple) as sleep:
            with mock.patch.object(random, "uniform") as uniform:
                uniform.return_value = 0
                try:
                    uniform.side_effect = asyncio.CancelledError
                    client = self._make_client(project="project-id", pool_size=1)
                except asyncio.CancelledError:
                    uniform.side_effect = None
                    uniform.reset_mock()
                    sleep.reset_mock()
                min_val = 200
                max_val = 205
                uniform.side_effect = lambda min_, max_: min_
                sleep.side_effect = [None, None, asyncio.CancelledError]
                try:
                    with mock.patch.object(client.transport, "replace_channel"):
                        client._manage_channel(0, min_val, max_val)
                except asyncio.CancelledError:
                    pass
                assert uniform.call_count == 3
                uniform_args = [call[0] for call in uniform.call_args_list]
                for found_min, found_max in uniform_args:
                    assert found_min == min_val
                    assert found_max == max_val

    @pytest.mark.parametrize("num_cycles", [0, 1, 10, 100])
    def test__manage_channel_refresh(self, num_cycles):
        import threading

        expected_grace = 9
        expected_refresh = 0.5
        channel_idx = 1
        grpc_lib = grpc.aio if self.is_async else grpc
        new_channel = grpc_lib.insecure_channel("localhost:8080")
        with mock.patch.object(
            PooledBigtableGrpcTransport, "replace_channel"
        ) as replace_channel:
            sleep_tuple = (
                (asyncio, "sleep") if self.is_async else (threading.Event, "wait")
            )
            with mock.patch.object(*sleep_tuple) as sleep:
                sleep.side_effect = [None for i in range(num_cycles)] + [
                    asyncio.CancelledError
                ]
                with mock.patch.object(
                    grpc_helpers, "create_channel"
                ) as create_channel:
                    create_channel.return_value = new_channel
                    with mock.patch.object(
                        self._get_target_class(), "_start_background_channel_refresh"
                    ):
                        client = self._make_client(
                            project="project-id", use_emulator=False
                        )
                    create_channel.reset_mock()
                    try:
                        client._manage_channel(
                            channel_idx,
                            refresh_interval_min=expected_refresh,
                            refresh_interval_max=expected_refresh,
                            grace_period=expected_grace,
                        )
                    except asyncio.CancelledError:
                        pass
                    assert sleep.call_count == num_cycles + 1
                    assert create_channel.call_count == num_cycles
                    assert replace_channel.call_count == num_cycles
                    for call in replace_channel.call_args_list:
                        (args, kwargs) = call
                        assert args[0] == channel_idx
                        assert kwargs["grace"] == expected_grace
                        assert kwargs["new_channel"] == new_channel
                client.close()

    def test__register_instance(self):
        """test instance registration"""
        client_mock = mock.Mock()
        client_mock._gapic_client.instance_path.side_effect = lambda a, b: f"prefix/{b}"
        active_instances = set()
        instance_owners = {}
        client_mock._active_instances = active_instances
        client_mock._instance_owners = instance_owners
        client_mock._channel_refresh_tasks = []
        client_mock._start_background_channel_refresh.side_effect = (
            lambda: client_mock._channel_refresh_tasks.append(mock.Mock)
        )
        mock_channels = [mock.Mock() for i in range(5)]
        client_mock.transport.channels = mock_channels
        client_mock._ping_and_warm_instances = mock.Mock()
        table_mock = mock.Mock()
        self._get_target_class()._register_instance(
            client_mock, "instance-1", table_mock
        )
        assert client_mock._start_background_channel_refresh.call_count == 1
        expected_key = (
            "prefix/instance-1",
            table_mock.table_name,
            table_mock.app_profile_id,
        )
        assert len(active_instances) == 1
        assert expected_key == tuple(list(active_instances)[0])
        assert len(instance_owners) == 1
        assert expected_key == tuple(list(instance_owners)[0])
        assert client_mock._channel_refresh_tasks
        table_mock2 = mock.Mock()
        self._get_target_class()._register_instance(
            client_mock, "instance-2", table_mock2
        )
        assert client_mock._start_background_channel_refresh.call_count == 1
        assert client_mock._ping_and_warm_instances.call_count == len(mock_channels)
        for channel in mock_channels:
            assert channel in [
                call[0][0]
                for call in client_mock._ping_and_warm_instances.call_args_list
            ]
        assert len(active_instances) == 2
        assert len(instance_owners) == 2
        expected_key2 = (
            "prefix/instance-2",
            table_mock2.table_name,
            table_mock2.app_profile_id,
        )
        assert any(
            [
                expected_key2 == tuple(list(active_instances)[i])
                for i in range(len(active_instances))
            ]
        )
        assert any(
            [
                expected_key2 == tuple(list(instance_owners)[i])
                for i in range(len(instance_owners))
            ]
        )

    @pytest.mark.parametrize(
        "insert_instances,expected_active,expected_owner_keys",
        [
            ([("i", "t", None)], [("i", "t", None)], [("i", "t", None)]),
            ([("i", "t", "p")], [("i", "t", "p")], [("i", "t", "p")]),
            ([("1", "t", "p"), ("1", "t", "p")], [("1", "t", "p")], [("1", "t", "p")]),
            (
                [("1", "t", "p"), ("2", "t", "p")],
                [("1", "t", "p"), ("2", "t", "p")],
                [("1", "t", "p"), ("2", "t", "p")],
            ),
        ],
    )
    def test__register_instance_state(
        self, insert_instances, expected_active, expected_owner_keys
    ):
        """test that active_instances and instance_owners are updated as expected"""
        client_mock = mock.Mock()
        client_mock._gapic_client.instance_path.side_effect = lambda a, b: b
        active_instances = set()
        instance_owners = {}
        client_mock._active_instances = active_instances
        client_mock._instance_owners = instance_owners
        client_mock._channel_refresh_tasks = []
        client_mock._start_background_channel_refresh.side_effect = (
            lambda: client_mock._channel_refresh_tasks.append(mock.Mock)
        )
        mock_channels = [mock.Mock() for i in range(5)]
        client_mock.transport.channels = mock_channels
        client_mock._ping_and_warm_instances = mock.Mock()
        table_mock = mock.Mock()
        for instance, table, profile in insert_instances:
            table_mock.table_name = table
            table_mock.app_profile_id = profile
            self._get_target_class()._register_instance(
                client_mock, instance, table_mock
            )
        assert len(active_instances) == len(expected_active)
        assert len(instance_owners) == len(expected_owner_keys)
        for expected in expected_active:
            assert any(
                [
                    expected == tuple(list(active_instances)[i])
                    for i in range(len(active_instances))
                ]
            )
        for expected in expected_owner_keys:
            assert any(
                [
                    expected == tuple(list(instance_owners)[i])
                    for i in range(len(instance_owners))
                ]
            )

    def test__remove_instance_registration(self):
        client = self._make_client(project="project-id")
        table = mock.Mock()
        client._register_instance("instance-1", table)
        client._register_instance("instance-2", table)
        assert len(client._active_instances) == 2
        assert len(client._instance_owners.keys()) == 2
        instance_1_path = client._gapic_client.instance_path(
            client.project, "instance-1"
        )
        instance_1_key = (instance_1_path, table.table_name, table.app_profile_id)
        instance_2_path = client._gapic_client.instance_path(
            client.project, "instance-2"
        )
        instance_2_key = (instance_2_path, table.table_name, table.app_profile_id)
        assert len(client._instance_owners[instance_1_key]) == 1
        assert list(client._instance_owners[instance_1_key])[0] == id(table)
        assert len(client._instance_owners[instance_2_key]) == 1
        assert list(client._instance_owners[instance_2_key])[0] == id(table)
        success = client._remove_instance_registration("instance-1", table)
        assert success
        assert len(client._active_instances) == 1
        assert len(client._instance_owners[instance_1_key]) == 0
        assert len(client._instance_owners[instance_2_key]) == 1
        assert client._active_instances == {instance_2_key}
        success = client._remove_instance_registration("fake-key", table)
        assert not success
        assert len(client._active_instances) == 1
        client.close()

    def test__multiple_table_registration(self):
        """
        registering with multiple tables with the same key should
        add multiple owners to instance_owners, but only keep one copy
        of shared key in active_instances
        """
        from google.cloud.bigtable.data._helpers import _WarmedInstanceKey

        with self._make_client(project="project-id") as client:
            with client.get_table("instance_1", "table_1") as table_1:
                instance_1_path = client._gapic_client.instance_path(
                    client.project, "instance_1"
                )
                instance_1_key = _WarmedInstanceKey(
                    instance_1_path, table_1.table_name, table_1.app_profile_id
                )
                assert len(client._instance_owners[instance_1_key]) == 1
                assert len(client._active_instances) == 1
                assert id(table_1) in client._instance_owners[instance_1_key]
                with client.get_table("instance_1", "table_1") as table_2:
                    assert len(client._instance_owners[instance_1_key]) == 2
                    assert len(client._active_instances) == 1
                    assert id(table_1) in client._instance_owners[instance_1_key]
                    assert id(table_2) in client._instance_owners[instance_1_key]
                    with client.get_table("instance_1", "table_3") as table_3:
                        instance_3_path = client._gapic_client.instance_path(
                            client.project, "instance_1"
                        )
                        instance_3_key = _WarmedInstanceKey(
                            instance_3_path, table_3.table_name, table_3.app_profile_id
                        )
                        assert len(client._instance_owners[instance_1_key]) == 2
                        assert len(client._instance_owners[instance_3_key]) == 1
                        assert len(client._active_instances) == 2
                        assert id(table_1) in client._instance_owners[instance_1_key]
                        assert id(table_2) in client._instance_owners[instance_1_key]
                        assert id(table_3) in client._instance_owners[instance_3_key]
                assert len(client._active_instances) == 1
                assert instance_1_key in client._active_instances
                assert id(table_2) not in client._instance_owners[instance_1_key]
            assert len(client._active_instances) == 0
            assert instance_1_key not in client._active_instances
            assert len(client._instance_owners[instance_1_key]) == 0

    def test__multiple_instance_registration(self):
        """
        registering with multiple instance keys should update the key
        in instance_owners and active_instances
        """
        from google.cloud.bigtable.data._helpers import _WarmedInstanceKey

        with self._make_client(project="project-id") as client:
            with client.get_table("instance_1", "table_1") as table_1:
                with client.get_table("instance_2", "table_2") as table_2:
                    instance_1_path = client._gapic_client.instance_path(
                        client.project, "instance_1"
                    )
                    instance_1_key = _WarmedInstanceKey(
                        instance_1_path, table_1.table_name, table_1.app_profile_id
                    )
                    instance_2_path = client._gapic_client.instance_path(
                        client.project, "instance_2"
                    )
                    instance_2_key = _WarmedInstanceKey(
                        instance_2_path, table_2.table_name, table_2.app_profile_id
                    )
                    assert len(client._instance_owners[instance_1_key]) == 1
                    assert len(client._instance_owners[instance_2_key]) == 1
                    assert len(client._active_instances) == 2
                    assert id(table_1) in client._instance_owners[instance_1_key]
                    assert id(table_2) in client._instance_owners[instance_2_key]
                assert len(client._active_instances) == 1
                assert instance_1_key in client._active_instances
                assert len(client._instance_owners[instance_2_key]) == 0
                assert len(client._instance_owners[instance_1_key]) == 1
                assert id(table_1) in client._instance_owners[instance_1_key]
            assert len(client._active_instances) == 0
            assert len(client._instance_owners[instance_1_key]) == 0
            assert len(client._instance_owners[instance_2_key]) == 0

    def test_get_table(self):
        from google.cloud.bigtable.data._helpers import _WarmedInstanceKey

        client = self._make_client(project="project-id")
        assert not client._active_instances
        expected_table_id = "table-id"
        expected_instance_id = "instance-id"
        expected_app_profile_id = "app-profile-id"
        table = client.get_table(
            expected_instance_id, expected_table_id, expected_app_profile_id
        )
        time.sleep(0)
        assert isinstance(table, TestTable._get_target_class())
        assert table.table_id == expected_table_id
        assert (
            table.table_name
            == f"projects/{client.project}/instances/{expected_instance_id}/tables/{expected_table_id}"
        )
        assert table.instance_id == expected_instance_id
        assert (
            table.instance_name
            == f"projects/{client.project}/instances/{expected_instance_id}"
        )
        assert table.app_profile_id == expected_app_profile_id
        assert table.client is client
        instance_key = _WarmedInstanceKey(
            table.instance_name, table.table_name, table.app_profile_id
        )
        assert instance_key in client._active_instances
        assert client._instance_owners[instance_key] == {id(table)}
        client.close()

    def test_get_table_arg_passthrough(self):
        """All arguments passed in get_table should be sent to constructor"""
        with self._make_client(project="project-id") as client:
            with mock.patch.object(
                TestTable._get_target_class(), "__init__"
            ) as mock_constructor:
                mock_constructor.return_value = None
                assert not client._active_instances
                expected_table_id = "table-id"
                expected_instance_id = "instance-id"
                expected_app_profile_id = "app-profile-id"
                expected_args = (1, "test", {"test": 2})
                expected_kwargs = {"hello": "world", "test": 2}
                client.get_table(
                    expected_instance_id,
                    expected_table_id,
                    expected_app_profile_id,
                    *expected_args,
                    **expected_kwargs,
                )
                mock_constructor.assert_called_once_with(
                    client,
                    expected_instance_id,
                    expected_table_id,
                    expected_app_profile_id,
                    *expected_args,
                    **expected_kwargs,
                )

    def test_get_table_context_manager(self):
        from google.cloud.bigtable.data._helpers import _WarmedInstanceKey

        expected_table_id = "table-id"
        expected_instance_id = "instance-id"
        expected_app_profile_id = "app-profile-id"
        expected_project_id = "project-id"
        with mock.patch.object(TestTable._get_target_class(), "close") as close_mock:
            with self._make_client(project=expected_project_id) as client:
                with client.get_table(
                    expected_instance_id, expected_table_id, expected_app_profile_id
                ) as table:
                    time.sleep(0)
                    assert isinstance(table, TestTable._get_target_class())
                    assert table.table_id == expected_table_id
                    assert (
                        table.table_name
                        == f"projects/{expected_project_id}/instances/{expected_instance_id}/tables/{expected_table_id}"
                    )
                    assert table.instance_id == expected_instance_id
                    assert (
                        table.instance_name
                        == f"projects/{expected_project_id}/instances/{expected_instance_id}"
                    )
                    assert table.app_profile_id == expected_app_profile_id
                    assert table.client is client
                    instance_key = _WarmedInstanceKey(
                        table.instance_name, table.table_name, table.app_profile_id
                    )
                    assert instance_key in client._active_instances
                    assert client._instance_owners[instance_key] == {id(table)}
            assert close_mock.call_count == 1

    def test_multiple_pool_sizes(self):
        pool_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]
        for pool_size in pool_sizes:
            client = self._make_client(
                project="project-id", pool_size=pool_size, use_emulator=False
            )
            assert len(client._channel_refresh_tasks) == pool_size
            client_duplicate = self._make_client(
                project="project-id", pool_size=pool_size, use_emulator=False
            )
            assert len(client_duplicate._channel_refresh_tasks) == pool_size
            assert str(pool_size) in str(client.transport)
            client.close()
            client_duplicate.close()

    def test_close(self):
        pool_size = 7
        client = self._make_client(
            project="project-id", pool_size=pool_size, use_emulator=False
        )
        assert len(client._channel_refresh_tasks) == pool_size
        tasks_list = list(client._channel_refresh_tasks)
        for task in client._channel_refresh_tasks:
            assert not task.done()
        with mock.patch.object(
            PooledBigtableGrpcTransport, "close", mock.Mock()
        ) as close_mock:
            client.close()
            close_mock.assert_called_once()
            close_mock.assert_called_once()
        for task in tasks_list:
            assert task.done()
        assert client._channel_refresh_tasks == []

    def test_context_manager(self):
        close_mock = mock.Mock()
        true_close = None
        with self._make_client(project="project-id") as client:
            true_close = client.close()
            client.close = close_mock
            for task in client._channel_refresh_tasks:
                assert not task.done()
            assert client.project == "project-id"
            assert client._active_instances == set()
            close_mock.assert_not_called()
        close_mock.assert_called_once()
        close_mock.assert_called_once()
        true_close


class TestTable(ABC):
    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    @staticmethod
    def _get_target_class():
        from google.cloud.bigtable.data._sync.client import Table

        return Table

    @property
    def is_async(self):
        return False

    def test_table_ctor(self):
        from google.cloud.bigtable.data._helpers import _WarmedInstanceKey

        expected_table_id = "table-id"
        expected_instance_id = "instance-id"
        expected_app_profile_id = "app-profile-id"
        expected_operation_timeout = 123
        expected_attempt_timeout = 12
        expected_read_rows_operation_timeout = 1.5
        expected_read_rows_attempt_timeout = 0.5
        expected_mutate_rows_operation_timeout = 2.5
        expected_mutate_rows_attempt_timeout = 0.75
        client = self._make_client()
        assert not client._active_instances
        table = self._get_target_class()(
            client,
            expected_instance_id,
            expected_table_id,
            expected_app_profile_id,
            default_operation_timeout=expected_operation_timeout,
            default_attempt_timeout=expected_attempt_timeout,
            default_read_rows_operation_timeout=expected_read_rows_operation_timeout,
            default_read_rows_attempt_timeout=expected_read_rows_attempt_timeout,
            default_mutate_rows_operation_timeout=expected_mutate_rows_operation_timeout,
            default_mutate_rows_attempt_timeout=expected_mutate_rows_attempt_timeout,
        )
        time.sleep(0)
        assert table.table_id == expected_table_id
        assert table.instance_id == expected_instance_id
        assert table.app_profile_id == expected_app_profile_id
        assert table.client is client
        instance_key = _WarmedInstanceKey(
            table.instance_name, table.table_name, table.app_profile_id
        )
        assert instance_key in client._active_instances
        assert client._instance_owners[instance_key] == {id(table)}
        assert table.default_operation_timeout == expected_operation_timeout
        assert table.default_attempt_timeout == expected_attempt_timeout
        assert (
            table.default_read_rows_operation_timeout
            == expected_read_rows_operation_timeout
        )
        assert (
            table.default_read_rows_attempt_timeout
            == expected_read_rows_attempt_timeout
        )
        assert (
            table.default_mutate_rows_operation_timeout
            == expected_mutate_rows_operation_timeout
        )
        assert (
            table.default_mutate_rows_attempt_timeout
            == expected_mutate_rows_attempt_timeout
        )
        table._register_instance_future
        assert table._register_instance_future.done()
        assert not table._register_instance_future.cancelled()
        assert table._register_instance_future.exception() is None
        client.close()

    def test_table_ctor_defaults(self):
        """should provide default timeout values and app_profile_id"""
        expected_table_id = "table-id"
        expected_instance_id = "instance-id"
        client = self._make_client()
        assert not client._active_instances
        table = Table(client, expected_instance_id, expected_table_id)
        time.sleep(0)
        assert table.table_id == expected_table_id
        assert table.instance_id == expected_instance_id
        assert table.app_profile_id is None
        assert table.client is client
        assert table.default_operation_timeout == 60
        assert table.default_read_rows_operation_timeout == 600
        assert table.default_mutate_rows_operation_timeout == 600
        assert table.default_attempt_timeout == 20
        assert table.default_read_rows_attempt_timeout == 20
        assert table.default_mutate_rows_attempt_timeout == 60
        client.close()

    def test_table_ctor_invalid_timeout_values(self):
        """bad timeout values should raise ValueError"""
        client = self._make_client()
        timeout_pairs = [
            ("default_operation_timeout", "default_attempt_timeout"),
            (
                "default_read_rows_operation_timeout",
                "default_read_rows_attempt_timeout",
            ),
            (
                "default_mutate_rows_operation_timeout",
                "default_mutate_rows_attempt_timeout",
            ),
        ]
        for operation_timeout, attempt_timeout in timeout_pairs:
            with pytest.raises(ValueError) as e:
                Table(client, "", "", **{attempt_timeout: -1})
            assert "attempt_timeout must be greater than 0" in str(e.value)
            with pytest.raises(ValueError) as e:
                Table(client, "", "", **{operation_timeout: -1})
            assert "operation_timeout must be greater than 0" in str(e.value)
        client.close()

    @pytest.mark.parametrize(
        "fn_name,fn_args,is_stream,extra_retryables",
        [
            ("read_rows_stream", (ReadRowsQuery(),), True, ()),
            ("read_rows", (ReadRowsQuery(),), True, ()),
            ("read_row", (b"row_key",), True, ()),
            ("read_rows_sharded", ([ReadRowsQuery()],), True, ()),
            ("row_exists", (b"row_key",), True, ()),
            ("sample_row_keys", (), False, ()),
            ("mutate_row", (b"row_key", [mock.Mock()]), False, ()),
            (
                "bulk_mutate_rows",
                ([mutations.RowMutationEntry(b"key", [mutations.DeleteAllFromRow()])],),
                False,
                (_MutateRowsIncomplete,),
            ),
        ],
    )
    @pytest.mark.parametrize(
        "input_retryables,expected_retryables",
        [
            (
                TABLE_DEFAULT.READ_ROWS,
                [
                    core_exceptions.DeadlineExceeded,
                    core_exceptions.ServiceUnavailable,
                    core_exceptions.Aborted,
                ],
            ),
            (
                TABLE_DEFAULT.DEFAULT,
                [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
            ),
            (
                TABLE_DEFAULT.MUTATE_ROWS,
                [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
            ),
            ([], []),
            ([4], [core_exceptions.DeadlineExceeded]),
        ],
    )
    def test_customizable_retryable_errors(
        self,
        input_retryables,
        expected_retryables,
        fn_name,
        fn_args,
        is_stream,
        extra_retryables,
    ):
        """
        Test that retryable functions support user-configurable arguments, and that the configured retryables are passed
        down to the gapic layer.
        """
        retry_fn = "retry_target"
        if is_stream:
            retry_fn += "_stream"
        if self.is_async:
            retry_fn += "_async"
        with mock.patch(f"google.api_core.retry.{retry_fn}") as retry_fn_mock:
            with self._make_client() as client:
                table = client.get_table("instance-id", "table-id")
                expected_predicate = lambda a: a in expected_retryables
                retry_fn_mock.side_effect = RuntimeError("stop early")
                with mock.patch(
                    "google.api_core.retry.if_exception_type"
                ) as predicate_builder_mock:
                    predicate_builder_mock.return_value = expected_predicate
                    with pytest.raises(Exception):
                        test_fn = table.__getattribute__(fn_name)
                        test_fn(*fn_args, retryable_errors=input_retryables)
                    predicate_builder_mock.assert_called_once_with(
                        *expected_retryables, *extra_retryables
                    )
                    retry_call_args = retry_fn_mock.call_args_list[0].args
                    assert retry_call_args[1] is expected_predicate

    @pytest.mark.parametrize(
        "fn_name,fn_args,gapic_fn",
        [
            ("read_rows_stream", (ReadRowsQuery(),), "read_rows"),
            ("read_rows", (ReadRowsQuery(),), "read_rows"),
            ("read_row", (b"row_key",), "read_rows"),
            ("read_rows_sharded", ([ReadRowsQuery()],), "read_rows"),
            ("row_exists", (b"row_key",), "read_rows"),
            ("sample_row_keys", (), "sample_row_keys"),
            ("mutate_row", (b"row_key", [mock.Mock()]), "mutate_row"),
            (
                "bulk_mutate_rows",
                ([mutations.RowMutationEntry(b"key", [mutations.DeleteAllFromRow()])],),
                "mutate_rows",
            ),
            ("check_and_mutate_row", (b"row_key", None), "check_and_mutate_row"),
            (
                "read_modify_write_row",
                (b"row_key", mock.Mock()),
                "read_modify_write_row",
            ),
        ],
    )
    @pytest.mark.parametrize("include_app_profile", [True, False])
    def test_call_metadata(self, include_app_profile, fn_name, fn_args, gapic_fn):
        """check that all requests attach proper metadata headers"""
        profile = "profile" if include_app_profile else None
        with mock.patch.object(
            BigtableClient, gapic_fn, mock.mock.Mock()
        ) as gapic_mock:
            gapic_mock.side_effect = RuntimeError("stop early")
            with self._make_client() as client:
                table = Table(client, "instance-id", "table-id", profile)
                try:
                    test_fn = table.__getattribute__(fn_name)
                    maybe_stream = test_fn(*fn_args)
                    [i for i in maybe_stream]
                except Exception:
                    pass
                kwargs = gapic_mock.call_args_list[0].kwargs
                metadata = kwargs["metadata"]
                goog_metadata = None
                for key, value in metadata:
                    if key == "x-goog-request-params":
                        goog_metadata = value
                assert goog_metadata is not None, "x-goog-request-params not found"
                assert "table_name=" + table.table_name in goog_metadata
                if include_app_profile:
                    assert "app_profile_id=profile" in goog_metadata
                else:
                    assert "app_profile_id=" not in goog_metadata


class TestReadRows(ABC):
    """
    Tests for table.read_rows and related methods.
    """

    @staticmethod
    def _get_operation_class():
        from google.cloud.bigtable.data._sync._read_rows import _ReadRowsOperation

        return _ReadRowsOperation

    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    def _make_table(self, *args, **kwargs):
        client_mock = mock.Mock()
        client_mock._register_instance.side_effect = lambda *args, **kwargs: time.sleep(
            0
        )
        client_mock._remove_instance_registration.side_effect = (
            lambda *args, **kwargs: time.sleep(0)
        )
        kwargs["instance_id"] = kwargs.get(
            "instance_id", args[0] if args else "instance"
        )
        kwargs["table_id"] = kwargs.get(
            "table_id", args[1] if len(args) > 1 else "table"
        )
        client_mock._gapic_client.table_path.return_value = kwargs["table_id"]
        client_mock._gapic_client.instance_path.return_value = kwargs["instance_id"]
        return Table(client_mock, *args, **kwargs)

    def _make_stats(self):
        from google.cloud.bigtable_v2.types import RequestStats
        from google.cloud.bigtable_v2.types import FullReadStatsView
        from google.cloud.bigtable_v2.types import ReadIterationStats

        return RequestStats(
            full_read_stats_view=FullReadStatsView(
                read_iteration_stats=ReadIterationStats(
                    rows_seen_count=1,
                    rows_returned_count=2,
                    cells_seen_count=3,
                    cells_returned_count=4,
                )
            )
        )

    @staticmethod
    def _make_chunk(*args, **kwargs):
        from google.cloud.bigtable_v2 import ReadRowsResponse

        kwargs["row_key"] = kwargs.get("row_key", b"row_key")
        kwargs["family_name"] = kwargs.get("family_name", "family_name")
        kwargs["qualifier"] = kwargs.get("qualifier", b"qualifier")
        kwargs["value"] = kwargs.get("value", b"value")
        kwargs["commit_row"] = kwargs.get("commit_row", True)
        return ReadRowsResponse.CellChunk(*args, **kwargs)

    @staticmethod
    def _make_gapic_stream(
        chunk_list: list[ReadRowsResponse.CellChunk | Exception], sleep_time=0
    ):
        from google.cloud.bigtable_v2 import ReadRowsResponse

        class mock_stream:
            def __init__(self, chunk_list, sleep_time):
                self.chunk_list = chunk_list
                self.idx = -1
                self.sleep_time = sleep_time

            def __iter__(self):
                return self

            def __next__(self):
                self.idx += 1
                if len(self.chunk_list) > self.idx:
                    if sleep_time:
                        time.sleep(self.sleep_time)
                    chunk = self.chunk_list[self.idx]
                    if isinstance(chunk, Exception):
                        raise chunk
                    else:
                        return ReadRowsResponse(chunks=[chunk])
                raise StopIteration

            def cancel(self):
                pass

        return mock_stream(chunk_list, sleep_time)

    def execute_fn(self, table, *args, **kwargs):
        return table.read_rows(*args, **kwargs)

    def test_read_rows(self):
        query = ReadRowsQuery()
        chunks = [
            self._make_chunk(row_key=b"test_1"),
            self._make_chunk(row_key=b"test_2"),
        ]
        with self._make_table() as table:
            read_rows = table.client._gapic_client.read_rows
            read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream(
                chunks
            )
            results = self.execute_fn(table, query, operation_timeout=3)
            assert len(results) == 2
            assert results[0].row_key == b"test_1"
            assert results[1].row_key == b"test_2"

    def test_read_rows_stream(self):
        query = ReadRowsQuery()
        chunks = [
            self._make_chunk(row_key=b"test_1"),
            self._make_chunk(row_key=b"test_2"),
        ]
        with self._make_table() as table:
            read_rows = table.client._gapic_client.read_rows
            read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream(
                chunks
            )
            gen = table.read_rows_stream(query, operation_timeout=3)
            results = [row for row in gen]
            assert len(results) == 2
            assert results[0].row_key == b"test_1"
            assert results[1].row_key == b"test_2"

    @pytest.mark.parametrize("include_app_profile", [True, False])
    def test_read_rows_query_matches_request(self, include_app_profile):
        from google.cloud.bigtable.data import RowRange
        from google.cloud.bigtable.data.row_filters import PassAllFilter

        app_profile_id = "app_profile_id" if include_app_profile else None
        with self._make_table(app_profile_id=app_profile_id) as table:
            read_rows = table.client._gapic_client.read_rows
            read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream([])
            row_keys = [b"test_1", "test_2"]
            row_ranges = RowRange("1start", "2end")
            filter_ = PassAllFilter(True)
            limit = 99
            query = ReadRowsQuery(
                row_keys=row_keys,
                row_ranges=row_ranges,
                row_filter=filter_,
                limit=limit,
            )
            results = table.read_rows(query, operation_timeout=3)
            assert len(results) == 0
            call_request = read_rows.call_args_list[0][0][0]
            query_pb = query._to_pb(table)
            assert call_request == query_pb

    @pytest.mark.parametrize("operation_timeout", [0.001, 0.023, 0.1])
    def test_read_rows_timeout(self, operation_timeout):
        with self._make_table() as table:
            read_rows = table.client._gapic_client.read_rows
            query = ReadRowsQuery()
            chunks = [self._make_chunk(row_key=b"test_1")]
            read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream(
                chunks, sleep_time=1
            )
            try:
                table.read_rows(query, operation_timeout=operation_timeout)
            except core_exceptions.DeadlineExceeded as e:
                assert (
                    e.message
                    == f"operation_timeout of {operation_timeout:0.1f}s exceeded"
                )

    @pytest.mark.parametrize(
        "per_request_t, operation_t, expected_num",
        [(0.05, 0.08, 2), (0.05, 0.54, 11), (0.05, 0.14, 3), (0.05, 0.24, 5)],
    )
    def test_read_rows_attempt_timeout(self, per_request_t, operation_t, expected_num):
        """
        Ensures that the attempt_timeout is respected and that the number of
        requests is as expected.

        operation_timeout does not cancel the request, so we expect the number of
        requests to be the ceiling of operation_timeout / attempt_timeout.
        """
        from google.cloud.bigtable.data.exceptions import RetryExceptionGroup

        expected_last_timeout = operation_t - (expected_num - 1) * per_request_t
        with mock.patch("random.uniform", side_effect=lambda a, b: 0):
            with self._make_table() as table:
                read_rows = table.client._gapic_client.read_rows
                read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream(
                    chunks, sleep_time=per_request_t
                )
                query = ReadRowsQuery()
                chunks = [core_exceptions.DeadlineExceeded("mock deadline")]
                try:
                    table.read_rows(
                        query,
                        operation_timeout=operation_t,
                        attempt_timeout=per_request_t,
                    )
                except core_exceptions.DeadlineExceeded as e:
                    retry_exc = e.__cause__
                    if expected_num == 0:
                        assert retry_exc is None
                    else:
                        assert type(retry_exc) is RetryExceptionGroup
                        assert f"{expected_num} failed attempts" in str(retry_exc)
                        assert len(retry_exc.exceptions) == expected_num
                        for sub_exc in retry_exc.exceptions:
                            assert sub_exc.message == "mock deadline"
                assert read_rows.call_count == expected_num
                for _, call_kwargs in read_rows.call_args_list[:-1]:
                    assert call_kwargs["timeout"] == per_request_t
                    assert call_kwargs["retry"] is None
                assert (
                    abs(
                        read_rows.call_args_list[-1][1]["timeout"]
                        - expected_last_timeout
                    )
                    < 0.05
                )

    @pytest.mark.parametrize(
        "exc_type",
        [
            core_exceptions.Aborted,
            core_exceptions.DeadlineExceeded,
            core_exceptions.ServiceUnavailable,
        ],
    )
    def test_read_rows_retryable_error(self, exc_type):
        with self._make_table() as table:
            read_rows = table.client._gapic_client.read_rows
            read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream(
                [expected_error]
            )
            query = ReadRowsQuery()
            expected_error = exc_type("mock error")
            try:
                table.read_rows(query, operation_timeout=0.1)
            except core_exceptions.DeadlineExceeded as e:
                retry_exc = e.__cause__
                root_cause = retry_exc.exceptions[0]
                assert type(root_cause) is exc_type
                assert root_cause == expected_error

    @pytest.mark.parametrize(
        "exc_type",
        [
            core_exceptions.Cancelled,
            core_exceptions.PreconditionFailed,
            core_exceptions.NotFound,
            core_exceptions.PermissionDenied,
            core_exceptions.Conflict,
            core_exceptions.InternalServerError,
            core_exceptions.TooManyRequests,
            core_exceptions.ResourceExhausted,
            InvalidChunk,
        ],
    )
    def test_read_rows_non_retryable_error(self, exc_type):
        with self._make_table() as table:
            read_rows = table.client._gapic_client.read_rows
            read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream(
                [expected_error]
            )
            query = ReadRowsQuery()
            expected_error = exc_type("mock error")
            try:
                table.read_rows(query, operation_timeout=0.1)
            except exc_type as e:
                assert e == expected_error

    def test_read_rows_revise_request(self):
        """Ensure that _revise_request is called between retries"""
        from google.cloud.bigtable.data.exceptions import InvalidChunk
        from google.cloud.bigtable_v2.types import RowSet

        return_val = RowSet()
        with mock.patch.object(
            self._get_operation_class(), "_revise_request_rowset"
        ) as revise_rowset:
            revise_rowset.return_value = return_val
            with self._make_table() as table:
                read_rows = table.client._gapic_client.read_rows
                read_rows.side_effect = lambda *args, **kwargs: self._make_gapic_stream(
                    chunks
                )
                row_keys = [b"test_1", b"test_2", b"test_3"]
                query = ReadRowsQuery(row_keys=row_keys)
                chunks = [
                    self._make_chunk(row_key=b"test_1"),
                    core_exceptions.Aborted("mock retryable error"),
                ]
                try:
                    table.read_rows(query)
                except InvalidChunk:
                    revise_rowset.assert_called()
                    first_call_kwargs = revise_rowset.call_args_list[0].kwargs
                    assert first_call_kwargs["row_set"] == query._to_pb(table).rows
                    assert first_call_kwargs["last_seen_row_key"] == b"test_1"
                    revised_call = read_rows.call_args_list[1].args[0]
                    assert revised_call.rows == return_val

    def test_read_rows_default_timeouts(self):
        """Ensure that the default timeouts are set on the read rows operation when not overridden"""
        operation_timeout = 8
        attempt_timeout = 4
        with mock.patch.object(self._get_operation_class(), "__init__") as mock_op:
            mock_op.side_effect = RuntimeError("mock error")
            with self._make_table(
                default_read_rows_operation_timeout=operation_timeout,
                default_read_rows_attempt_timeout=attempt_timeout,
            ) as table:
                try:
                    table.read_rows(ReadRowsQuery())
                except RuntimeError:
                    pass
                kwargs = mock_op.call_args_list[0].kwargs
                assert kwargs["operation_timeout"] == operation_timeout
                assert kwargs["attempt_timeout"] == attempt_timeout

    def test_read_rows_default_timeout_override(self):
        """When timeouts are passed, they overwrite default values"""
        operation_timeout = 8
        attempt_timeout = 4
        with mock.patch.object(self._get_operation_class(), "__init__") as mock_op:
            mock_op.side_effect = RuntimeError("mock error")
            with self._make_table(
                default_operation_timeout=99, default_attempt_timeout=97
            ) as table:
                try:
                    table.read_rows(
                        ReadRowsQuery(),
                        operation_timeout=operation_timeout,
                        attempt_timeout=attempt_timeout,
                    )
                except RuntimeError:
                    pass
                kwargs = mock_op.call_args_list[0].kwargs
                assert kwargs["operation_timeout"] == operation_timeout
                assert kwargs["attempt_timeout"] == attempt_timeout

    def test_read_row(self):
        """Test reading a single row"""
        with self._make_client() as client:
            table = client.get_table("instance", "table")
            row_key = b"test_1"
            with mock.patch.object(table, "read_rows") as read_rows:
                expected_result = object()
                read_rows.side_effect = lambda *args, **kwargs: [expected_result]
                expected_op_timeout = 8
                expected_req_timeout = 4
                row = table.read_row(
                    row_key,
                    operation_timeout=expected_op_timeout,
                    attempt_timeout=expected_req_timeout,
                )
                assert row == expected_result
                assert read_rows.call_count == 1
                (args, kwargs) = read_rows.call_args_list[0]
                assert kwargs["operation_timeout"] == expected_op_timeout
                assert kwargs["attempt_timeout"] == expected_req_timeout
                assert len(args) == 1
                assert isinstance(args[0], ReadRowsQuery)
                query = args[0]
                assert query.row_keys == [row_key]
                assert query.row_ranges == []
                assert query.limit == 1

    def test_read_row_w_filter(self):
        """Test reading a single row with an added filter"""
        with self._make_client() as client:
            table = client.get_table("instance", "table")
            row_key = b"test_1"
            with mock.patch.object(table, "read_rows") as read_rows:
                expected_result = object()
                read_rows.side_effect = lambda *args, **kwargs: [expected_result]
                expected_op_timeout = 8
                expected_req_timeout = 4
                mock_filter = mock.Mock()
                expected_filter = {"filter": "mock filter"}
                mock_filter._to_dict.return_value = expected_filter
                row = table.read_row(
                    row_key,
                    operation_timeout=expected_op_timeout,
                    attempt_timeout=expected_req_timeout,
                    row_filter=expected_filter,
                )
                assert row == expected_result
                assert read_rows.call_count == 1
                (args, kwargs) = read_rows.call_args_list[0]
                assert kwargs["operation_timeout"] == expected_op_timeout
                assert kwargs["attempt_timeout"] == expected_req_timeout
                assert len(args) == 1
                assert isinstance(args[0], ReadRowsQuery)
                query = args[0]
                assert query.row_keys == [row_key]
                assert query.row_ranges == []
                assert query.limit == 1
                assert query.filter == expected_filter

    def test_read_row_no_response(self):
        """should return None if row does not exist"""
        with self._make_client() as client:
            table = client.get_table("instance", "table")
            row_key = b"test_1"
            with mock.patch.object(table, "read_rows") as read_rows:
                read_rows.side_effect = lambda *args, **kwargs: []
                expected_op_timeout = 8
                expected_req_timeout = 4
                result = table.read_row(
                    row_key,
                    operation_timeout=expected_op_timeout,
                    attempt_timeout=expected_req_timeout,
                )
                assert result is None
                assert read_rows.call_count == 1
                (args, kwargs) = read_rows.call_args_list[0]
                assert kwargs["operation_timeout"] == expected_op_timeout
                assert kwargs["attempt_timeout"] == expected_req_timeout
                assert isinstance(args[0], ReadRowsQuery)
                query = args[0]
                assert query.row_keys == [row_key]
                assert query.row_ranges == []
                assert query.limit == 1

    @pytest.mark.parametrize(
        "return_value,expected_result",
        [([], False), ([object()], True), ([object(), object()], True)],
    )
    def test_row_exists(self, return_value, expected_result):
        """Test checking for row existence"""
        with self._make_client() as client:
            table = client.get_table("instance", "table")
            row_key = b"test_1"
            with mock.patch.object(table, "read_rows") as read_rows:
                read_rows.side_effect = lambda *args, **kwargs: return_value
                expected_op_timeout = 1
                expected_req_timeout = 2
                result = table.row_exists(
                    row_key,
                    operation_timeout=expected_op_timeout,
                    attempt_timeout=expected_req_timeout,
                )
                assert expected_result == result
                assert read_rows.call_count == 1
                (args, kwargs) = read_rows.call_args_list[0]
                assert kwargs["operation_timeout"] == expected_op_timeout
                assert kwargs["attempt_timeout"] == expected_req_timeout
                assert isinstance(args[0], ReadRowsQuery)
                expected_filter = {
                    "chain": {
                        "filters": [
                            {"cells_per_row_limit_filter": 1},
                            {"strip_value_transformer": True},
                        ]
                    }
                }
                query = args[0]
                assert query.row_keys == [row_key]
                assert query.row_ranges == []
                assert query.limit == 1
                assert query.filter._to_dict() == expected_filter


class TestReadRowsSharded(ABC):
    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    def test_read_rows_sharded_empty_query(self):
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with pytest.raises(ValueError) as exc:
                    table.read_rows_sharded([])
                assert "empty sharded_query" in str(exc.value)

    def test_read_rows_sharded_multiple_queries(self):
        """Test with multiple queries. Should return results from both"""
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    table.client._gapic_client, "read_rows"
                ) as read_rows:
                    read_rows.side_effect = (
                        lambda *args, **kwargs: TestReadRows._make_gapic_stream(
                            [
                                TestReadRows._make_chunk(row_key=k)
                                for k in args[0].rows.row_keys
                            ]
                        )
                    )
                    query_1 = ReadRowsQuery(b"test_1")
                    query_2 = ReadRowsQuery(b"test_2")
                    result = table.read_rows_sharded([query_1, query_2])
                    assert len(result) == 2
                    assert result[0].row_key == b"test_1"
                    assert result[1].row_key == b"test_2"

    @pytest.mark.parametrize("n_queries", [1, 2, 5, 11, 24])
    def test_read_rows_sharded_multiple_queries_calls(self, n_queries):
        """Each query should trigger a separate read_rows call"""
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(table, "read_rows") as read_rows:
                    query_list = [ReadRowsQuery() for _ in range(n_queries)]
                    table.read_rows_sharded(query_list)
                    assert read_rows.call_count == n_queries

    def test_read_rows_sharded_errors(self):
        """Errors should be exposed as ShardedReadRowsExceptionGroups"""
        from google.cloud.bigtable.data.exceptions import ShardedReadRowsExceptionGroup
        from google.cloud.bigtable.data.exceptions import FailedQueryShardError

        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(table, "read_rows") as read_rows:
                    read_rows.side_effect = RuntimeError("mock error")
                    query_1 = ReadRowsQuery(b"test_1")
                    query_2 = ReadRowsQuery(b"test_2")
                    with pytest.raises(ShardedReadRowsExceptionGroup) as exc:
                        table.read_rows_sharded([query_1, query_2])
                    exc_group = exc.value
                    assert isinstance(exc_group, ShardedReadRowsExceptionGroup)
                    assert len(exc.value.exceptions) == 2
                    assert isinstance(exc.value.exceptions[0], FailedQueryShardError)
                    assert isinstance(exc.value.exceptions[0].__cause__, RuntimeError)
                    assert exc.value.exceptions[0].index == 0
                    assert exc.value.exceptions[0].query == query_1
                    assert isinstance(exc.value.exceptions[1], FailedQueryShardError)
                    assert isinstance(exc.value.exceptions[1].__cause__, RuntimeError)
                    assert exc.value.exceptions[1].index == 1
                    assert exc.value.exceptions[1].query == query_2

    def test_read_rows_sharded_concurrent(self):
        """Ensure sharded requests are concurrent"""
        import time

        def mock_call(*args, **kwargs):
            time.sleep(0.1)
            return [mock.Mock()]

        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(table, "read_rows") as read_rows:
                    read_rows.side_effect = mock_call
                    queries = [ReadRowsQuery() for _ in range(10)]
                    start_time = time.monotonic()
                    result = table.read_rows_sharded(queries)
                    call_time = time.monotonic() - start_time
                    assert read_rows.call_count == 10
                    assert len(result) == 10
                    assert call_time < 0.2

    def test_read_rows_sharded_batching(self):
        """
        Large queries should be processed in batches to limit concurrency
        operation timeout should change between batches
        """
        from google.cloud.bigtable.data._helpers import _CONCURRENCY_LIMIT

        assert _CONCURRENCY_LIMIT == 10
        n_queries = 90
        expected_num_batches = n_queries // _CONCURRENCY_LIMIT
        query_list = [ReadRowsQuery() for _ in range(n_queries)]
        start_operation_timeout = 10
        start_attempt_timeout = 3
        client = self._make_client(use_emulator=True)
        table = client.get_table(
            "instance",
            "table",
            default_read_rows_operation_timeout=start_operation_timeout,
            default_read_rows_attempt_timeout=start_attempt_timeout,
        )

        def mock_time_generator(start_op, _):
            for i in range(0, 100000):
                yield (start_op - i)

        with mock.patch(
            f"google.cloud.bigtable.data._helpers._attempt_timeout_generator"
        ) as time_gen_mock:
            time_gen_mock.side_effect = mock_time_generator
            with mock.patch.object(table, "read_rows", mock.Mock()) as read_rows_mock:
                read_rows_mock.return_value = []
                table.read_rows_sharded(query_list)
                assert read_rows_mock.call_count == n_queries
                kwargs = [
                    read_rows_mock.call_args_list[idx][1] for idx in range(n_queries)
                ]
                for batch_idx in range(expected_num_batches):
                    batch_kwargs = kwargs[
                        batch_idx
                        * _CONCURRENCY_LIMIT : (batch_idx + 1)
                        * _CONCURRENCY_LIMIT
                    ]
                    for req_kwargs in batch_kwargs:
                        expected_operation_timeout = start_operation_timeout - batch_idx
                        assert (
                            req_kwargs["operation_timeout"]
                            == expected_operation_timeout
                        )
                        expected_attempt_timeout = min(
                            start_attempt_timeout, expected_operation_timeout
                        )
                        assert req_kwargs["attempt_timeout"] == expected_attempt_timeout


class TestSampleRowKeys(ABC):
    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    def _make_gapic_stream(self, sample_list: list[tuple[bytes, int]]):
        from google.cloud.bigtable_v2.types import SampleRowKeysResponse

        for value in sample_list:
            yield SampleRowKeysResponse(row_key=value[0], offset_bytes=value[1])

    def test_sample_row_keys(self):
        """Test that method returns the expected key samples"""
        samples = [(b"test_1", 0), (b"test_2", 100), (b"test_3", 200)]
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    table.client._gapic_client, "sample_row_keys", mock.Mock()
                ) as sample_row_keys:
                    sample_row_keys.return_value = self._make_gapic_stream(samples)
                    result = table.sample_row_keys()
                    assert len(result) == 3
                    assert all((isinstance(r, tuple) for r in result))
                    assert all((isinstance(r[0], bytes) for r in result))
                    assert all((isinstance(r[1], int) for r in result))
                    assert result[0] == samples[0]
                    assert result[1] == samples[1]
                    assert result[2] == samples[2]

    def test_sample_row_keys_bad_timeout(self):
        """should raise error if timeout is negative"""
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with pytest.raises(ValueError) as e:
                    table.sample_row_keys(operation_timeout=-1)
                    assert "operation_timeout must be greater than 0" in str(e.value)
                with pytest.raises(ValueError) as e:
                    table.sample_row_keys(attempt_timeout=-1)
                    assert "attempt_timeout must be greater than 0" in str(e.value)

    def test_sample_row_keys_default_timeout(self):
        """Should fallback to using table default operation_timeout"""
        expected_timeout = 99
        with self._make_client() as client:
            with client.get_table(
                "i",
                "t",
                default_operation_timeout=expected_timeout,
                default_attempt_timeout=expected_timeout,
            ) as table:
                with mock.patch.object(
                    table.client._gapic_client, "sample_row_keys", mock.Mock()
                ) as sample_row_keys:
                    sample_row_keys.return_value = self._make_gapic_stream([])
                    result = table.sample_row_keys()
                    (_, kwargs) = sample_row_keys.call_args
                    assert abs(kwargs["timeout"] - expected_timeout) < 0.1
                    assert result == []
                    assert kwargs["retry"] is None

    def test_sample_row_keys_gapic_params(self):
        """make sure arguments are propagated to gapic call as expected"""
        expected_timeout = 10
        expected_profile = "test1"
        instance = "instance_name"
        table_id = "my_table"
        with self._make_client() as client:
            with client.get_table(
                instance, table_id, app_profile_id=expected_profile
            ) as table:
                with mock.patch.object(
                    table.client._gapic_client, "sample_row_keys", mock.Mock()
                ) as sample_row_keys:
                    sample_row_keys.return_value = self._make_gapic_stream([])
                    table.sample_row_keys(attempt_timeout=expected_timeout)
                    (args, kwargs) = sample_row_keys.call_args
                    assert len(args) == 0
                    assert len(kwargs) == 5
                    assert kwargs["timeout"] == expected_timeout
                    assert kwargs["app_profile_id"] == expected_profile
                    assert kwargs["table_name"] == table.table_name
                    assert kwargs["metadata"] is not None
                    assert kwargs["retry"] is None

    @pytest.mark.parametrize(
        "retryable_exception",
        [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
    )
    def test_sample_row_keys_retryable_errors(self, retryable_exception):
        """retryable errors should be retried until timeout"""
        from google.api_core.exceptions import DeadlineExceeded
        from google.cloud.bigtable.data.exceptions import RetryExceptionGroup

        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    table.client._gapic_client, "sample_row_keys", mock.Mock()
                ) as sample_row_keys:
                    sample_row_keys.side_effect = retryable_exception("mock")
                    with pytest.raises(DeadlineExceeded) as e:
                        table.sample_row_keys(operation_timeout=0.05)
                    cause = e.value.__cause__
                    assert isinstance(cause, RetryExceptionGroup)
                    assert len(cause.exceptions) > 0
                    assert isinstance(cause.exceptions[0], retryable_exception)

    @pytest.mark.parametrize(
        "non_retryable_exception",
        [
            core_exceptions.OutOfRange,
            core_exceptions.NotFound,
            core_exceptions.FailedPrecondition,
            RuntimeError,
            ValueError,
            core_exceptions.Aborted,
        ],
    )
    def test_sample_row_keys_non_retryable_errors(self, non_retryable_exception):
        """non-retryable errors should cause a raise"""
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    table.client._gapic_client, "sample_row_keys", mock.Mock()
                ) as sample_row_keys:
                    sample_row_keys.side_effect = non_retryable_exception("mock")
                    with pytest.raises(non_retryable_exception):
                        table.sample_row_keys()


class TestMutateRow(ABC):
    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    @pytest.mark.parametrize(
        "mutation_arg",
        [
            mutations.SetCell("family", b"qualifier", b"value"),
            mutations.SetCell(
                "family", b"qualifier", b"value", timestamp_micros=1234567890
            ),
            mutations.DeleteRangeFromColumn("family", b"qualifier"),
            mutations.DeleteAllFromFamily("family"),
            mutations.DeleteAllFromRow(),
            [mutations.SetCell("family", b"qualifier", b"value")],
            [
                mutations.DeleteRangeFromColumn("family", b"qualifier"),
                mutations.DeleteAllFromRow(),
            ],
        ],
    )
    def test_mutate_row(self, mutation_arg):
        """Test mutations with no errors"""
        expected_attempt_timeout = 19
        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_row"
                ) as mock_gapic:
                    mock_gapic.return_value = None
                    table.mutate_row(
                        "row_key",
                        mutation_arg,
                        attempt_timeout=expected_attempt_timeout,
                    )
                    assert mock_gapic.call_count == 1
                    kwargs = mock_gapic.call_args_list[0].kwargs
                    assert (
                        kwargs["table_name"]
                        == "projects/project/instances/instance/tables/table"
                    )
                    assert kwargs["row_key"] == b"row_key"
                    formatted_mutations = (
                        [mutation._to_pb() for mutation in mutation_arg]
                        if isinstance(mutation_arg, list)
                        else [mutation_arg._to_pb()]
                    )
                    assert kwargs["mutations"] == formatted_mutations
                    assert kwargs["timeout"] == expected_attempt_timeout
                    assert kwargs["retry"] is None

    @pytest.mark.parametrize(
        "retryable_exception",
        [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
    )
    def test_mutate_row_retryable_errors(self, retryable_exception):
        from google.api_core.exceptions import DeadlineExceeded
        from google.cloud.bigtable.data.exceptions import RetryExceptionGroup

        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_row"
                ) as mock_gapic:
                    mock_gapic.side_effect = retryable_exception("mock")
                    with pytest.raises(DeadlineExceeded) as e:
                        mutation = mutations.DeleteAllFromRow()
                        assert mutation.is_idempotent() is True
                        table.mutate_row("row_key", mutation, operation_timeout=0.01)
                    cause = e.value.__cause__
                    assert isinstance(cause, RetryExceptionGroup)
                    assert isinstance(cause.exceptions[0], retryable_exception)

    @pytest.mark.parametrize(
        "retryable_exception",
        [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
    )
    def test_mutate_row_non_idempotent_retryable_errors(self, retryable_exception):
        """Non-idempotent mutations should not be retried"""
        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_row"
                ) as mock_gapic:
                    mock_gapic.side_effect = retryable_exception("mock")
                    with pytest.raises(retryable_exception):
                        mutation = mutations.SetCell(
                            "family", b"qualifier", b"value", -1
                        )
                        assert mutation.is_idempotent() is False
                        table.mutate_row("row_key", mutation, operation_timeout=0.2)

    @pytest.mark.parametrize(
        "non_retryable_exception",
        [
            core_exceptions.OutOfRange,
            core_exceptions.NotFound,
            core_exceptions.FailedPrecondition,
            RuntimeError,
            ValueError,
            core_exceptions.Aborted,
        ],
    )
    def test_mutate_row_non_retryable_errors(self, non_retryable_exception):
        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_row"
                ) as mock_gapic:
                    mock_gapic.side_effect = non_retryable_exception("mock")
                    with pytest.raises(non_retryable_exception):
                        mutation = mutations.SetCell(
                            "family",
                            b"qualifier",
                            b"value",
                            timestamp_micros=1234567890,
                        )
                        assert mutation.is_idempotent() is True
                        table.mutate_row("row_key", mutation, operation_timeout=0.2)

    @pytest.mark.parametrize("include_app_profile", [True, False])
    def test_mutate_row_metadata(self, include_app_profile):
        """request should attach metadata headers"""
        profile = "profile" if include_app_profile else None
        with self._make_client() as client:
            with client.get_table("i", "t", app_profile_id=profile) as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_row", mock.Mock()
                ) as read_rows:
                    table.mutate_row("rk", mock.Mock())
                kwargs = read_rows.call_args_list[0].kwargs
                metadata = kwargs["metadata"]
                goog_metadata = None
                for key, value in metadata:
                    if key == "x-goog-request-params":
                        goog_metadata = value
                assert goog_metadata is not None, "x-goog-request-params not found"
                assert "table_name=" + table.table_name in goog_metadata
                if include_app_profile:
                    assert "app_profile_id=profile" in goog_metadata
                else:
                    assert "app_profile_id=" not in goog_metadata

    @pytest.mark.parametrize("mutations", [[], None])
    def test_mutate_row_no_mutations(self, mutations):
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with pytest.raises(ValueError) as e:
                    table.mutate_row("key", mutations=mutations)
                    assert e.value.args[0] == "No mutations provided"


class TestBulkMutateRows(ABC):
    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    def _mock_response(self, response_list):
        from google.cloud.bigtable_v2.types import MutateRowsResponse
        from google.rpc import status_pb2

        statuses = []
        for response in response_list:
            if isinstance(response, core_exceptions.GoogleAPICallError):
                statuses.append(
                    status_pb2.Status(
                        message=str(response), code=response.grpc_status_code.value[0]
                    )
                )
            else:
                statuses.append(status_pb2.Status(code=0))
        entries = [
            MutateRowsResponse.Entry(index=i, status=statuses[i])
            for i in range(len(response_list))
        ]

        def generator():
            yield MutateRowsResponse(entries=entries)

        return generator()

    @pytest.mark.parametrize(
        "mutation_arg",
        [
            [mutations.SetCell("family", b"qualifier", b"value")],
            [
                mutations.SetCell(
                    "family", b"qualifier", b"value", timestamp_micros=1234567890
                )
            ],
            [mutations.DeleteRangeFromColumn("family", b"qualifier")],
            [mutations.DeleteAllFromFamily("family")],
            [mutations.DeleteAllFromRow()],
            [mutations.SetCell("family", b"qualifier", b"value")],
            [
                mutations.DeleteRangeFromColumn("family", b"qualifier"),
                mutations.DeleteAllFromRow(),
            ],
        ],
    )
    def test_bulk_mutate_rows(self, mutation_arg):
        """Test mutations with no errors"""
        expected_attempt_timeout = 19
        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.return_value = self._mock_response([None])
                    bulk_mutation = mutations.RowMutationEntry(b"row_key", mutation_arg)
                    table.bulk_mutate_rows(
                        [bulk_mutation], attempt_timeout=expected_attempt_timeout
                    )
                    assert mock_gapic.call_count == 1
                    kwargs = mock_gapic.call_args[1]
                    assert (
                        kwargs["table_name"]
                        == "projects/project/instances/instance/tables/table"
                    )
                    assert kwargs["entries"] == [bulk_mutation._to_pb()]
                    assert kwargs["timeout"] == expected_attempt_timeout
                    assert kwargs["retry"] is None

    def test_bulk_mutate_rows_multiple_entries(self):
        """Test mutations with no errors"""
        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.return_value = self._mock_response([None, None])
                    mutation_list = [mutations.DeleteAllFromRow()]
                    entry_1 = mutations.RowMutationEntry(b"row_key_1", mutation_list)
                    entry_2 = mutations.RowMutationEntry(b"row_key_2", mutation_list)
                    table.bulk_mutate_rows([entry_1, entry_2])
                    assert mock_gapic.call_count == 1
                    kwargs = mock_gapic.call_args[1]
                    assert (
                        kwargs["table_name"]
                        == "projects/project/instances/instance/tables/table"
                    )
                    assert kwargs["entries"][0] == entry_1._to_pb()
                    assert kwargs["entries"][1] == entry_2._to_pb()

    @pytest.mark.parametrize(
        "exception",
        [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
    )
    def test_bulk_mutate_rows_idempotent_mutation_error_retryable(self, exception):
        """Individual idempotent mutations should be retried if they fail with a retryable error"""
        from google.cloud.bigtable.data.exceptions import (
            RetryExceptionGroup,
            FailedMutationEntryError,
            MutationsExceptionGroup,
        )

        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.side_effect = lambda *a, **k: self._mock_response(
                        [exception("mock")]
                    )
                    with pytest.raises(MutationsExceptionGroup) as e:
                        mutation = mutations.DeleteAllFromRow()
                        entry = mutations.RowMutationEntry(b"row_key", [mutation])
                        assert mutation.is_idempotent() is True
                        table.bulk_mutate_rows([entry], operation_timeout=0.05)
                    assert len(e.value.exceptions) == 1
                    failed_exception = e.value.exceptions[0]
                    assert "non-idempotent" not in str(failed_exception)
                    assert isinstance(failed_exception, FailedMutationEntryError)
                    cause = failed_exception.__cause__
                    assert isinstance(cause, RetryExceptionGroup)
                    assert isinstance(cause.exceptions[0], exception)
                    assert isinstance(
                        cause.exceptions[-1], core_exceptions.DeadlineExceeded
                    )

    @pytest.mark.parametrize(
        "exception",
        [
            core_exceptions.OutOfRange,
            core_exceptions.NotFound,
            core_exceptions.FailedPrecondition,
            core_exceptions.Aborted,
        ],
    )
    def test_bulk_mutate_rows_idempotent_mutation_error_non_retryable(self, exception):
        """Individual idempotent mutations should not be retried if they fail with a non-retryable error"""
        from google.cloud.bigtable.data.exceptions import (
            FailedMutationEntryError,
            MutationsExceptionGroup,
        )

        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.side_effect = lambda *a, **k: self._mock_response(
                        [exception("mock")]
                    )
                    with pytest.raises(MutationsExceptionGroup) as e:
                        mutation = mutations.DeleteAllFromRow()
                        entry = mutations.RowMutationEntry(b"row_key", [mutation])
                        assert mutation.is_idempotent() is True
                        table.bulk_mutate_rows([entry], operation_timeout=0.05)
                    assert len(e.value.exceptions) == 1
                    failed_exception = e.value.exceptions[0]
                    assert "non-idempotent" not in str(failed_exception)
                    assert isinstance(failed_exception, FailedMutationEntryError)
                    cause = failed_exception.__cause__
                    assert isinstance(cause, exception)

    @pytest.mark.parametrize(
        "retryable_exception",
        [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
    )
    def test_bulk_mutate_idempotent_retryable_request_errors(self, retryable_exception):
        """Individual idempotent mutations should be retried if the request fails with a retryable error"""
        from google.cloud.bigtable.data.exceptions import (
            RetryExceptionGroup,
            FailedMutationEntryError,
            MutationsExceptionGroup,
        )

        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.side_effect = retryable_exception("mock")
                    with pytest.raises(MutationsExceptionGroup) as e:
                        mutation = mutations.SetCell(
                            "family", b"qualifier", b"value", timestamp_micros=123
                        )
                        entry = mutations.RowMutationEntry(b"row_key", [mutation])
                        assert mutation.is_idempotent() is True
                        table.bulk_mutate_rows([entry], operation_timeout=0.05)
                    assert len(e.value.exceptions) == 1
                    failed_exception = e.value.exceptions[0]
                    assert isinstance(failed_exception, FailedMutationEntryError)
                    assert "non-idempotent" not in str(failed_exception)
                    cause = failed_exception.__cause__
                    assert isinstance(cause, RetryExceptionGroup)
                    assert isinstance(cause.exceptions[0], retryable_exception)

    @pytest.mark.parametrize(
        "retryable_exception",
        [core_exceptions.DeadlineExceeded, core_exceptions.ServiceUnavailable],
    )
    def test_bulk_mutate_rows_non_idempotent_retryable_errors(
        self, retryable_exception
    ):
        """Non-Idempotent mutations should never be retried"""
        from google.cloud.bigtable.data.exceptions import (
            FailedMutationEntryError,
            MutationsExceptionGroup,
        )

        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.side_effect = lambda *a, **k: self._mock_response(
                        [retryable_exception("mock")]
                    )
                    with pytest.raises(MutationsExceptionGroup) as e:
                        mutation = mutations.SetCell(
                            "family", b"qualifier", b"value", -1
                        )
                        entry = mutations.RowMutationEntry(b"row_key", [mutation])
                        assert mutation.is_idempotent() is False
                        table.bulk_mutate_rows([entry], operation_timeout=0.2)
                    assert len(e.value.exceptions) == 1
                    failed_exception = e.value.exceptions[0]
                    assert isinstance(failed_exception, FailedMutationEntryError)
                    assert "non-idempotent" in str(failed_exception)
                    cause = failed_exception.__cause__
                    assert isinstance(cause, retryable_exception)

    @pytest.mark.parametrize(
        "non_retryable_exception",
        [
            core_exceptions.OutOfRange,
            core_exceptions.NotFound,
            core_exceptions.FailedPrecondition,
            RuntimeError,
            ValueError,
        ],
    )
    def test_bulk_mutate_rows_non_retryable_errors(self, non_retryable_exception):
        """If the request fails with a non-retryable error, mutations should not be retried"""
        from google.cloud.bigtable.data.exceptions import (
            FailedMutationEntryError,
            MutationsExceptionGroup,
        )

        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.side_effect = non_retryable_exception("mock")
                    with pytest.raises(MutationsExceptionGroup) as e:
                        mutation = mutations.SetCell(
                            "family", b"qualifier", b"value", timestamp_micros=123
                        )
                        entry = mutations.RowMutationEntry(b"row_key", [mutation])
                        assert mutation.is_idempotent() is True
                        table.bulk_mutate_rows([entry], operation_timeout=0.2)
                    assert len(e.value.exceptions) == 1
                    failed_exception = e.value.exceptions[0]
                    assert isinstance(failed_exception, FailedMutationEntryError)
                    assert "non-idempotent" not in str(failed_exception)
                    cause = failed_exception.__cause__
                    assert isinstance(cause, non_retryable_exception)

    def test_bulk_mutate_error_index(self):
        """Test partial failure, partial success. Errors should be associated with the correct index"""
        from google.api_core.exceptions import (
            DeadlineExceeded,
            ServiceUnavailable,
            FailedPrecondition,
        )
        from google.cloud.bigtable.data.exceptions import (
            RetryExceptionGroup,
            FailedMutationEntryError,
            MutationsExceptionGroup,
        )

        with self._make_client(project="project") as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "mutate_rows"
                ) as mock_gapic:
                    mock_gapic.side_effect = [
                        self._mock_response([None, ServiceUnavailable("mock"), None]),
                        self._mock_response([DeadlineExceeded("mock")]),
                        self._mock_response([FailedPrecondition("final")]),
                    ]
                    with pytest.raises(MutationsExceptionGroup) as e:
                        mutation = mutations.SetCell(
                            "family", b"qualifier", b"value", timestamp_micros=123
                        )
                        entries = [
                            mutations.RowMutationEntry(
                                f"row_key_{i}".encode(), [mutation]
                            )
                            for i in range(3)
                        ]
                        assert mutation.is_idempotent() is True
                        table.bulk_mutate_rows(entries, operation_timeout=1000)
                    assert len(e.value.exceptions) == 1
                    failed = e.value.exceptions[0]
                    assert isinstance(failed, FailedMutationEntryError)
                    assert failed.index == 1
                    assert failed.entry == entries[1]
                    cause = failed.__cause__
                    assert isinstance(cause, RetryExceptionGroup)
                    assert len(cause.exceptions) == 3
                    assert isinstance(cause.exceptions[0], ServiceUnavailable)
                    assert isinstance(cause.exceptions[1], DeadlineExceeded)
                    assert isinstance(cause.exceptions[2], FailedPrecondition)

    def test_bulk_mutate_error_recovery(self):
        """If an error occurs, then resolves, no exception should be raised"""
        from google.api_core.exceptions import DeadlineExceeded

        with self._make_client(project="project") as client:
            table = client.get_table("instance", "table")
            with mock.patch.object(client._gapic_client, "mutate_rows") as mock_gapic:
                mock_gapic.side_effect = [
                    self._mock_response([DeadlineExceeded("mock")]),
                    self._mock_response([None]),
                ]
                mutation = mutations.SetCell(
                    "family", b"qualifier", b"value", timestamp_micros=123
                )
                entries = [
                    mutations.RowMutationEntry(f"row_key_{i}".encode(), [mutation])
                    for i in range(3)
                ]
                table.bulk_mutate_rows(entries, operation_timeout=1000)


class TestCheckAndMutateRow(ABC):
    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    @pytest.mark.parametrize("gapic_result", [True, False])
    def test_check_and_mutate(self, gapic_result):
        from google.cloud.bigtable_v2.types import CheckAndMutateRowResponse

        app_profile = "app_profile_id"
        with self._make_client() as client:
            with client.get_table(
                "instance", "table", app_profile_id=app_profile
            ) as table:
                with mock.patch.object(
                    client._gapic_client, "check_and_mutate_row"
                ) as mock_gapic:
                    mock_gapic.return_value = CheckAndMutateRowResponse(
                        predicate_matched=gapic_result
                    )
                    row_key = b"row_key"
                    predicate = None
                    true_mutations = [mock.Mock()]
                    false_mutations = [mock.Mock(), mock.Mock()]
                    operation_timeout = 0.2
                    found = table.check_and_mutate_row(
                        row_key,
                        predicate,
                        true_case_mutations=true_mutations,
                        false_case_mutations=false_mutations,
                        operation_timeout=operation_timeout,
                    )
                    assert found == gapic_result
                    kwargs = mock_gapic.call_args[1]
                    assert kwargs["table_name"] == table.table_name
                    assert kwargs["row_key"] == row_key
                    assert kwargs["predicate_filter"] == predicate
                    assert kwargs["true_mutations"] == [
                        m._to_pb() for m in true_mutations
                    ]
                    assert kwargs["false_mutations"] == [
                        m._to_pb() for m in false_mutations
                    ]
                    assert kwargs["app_profile_id"] == app_profile
                    assert kwargs["timeout"] == operation_timeout
                    assert kwargs["retry"] is None

    def test_check_and_mutate_bad_timeout(self):
        """Should raise error if operation_timeout < 0"""
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with pytest.raises(ValueError) as e:
                    table.check_and_mutate_row(
                        b"row_key",
                        None,
                        true_case_mutations=[mock.Mock()],
                        false_case_mutations=[],
                        operation_timeout=-1,
                    )
                assert str(e.value) == "operation_timeout must be greater than 0"

    def test_check_and_mutate_single_mutations(self):
        """if single mutations are passed, they should be internally wrapped in a list"""
        from google.cloud.bigtable.data.mutations import SetCell
        from google.cloud.bigtable_v2.types import CheckAndMutateRowResponse

        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "check_and_mutate_row"
                ) as mock_gapic:
                    mock_gapic.return_value = CheckAndMutateRowResponse(
                        predicate_matched=True
                    )
                    true_mutation = SetCell("family", b"qualifier", b"value")
                    false_mutation = SetCell("family", b"qualifier", b"value")
                    table.check_and_mutate_row(
                        b"row_key",
                        None,
                        true_case_mutations=true_mutation,
                        false_case_mutations=false_mutation,
                    )
                    kwargs = mock_gapic.call_args[1]
                    assert kwargs["true_mutations"] == [true_mutation._to_pb()]
                    assert kwargs["false_mutations"] == [false_mutation._to_pb()]

    def test_check_and_mutate_predicate_object(self):
        """predicate filter should be passed to gapic request"""
        from google.cloud.bigtable_v2.types import CheckAndMutateRowResponse

        mock_predicate = mock.Mock()
        predicate_pb = {"predicate": "dict"}
        mock_predicate._to_pb.return_value = predicate_pb
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "check_and_mutate_row"
                ) as mock_gapic:
                    mock_gapic.return_value = CheckAndMutateRowResponse(
                        predicate_matched=True
                    )
                    table.check_and_mutate_row(
                        b"row_key", mock_predicate, false_case_mutations=[mock.Mock()]
                    )
                    kwargs = mock_gapic.call_args[1]
                    assert kwargs["predicate_filter"] == predicate_pb
                    assert mock_predicate._to_pb.call_count == 1
                    assert kwargs["retry"] is None

    def test_check_and_mutate_mutations_parsing(self):
        """mutations objects should be converted to protos"""
        from google.cloud.bigtable_v2.types import CheckAndMutateRowResponse
        from google.cloud.bigtable.data.mutations import DeleteAllFromRow

        mutations = [mock.Mock() for _ in range(5)]
        for idx, mutation in enumerate(mutations):
            mutation._to_pb.return_value = f"fake {idx}"
        mutations.append(DeleteAllFromRow())
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "check_and_mutate_row"
                ) as mock_gapic:
                    mock_gapic.return_value = CheckAndMutateRowResponse(
                        predicate_matched=True
                    )
                    table.check_and_mutate_row(
                        b"row_key",
                        None,
                        true_case_mutations=mutations[0:2],
                        false_case_mutations=mutations[2:],
                    )
                    kwargs = mock_gapic.call_args[1]
                    assert kwargs["true_mutations"] == ["fake 0", "fake 1"]
                    assert kwargs["false_mutations"] == [
                        "fake 2",
                        "fake 3",
                        "fake 4",
                        DeleteAllFromRow()._to_pb(),
                    ]
                    assert all(
                        (mutation._to_pb.call_count == 1 for mutation in mutations[:5])
                    )


class TestReadModifyWriteRow(ABC):
    def _make_client(self, *args, **kwargs):
        return TestBigtableDataClient._make_client(*args, **kwargs)

    @pytest.mark.parametrize(
        "call_rules,expected_rules",
        [
            (
                AppendValueRule("f", "c", b"1"),
                [AppendValueRule("f", "c", b"1")._to_pb()],
            ),
            (
                [AppendValueRule("f", "c", b"1")],
                [AppendValueRule("f", "c", b"1")._to_pb()],
            ),
            (IncrementRule("f", "c", 1), [IncrementRule("f", "c", 1)._to_pb()]),
            (
                [AppendValueRule("f", "c", b"1"), IncrementRule("f", "c", 1)],
                [
                    AppendValueRule("f", "c", b"1")._to_pb(),
                    IncrementRule("f", "c", 1)._to_pb(),
                ],
            ),
        ],
    )
    def test_read_modify_write_call_rule_args(self, call_rules, expected_rules):
        """Test that the gapic call is called with given rules"""
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with mock.patch.object(
                    client._gapic_client, "read_modify_write_row"
                ) as mock_gapic:
                    table.read_modify_write_row("key", call_rules)
                assert mock_gapic.call_count == 1
                found_kwargs = mock_gapic.call_args_list[0][1]
                assert found_kwargs["rules"] == expected_rules
                assert found_kwargs["retry"] is None

    @pytest.mark.parametrize("rules", [[], None])
    def test_read_modify_write_no_rules(self, rules):
        with self._make_client() as client:
            with client.get_table("instance", "table") as table:
                with pytest.raises(ValueError) as e:
                    table.read_modify_write_row("key", rules=rules)
                    assert e.value.args[0] == "rules must contain at least one item"

    def test_read_modify_write_call_defaults(self):
        instance = "instance1"
        table_id = "table1"
        project = "project1"
        row_key = "row_key1"
        with self._make_client(project=project) as client:
            with client.get_table(instance, table_id) as table:
                with mock.patch.object(
                    client._gapic_client, "read_modify_write_row"
                ) as mock_gapic:
                    table.read_modify_write_row(row_key, mock.Mock())
                    assert mock_gapic.call_count == 1
                    kwargs = mock_gapic.call_args_list[0][1]
                    assert (
                        kwargs["table_name"]
                        == f"projects/{project}/instances/{instance}/tables/{table_id}"
                    )
                    assert kwargs["app_profile_id"] is None
                    assert kwargs["row_key"] == row_key.encode()
                    assert kwargs["timeout"] > 1

    def test_read_modify_write_call_overrides(self):
        row_key = b"row_key1"
        expected_timeout = 12345
        profile_id = "profile1"
        with self._make_client() as client:
            with client.get_table(
                "instance", "table_id", app_profile_id=profile_id
            ) as table:
                with mock.patch.object(
                    client._gapic_client, "read_modify_write_row"
                ) as mock_gapic:
                    table.read_modify_write_row(
                        row_key, mock.Mock(), operation_timeout=expected_timeout
                    )
                    assert mock_gapic.call_count == 1
                    kwargs = mock_gapic.call_args_list[0][1]
                    assert kwargs["app_profile_id"] is profile_id
                    assert kwargs["row_key"] == row_key
                    assert kwargs["timeout"] == expected_timeout

    def test_read_modify_write_string_key(self):
        row_key = "string_row_key1"
        with self._make_client() as client:
            with client.get_table("instance", "table_id") as table:
                with mock.patch.object(
                    client._gapic_client, "read_modify_write_row"
                ) as mock_gapic:
                    table.read_modify_write_row(row_key, mock.Mock())
                    assert mock_gapic.call_count == 1
                    kwargs = mock_gapic.call_args_list[0][1]
                    assert kwargs["row_key"] == row_key.encode()

    def test_read_modify_write_row_building(self):
        """results from gapic call should be used to construct row"""
        from google.cloud.bigtable.data.row import Row
        from google.cloud.bigtable_v2.types import ReadModifyWriteRowResponse
        from google.cloud.bigtable_v2.types import Row as RowPB

        mock_response = ReadModifyWriteRowResponse(row=RowPB())
        with self._make_client() as client:
            with client.get_table("instance", "table_id") as table:
                with mock.patch.object(
                    client._gapic_client, "read_modify_write_row"
                ) as mock_gapic:
                    with mock.patch.object(Row, "_from_pb") as constructor_mock:
                        mock_gapic.return_value = mock_response
                        table.read_modify_write_row("key", mock.Mock())
                        assert constructor_mock.call_count == 1
                        constructor_mock.assert_called_once_with(mock_response.row)


class TestReadRowsAcceptance(ABC):
    @staticmethod
    def _get_operation_class():
        from google.cloud.bigtable.data._sync._read_rows import _ReadRowsOperation

        return _ReadRowsOperation

    @staticmethod
    def _get_client_class():
        from google.cloud.bigtable.data._sync.client import BigtableDataClient

        return BigtableDataClient

    def parse_readrows_acceptance_tests():
        dirname = os.path.dirname(__file__)
        filename = os.path.join(dirname, "../read-rows-acceptance-test.json")
        with open(filename) as json_file:
            test_json = TestFile.from_json(json_file.read())
            return test_json.read_rows_tests

    @staticmethod
    def extract_results_from_row(row: Row):
        results = []
        for family, col, cells in row.items():
            for cell in cells:
                results.append(
                    ReadRowsTest.Result(
                        row_key=row.row_key,
                        family_name=family,
                        qualifier=col,
                        timestamp_micros=cell.timestamp_ns // 1000,
                        value=cell.value,
                        label=cell.labels[0] if cell.labels else "",
                    )
                )
        return results

    @staticmethod
    def _coro_wrapper(stream):
        return stream

    def _process_chunks(self, *chunks):
        def _row_stream():
            yield ReadRowsResponse(chunks=chunks)

        instance = mock.Mock()
        instance._remaining_count = None
        instance._last_yielded_row_key = None
        chunker = self._get_operation_class().chunk_stream(
            instance, self._coro_wrapper(_row_stream())
        )
        merger = self._get_operation_class().merge_rows(chunker)
        results = []
        for row in merger:
            results.append(row)
        return results

    @pytest.mark.parametrize(
        "test_case", parse_readrows_acceptance_tests(), ids=lambda t: t.description
    )
    def test_row_merger_scenario(self, test_case: ReadRowsTest):
        def _scenerio_stream():
            for chunk in test_case.chunks:
                yield ReadRowsResponse(chunks=[chunk])

        try:
            results = []
            instance = mock.Mock()
            instance._last_yielded_row_key = None
            instance._remaining_count = None
            chunker = self._get_operation_class().chunk_stream(
                instance, self._coro_wrapper(_scenerio_stream())
            )
            merger = self._get_operation_class().merge_rows(chunker)
            for row in merger:
                for cell in row:
                    cell_result = ReadRowsTest.Result(
                        row_key=cell.row_key,
                        family_name=cell.family,
                        qualifier=cell.qualifier,
                        timestamp_micros=cell.timestamp_micros,
                        value=cell.value,
                        label=cell.labels[0] if cell.labels else "",
                    )
                    results.append(cell_result)
        except InvalidChunk:
            results.append(ReadRowsTest.Result(error=True))
        for expected, actual in zip_longest(test_case.results, results):
            assert actual == expected

    @pytest.mark.parametrize(
        "test_case", parse_readrows_acceptance_tests(), ids=lambda t: t.description
    )
    def test_read_rows_scenario(self, test_case: ReadRowsTest):
        def _make_gapic_stream(chunk_list: list[ReadRowsResponse]):
            from google.cloud.bigtable_v2 import ReadRowsResponse

            class mock_stream:
                def __init__(self, chunk_list):
                    self.chunk_list = chunk_list
                    self.idx = -1

                def __iter__(self):
                    return self

                def __next__(self):
                    self.idx += 1
                    if len(self.chunk_list) > self.idx:
                        chunk = self.chunk_list[self.idx]
                        return ReadRowsResponse(chunks=[chunk])
                    raise StopIteration

                def cancel(self):
                    pass

            return mock_stream(chunk_list)

        with mock.patch.dict(os.environ, {"BIGTABLE_EMULATOR_HOST": "localhost"}):
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                client = self._get_client_class()()
        try:
            table = client.get_table("instance", "table")
            results = []
            with mock.patch.object(
                table.client._gapic_client, "read_rows"
            ) as read_rows:
                read_rows.return_value = _make_gapic_stream(test_case.chunks)
                for row in table.read_rows_stream(query={}):
                    for cell in row:
                        cell_result = ReadRowsTest.Result(
                            row_key=cell.row_key,
                            family_name=cell.family,
                            qualifier=cell.qualifier,
                            timestamp_micros=cell.timestamp_micros,
                            value=cell.value,
                            label=cell.labels[0] if cell.labels else "",
                        )
                        results.append(cell_result)
        except InvalidChunk:
            results.append(ReadRowsTest.Result(error=True))
        finally:
            client.close()
        for expected, actual in zip_longest(test_case.results, results):
            assert actual == expected

    def test_out_of_order_rows(self):
        def _row_stream():
            yield ReadRowsResponse(last_scanned_row_key=b"a")

        instance = mock.Mock()
        instance._remaining_count = None
        instance._last_yielded_row_key = b"b"
        chunker = self._get_operation_class().chunk_stream(
            instance, self._coro_wrapper(_row_stream())
        )
        merger = self._get_operation_class().merge_rows(chunker)
        with pytest.raises(InvalidChunk):
            for _ in merger:
                pass

    def test_bare_reset(self):
        first_chunk = ReadRowsResponse.CellChunk(
            ReadRowsResponse.CellChunk(
                row_key=b"a", family_name="f", qualifier=b"q", value=b"v"
            )
        )
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                first_chunk,
                ReadRowsResponse.CellChunk(
                    ReadRowsResponse.CellChunk(reset_row=True, row_key=b"a")
                ),
            )
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                first_chunk,
                ReadRowsResponse.CellChunk(
                    ReadRowsResponse.CellChunk(reset_row=True, family_name="f")
                ),
            )
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                first_chunk,
                ReadRowsResponse.CellChunk(
                    ReadRowsResponse.CellChunk(reset_row=True, qualifier=b"q")
                ),
            )
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                first_chunk,
                ReadRowsResponse.CellChunk(
                    ReadRowsResponse.CellChunk(reset_row=True, timestamp_micros=1000)
                ),
            )
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                first_chunk,
                ReadRowsResponse.CellChunk(
                    ReadRowsResponse.CellChunk(reset_row=True, labels=["a"])
                ),
            )
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                first_chunk,
                ReadRowsResponse.CellChunk(
                    ReadRowsResponse.CellChunk(reset_row=True, value=b"v")
                ),
            )

    def test_missing_family(self):
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                ReadRowsResponse.CellChunk(
                    row_key=b"a",
                    qualifier=b"q",
                    timestamp_micros=1000,
                    value=b"v",
                    commit_row=True,
                )
            )

    def test_mid_cell_row_key_change(self):
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                ReadRowsResponse.CellChunk(
                    row_key=b"a",
                    family_name="f",
                    qualifier=b"q",
                    timestamp_micros=1000,
                    value_size=2,
                    value=b"v",
                ),
                ReadRowsResponse.CellChunk(row_key=b"b", value=b"v", commit_row=True),
            )

    def test_mid_cell_family_change(self):
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                ReadRowsResponse.CellChunk(
                    row_key=b"a",
                    family_name="f",
                    qualifier=b"q",
                    timestamp_micros=1000,
                    value_size=2,
                    value=b"v",
                ),
                ReadRowsResponse.CellChunk(
                    family_name="f2", value=b"v", commit_row=True
                ),
            )

    def test_mid_cell_qualifier_change(self):
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                ReadRowsResponse.CellChunk(
                    row_key=b"a",
                    family_name="f",
                    qualifier=b"q",
                    timestamp_micros=1000,
                    value_size=2,
                    value=b"v",
                ),
                ReadRowsResponse.CellChunk(
                    qualifier=b"q2", value=b"v", commit_row=True
                ),
            )

    def test_mid_cell_timestamp_change(self):
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                ReadRowsResponse.CellChunk(
                    row_key=b"a",
                    family_name="f",
                    qualifier=b"q",
                    timestamp_micros=1000,
                    value_size=2,
                    value=b"v",
                ),
                ReadRowsResponse.CellChunk(
                    timestamp_micros=2000, value=b"v", commit_row=True
                ),
            )

    def test_mid_cell_labels_change(self):
        with pytest.raises(InvalidChunk):
            self._process_chunks(
                ReadRowsResponse.CellChunk(
                    row_key=b"a",
                    family_name="f",
                    qualifier=b"q",
                    timestamp_micros=1000,
                    value_size=2,
                    value=b"v",
                ),
                ReadRowsResponse.CellChunk(labels=["b"], value=b"v", commit_row=True),
            )
